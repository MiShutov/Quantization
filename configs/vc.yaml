layer_quantizer: &layer_quantizer !VectorQuantizer
  codebook_size: 1024
  group_size: 8
  scaler: !RowScaler
    n_blocks: 1
    trainable: False


lm_head_quantizer: &lm_head_quantizer !QuantizerLSQ
  group_size: channel
  bit_width: 8 
  use_offset: False
  initializer: !MinMaxInitializer


embed_quantizer: &embed_quantizer !QuantizerLSQ
  group_size: channel
  bit_width: 8 
  use_offset: False
  initializer: !MinMaxInitializer


wrapper: &wrapper !Wrapper
  wrap_rule: 
    Linear: !QLinear
      weight_quantizer: *layer_quantizer
    Embedding: !QEmbedding
      weight_quantizer: *embed_quantizer
  exceptions:
    Linear:
      lm_head: !QLinear
        weight_quantizer: *lm_head_quantizer

      # layers.1.: !QLinear
      #   weight_quantizer: *layer_quantizer_4bit

      # layers.15.: !QLinear
      #   weight_quantizer: *layer_quantizer_4bit

      # self_attn.q_proj: !QLinear
      #   weight_quantizer: *lm_head_quantizer

training_params:
  dataset:
    dataset_name: slim_pajama
    split: train[:2000]
    seq_length: 2048
    n_train_seq: 256 #1024 #512
    n_val_seq: 64
    batch_size: 2

  optimization:
    n_epochs: 2
    loss_fn: !MomentCriteria
      p: 2
    optimizers:
      levels_optimizer:
        param_label: codebook
        class: Adam
        kwargs:
          lr: 3e-5
        scheduler:
          class: CosineAnnealingLR
          kwargs:
            T_max: 1024



