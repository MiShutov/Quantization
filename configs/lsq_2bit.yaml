layer_initializer: &layer_initializer !GreedyInitializer
  criteria: !MomentCriteria
    p: 2
    along_axis: -1
  n_grid_steps: 5
  n_grid_zooms: 2


layer_quantizer: &layer_quantizer !QuantizerLSQ
  group_size: 64
  bit_width: 2
  use_offset: True
  initializer: *layer_initializer


layer_quantizer_high_bit: &layer_quantizer_high_bit !QuantizerLSQ
  group_size: 64
  bit_width: 4
  use_offset: True
  initializer: *layer_initializer


lm_head_quantizer: &lm_head_quantizer !QuantizerLSQ
  group_size: channel
  bit_width: 8 
  use_offset: False
  initializer: !MinMaxInitializer


embed_quantizer: &embed_quantizer !QuantizerLSQ
  group_size: channel
  bit_width: 8 
  use_offset: False
  initializer: !MinMaxInitializer


wrapper: &wrapper !Wrapper
  wrap_rule: 
    Linear: !QLinear
      weight_quantizer: *layer_quantizer
    Embedding: !QEmbedding
      weight_quantizer: *embed_quantizer
  exceptions:
    Linear:
      lm_head: !QLinear
        weight_quantizer: *lm_head_quantizer

      # layers.1.: !QLinear
      #   weight_quantizer: *layer_quantizer_high_bit

      # layers.15.: !QLinear
      #   weight_quantizer: *layer_quantizer_high_bit

      # self_attn.q_proj: !QLinear
      #   weight_quantizer: *lm_head_quantizer

training_params:
  dataset:
    dataset_name: slim_pajama
    split: train[:1000]
    seq_length: 2048
    n_train_seq: 256 #512
    n_val_seq: 32
    batch_size: 2

  optimization:
    n_epochs: 2
    loss_fn: !MomentCriteria
      p: 2
    optimizers:
      step_optimizer:
        param_label: step
        class: Adam
        kwargs:
          lr: 1e-4
        scheduler:
          class: CosineAnnealingLR
          kwargs:
            T_max: 1024

      offset_optimizer:
        param_label: offset
        class: Adam
        kwargs:
          lr: 1e-4
        scheduler:
          class: CosineAnnealingLR
          kwargs:
            T_max: 1024
