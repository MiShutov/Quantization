{
	"VQLinear_params": {
	  "codebook_shape": [
		65536,
		8
	  ],
	  "path_to_init_data": "/srv/workspace/Kirin_AI_Workspace/Kirin_AI_User/TMG/l00930864/project/RuProject/pretrained-models/llm/kmeans/Llama-2-70b-hf/vecdim8_cbsize16_scalesOUTL2_weightNO",
	  "use_channel_scales": true,
	  "vecdim": 8
	},
	"architectures": [
	  "modeling_VQ_llama.VQLlamaForCausalLM"
	],
	"attention_bias": false,
	"attention_dropout": 0.0,
	"auto_map": {
	  "AutoModel": "modeling_VQ_llama.VQLlamaForCausalLM",
	  "AutoModelForCausalLM": "modeling_VQ_llama.VQLlamaForCausalLM"
	},
	"bos_token_id": 1,
	"eos_token_id": 2,
	"head_dim": 128,
	"hidden_act": "silu",
	"hidden_size": 8192,
	"initializer_range": 0.02,
	"intermediate_size": 28672,
	"max_position_embeddings": 4096,
	"mlp_bias": false,
	"model_type": "llama",
	"num_attention_heads": 64,
	"num_hidden_layers": 80,
	"num_key_value_heads": 8,
	"pad_token_id": 0,
	"pretraining_tp": 1,
	"rms_norm_eps": 1e-05,
	"rope_scaling": null,
	"rope_theta": 10000.0,
	"tie_word_embeddings": false,
	"torch_dtype": "float32",
	"transformers_version": "4.52.4",
	"use_cache": true,
	"vocab_size": 32000
  }