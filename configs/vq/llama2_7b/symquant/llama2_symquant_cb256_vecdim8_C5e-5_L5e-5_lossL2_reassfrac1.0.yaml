logdir: /home/msst/repo/Quantization/logs/llama2-7b/symquant/llama2_symquant_cb256_vecdim8_C5e-5_L5e-5_lossL2_reassfrac1.0
model_name: Llama2-7b-hf
fp_blocks_path: /mnt/ssd_storage/ml/weights/vc_data/Llama2-7b-hf/per_block_fp
q_blocks_path: /mnt/ssd_storage/ml/weights/vc_data/Llama2-7b-hf/per_block_q/cb256_vecdim8_weightPERCOORD_scaleOUTL2_distMSE_blocksizeNone_iters25_abscoords


training_params:
  train_data:
    dataset_name: slim_pajama
    split: train[:300] #[:2000]
    seq_length: 4096
    n_train_seq: &n_train_seq 64 #256 #384
    n_val_seq: 16 #48
    batch_size: &batch_size 4

  validation_settings:
    val_before_trainig: True
    n_intermediate_val: 4 #8

  optimization_settings:
    n_epochs: 2
    loss_fn: !LpLoss
      p: 2

    method_params:
      reassine_params:
        reassine_frac: 1.0
        type: 'torch'
        batch_size: 4096

    scheduler: &scheduler
      class: CosineAnnealingLR
      kwargs:
        T_max: 256

    optimizers:
      # codebook_optimizer:
      #   param_label: codebook
      #   class: Adam
      #   kwargs:
      #     lr: 1e-2 # 5e-5
      #   scheduler: *scheduler

      latent_optimizer:
        param_label: latent_weight
        class: Adam 
        kwargs:
          lr: 1e-4
        scheduler: *scheduler
