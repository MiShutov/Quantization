logdir: /home/msst/repo/Quantization/logs/llama2-7b/trellis/llama2_trellis_128seq_bs2_ne2_quant_act
model_name: Llama2-7b-hf
fp_blocks_path: /mnt/ssd_storage/ml/weights/vc_data/Llama2-7b-hf/per_block_fp
q_blocks_path: /home/msst/repo/Quantization/ml/weights/vc_data/Llama2-7b-hf/per_block_q/T256_L16_V2_K2_cbs10_LowBitSym_qtip_ptq_bs5_act8bit


training_params:
  train_data:
    dataset_name: slim_pajama
    split: train[:1500]
    seq_length: 4096
    n_train_seq: &n_train_seq 32 # 128
    n_val_seq: 32
    batch_size: &batch_size 2 # 4

  validation_settings:
    val_before_trainig: True
    n_intermediate_val: 4

  optimization_settings:
    n_epochs: 2
    loss_fn: !LpLoss
      p: 2

    scheduler: &scheduler
      class: CosineAnnealingLR
      kwargs:
        T_max: 256

    optimizers:
      act_scales_optimizer:
        param_label: act_scale
        class: Adam
        kwargs:
          lr: 0.0 # 1e-5
        scheduler: *scheduler

      scales_optimizer:
        param_label: scales
        class: Adam
        kwargs:
          lr: 5e-4
        scheduler: *scheduler

      SU_optimizer:
        param_label: SU
        class: Adam
        kwargs:
          lr: 5e-4
        scheduler: *scheduler

      SV_optimizer:
        param_label: SV
        class: Adam
        kwargs:
          lr: 5e-4
        scheduler: *scheduler

      norm_optimizer:
        param_label: norm
        class: Adam
        kwargs:
          lr: 5e-5
        scheduler: *scheduler