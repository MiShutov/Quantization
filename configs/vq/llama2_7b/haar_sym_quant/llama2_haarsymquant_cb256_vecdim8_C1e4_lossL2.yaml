logdir: /home/msst/repo/Quantization/logs/llama2-7b/haarsymquant/llama2_haarsymquant_cb256_vecdim8_C1e4_lossL2
model_name: Llama2-7b-hf
fp_blocks_path: /mnt/ssd_storage/ml/weights/vc_data/Llama2-7b-hf/per_block_fp
q_blocks_path: /mnt/ssd_storage/ml/weights/vc_data/Llama2-7b-hf/per_block_q/cb256_vecdim8_weightPERCOORD_scaleOUTL2_distMSE_blocksizeNone_iters10_abscoords_haar2


training_params:
  train_data:
    dataset_name: slim_pajama
    split: train[:1500]
    seq_length: 4096
    n_train_seq: &n_train_seq 360
    n_val_seq: 20 #48
    batch_size: &batch_size 4

  validation_settings:
    val_before_trainig: True
    n_intermediate_val: 6 #8

  optimization_settings:
    n_epochs: 3
    loss_fn: !L2

    method_params:
      use_latent_weight: False #True
      # reassine_params:
      #   reassine_frac: 0.0
      #   nlist: 256
      #   nprobe: 8

    scheduler: &scheduler
      class: CosineAnnealingLR
      kwargs:
        T_max: 300 #256

    optimizers:
      codebook_optimizer:
        param_label: codebook
        class: Adam
        kwargs:
          lr: 1e-4 #5e-5
        scheduler: *scheduler

      haar_optimizer:
        param_label: inverse_conv
        class: Adam
        kwargs:
          lr: 1e-5
        scheduler: *scheduler

      scales_optimizer:
        param_label: scales
        class: Adam
        kwargs:
          lr: 1e-5
        scheduler: *scheduler