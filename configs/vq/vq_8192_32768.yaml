group_size: &group_size 8
with_reassings: &with_reassings True
with_additions: &with_additions True

medium_quantizer: &medium_quantizer !VectorQuantizer
  codebook_size: 8192
  group_size: *group_size
  with_additions: *with_additions
  with_reassings: *with_reassings

large_quantizer: &large_quantizer !VectorQuantizer
  codebook_size: 32768
  group_size: *group_size
  with_additions: *with_additions
  with_reassings: *with_reassings

lm_head_quantizer: &lm_head_quantizer !QuantizerLSQ
  group_size: channel
  bit_width: 8 
  use_offset: False
  initializer: !MinMaxInitializer

embed_quantizer: &embed_quantizer !QuantizerLSQ
  group_size: channel
  bit_width: 8 
  use_offset: False
  initializer: !MinMaxInitializer

wrapper: &wrapper !Wrapper
  wrap_rule: 
    Linear: !QLinear
      weight_quantizer: *large_quantizer
    Embedding: !QEmbedding
      weight_quantizer: *embed_quantizer
  exceptions:
    Linear:
      lm_head: !QLinear
        weight_quantizer: *lm_head_quantizer

      self_attn: !QLinear
        weight_quantizer: *medium_quantizer

      mlp: !QLinear
        weight_quantizer: *large_quantizer


training_params:
  dataset:
    dataset_name: slim_pajama
    split: train[:2500]
    seq_length: 2048
    n_train_seq: 128
    n_val_seq: 32 #64
    batch_size: 8 # 2 #16 #2
    scheduler: &scheduler
      class: CosineAnnealingLR
      kwargs:
        T_max: 256 #512

  optimization:
    n_epochs: 1
    loss_fn: !MSE #!MAE

    optimizers:
      codebook_optimizer:
        param_label: codebook
        class: Adam
        kwargs:
          lr: 1e-4
        scheduler: *scheduler

      # additions_optimizer:
      #   param_label: additions
      #   class: Adam
      #   kwargs:
      #     lr: 2e-6 #2e-5
      #   scheduler: *scheduler



