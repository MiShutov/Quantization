initialization_params: &initialization_params
  optim : Adam
  lr : 1e-3
  steps : 100
  norm_grad : False
  criteria : !MomentCriteria
    p: 2


layer_initializer: &layer_initializer !GreedyInitializer
  criteria: !MomentCriteria
    p: 4
    along_axis: -1
  n_grid_steps: 5
  n_grid_zooms: 2


layer_quantizer: &layer_quantizer !QuantizerLUT
  group_size: 64
  bit_width: 2
  with_additions: True
  initialization_params: *initialization_params


layer_quantizer_high_bit: &layer_quantizer_high_bit !QuantizerLSQ
  group_size: 64
  bit_width: 4
  use_offset: True
  initializer: *layer_initializer


lm_head_quantizer: &lm_head_quantizer !QuantizerLSQ
  group_size: channel
  bit_width: 8 
  use_offset: False
  initializer: !MinMaxInitializer


embed_quantizer: &embed_quantizer !QuantizerLSQ
  group_size: channel
  bit_width: 8 
  use_offset: False
  initializer: !MinMaxInitializer


wrapper: &wrapper !Wrapper
  wrap_rule: 
    Linear: !QLinear
      weight_quantizer: *layer_quantizer
    Embedding: !QEmbedding
      weight_quantizer: *embed_quantizer
  exceptions:
    Linear:
      lm_head: !QLinear
        weight_quantizer: *lm_head_quantizer

      layers.0.: !QLinear
        weight_quantizer: *layer_quantizer_high_bit

      layers.1.: !QLinear
        weight_quantizer: *layer_quantizer_high_bit

      layers.15.: !QLinear
        weight_quantizer: *layer_quantizer_high_bit

      #layers.1.mlp: !QLinear
      # weight_quantizer: *layer_quantizer_high_bit

      # self_attn.q_proj: !QLinear
      #   weight_quantizer: *lm_head_quantizer

training_params:
  dataset:
    dataset_name: slim_pajama
    split: train[:2500]
    seq_length: 2048
    n_train_seq: 128 #512 #256 #512
    n_val_seq: 32
    batch_size: 2
    T_max: &T_max 1024

  optimization:
    n_epochs: 2
    loss_fn: !MomentCriteria
      p: 2
    optimizers:
      levels_optimizer:
        param_label: levels
        class: Adam
        kwargs:
          lr: 1e-4 #3e-5
        scheduler:
          class: CosineAnnealingLR
          kwargs:
            T_max: *T_max

      additions_optimizer:
        param_label: weight_additions
        class: Adam
        kwargs:
          lr: 2e-5
        scheduler:
          class: CosineAnnealingLR
          kwargs:
            T_max: *T_max

      step_optimizer:
        param_label: step
        class: Adam
        kwargs:
          lr: 3e-5 # 1e-5
        scheduler:
          class: CosineAnnealingLR
          kwargs:
            T_max: *T_max

      offset_optimizer:
        param_label: offset
        class: Adam
        kwargs:
          lr: 3e-5 # 1e-5
        scheduler:
          class: CosineAnnealingLR
          kwargs:
            T_max: *T_max