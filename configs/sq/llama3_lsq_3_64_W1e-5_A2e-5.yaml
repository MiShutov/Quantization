logdir: /home/msst/repo/Quantization/logs/llama3_lsq_3_64_W1e-5_A2e-5
model_name: Llama-3.2-1B
#path_to_checkpoint: /home/msst/repo/Quantization/logs/llama3_lsq_2_64_W1e-4_A2e-5/qmodel.pth

layer_initializer: &layer_initializer !GreedyInitializer
  criteria: !MomentCriteria
    p: 2
    along_axis: -1
  n_grid_steps: 5
  n_grid_zooms: 2

layer_quantizer_lsq: &layer_quantizer_lsq !QuantizerLSQ
  group_size: 64
  bit_width: 3
  use_offset: True
  with_additions: True
  initializer: *layer_initializer

lm_head_quantizer: &lm_head_quantizer !QuantizerLSQ
  group_size: channel
  bit_width: 8 
  use_offset: False
  initializer: !MinMaxInitializer

embed_quantizer: &embed_quantizer !QuantizerLSQ
  group_size: channel
  bit_width: 8 
  use_offset: False
  initializer: !MinMaxInitializer

wrapper: &wrapper !Wrapper
  wrap_rule: 
    Linear: !QLinear
      weight_quantizer: *layer_quantizer_lsq
    Embedding: !QEmbedding
      weight_quantizer: *embed_quantizer
  exceptions:
    Linear:
      lm_head: !QLinear
        weight_quantizer: *lm_head_quantizer


test_params:
  test_data:
    dataset_name: wiki
    split: test
    seq_length: 2048
    batch_size: 2

training_params:
  train_data:
    dataset_name: slim_pajama
    split: train[:2500]
    seq_length: 2048
    n_train_seq: 256 #800
    n_val_seq: 64 #128
    batch_size: 2

  validation_settings:
    val_before_trainig: True
    n_intermediate_val: 3

  optimization_settings:
    n_epochs: 2
    loss_fn: !LpLoss
      p: 2

    scheduler: &scheduler
      class: CosineAnnealingLR
      kwargs:
        T_max: 512

    optimizers:
      additions_optimizer:
        param_label: additions
        class: Adam
        kwargs:
          lr: 2e-5
        scheduler: *scheduler

      step_optimizer:
        param_label: step
        class: Adam
        kwargs:
          lr: 1e-5
        scheduler: *scheduler

      offset_optimizer:
        param_label: offset
        class: Adam
        kwargs:
          lr: 1e-5
        scheduler: *scheduler

