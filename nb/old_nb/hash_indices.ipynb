{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "m,n = 4096 // 4, 4096 // 4\n",
    "w = torch.randn(m, n).cuda()\n",
    "\n",
    "vecdim = 8\n",
    "index_bits = 12 #16 #8\n",
    "n_layers = 256\n",
    "\n",
    "indices_shape = m * n // vecdim\n",
    "\n",
    "with torch.no_grad():\n",
    "\tindices = torch.randint(0, 2**index_bits, (n_layers, indices_shape)).cuda()\n",
    "\n",
    "# codebook = torch.nn.Parameter(torch.randn((n_layers, 2**index_bits, vecdim), dtype=torch.float32).cuda(), requires_grad=True)\n",
    "# scales = torch.nn.Parameter(torch.ones((n_layers, m, 1), dtype=torch.float32).cuda() / n_layers, requires_grad=True)\n",
    "# optim = torch.optim.Adam(params=[codebook, scales], lr=1e-2)\n",
    "\n",
    "\n",
    "codebook = torch.nn.Parameter(torch.randn((2**index_bits, vecdim), dtype=torch.float32).cuda(), requires_grad=True)\n",
    "optim = torch.optim.Adam(params=[codebook,], lr=1e-2)\n",
    "\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1845, device='cuda:0', grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vector_quantize(w, codebook):\n",
    "    \"\"\"\n",
    "    Векторное квантование матрицы\n",
    "    \n",
    "    Args:\n",
    "        w: torch.Tensor shape (m, n) - исходная матрица\n",
    "        codebook: torch.Tensor shape (codebook_size, vecdim) - кодбук\n",
    "    \n",
    "    Returns:\n",
    "        w_q: torch.Tensor shape (m, n) - квантованная матрица\n",
    "        indices: torch.Tensor - выбранные индексы\n",
    "    \"\"\"\n",
    "    m, n = w.shape\n",
    "    codebook_size, vecdim = codebook.shape\n",
    "    \n",
    "    # Проверяем совместимость размеров\n",
    "    if (m * n) % vecdim != 0:\n",
    "        raise ValueError(f\"Размер матрицы {m}x{n} не делится на vecdim={vecdim}\")\n",
    "    \n",
    "    # Разбиваем матрицу на векторы\n",
    "    n_vectors = m * n // vecdim\n",
    "    w_vectors = w.reshape(n_vectors, vecdim)  # (n_vectors, vecdim)\n",
    "    \n",
    "    # Вычисляем попарные расстояния между всеми векторами и кодами\n",
    "    # w_vectors: (n_vectors, vecdim)\n",
    "    # codebook: (codebook_size, vecdim)\n",
    "    distances = torch.cdist(w_vectors, codebook, p=2)  # (n_vectors, codebook_size)\n",
    "    \n",
    "    # Находим ближайшие индексы\n",
    "    indices = torch.argmin(distances, dim=1)  # (n_vectors,)\n",
    "    \n",
    "    # Заменяем векторы на ближайшие из кодбука\n",
    "    w_q_vectors = codebook[indices]  # (n_vectors, vecdim)\n",
    "    \n",
    "    # Восстанавливаем исходную форму\n",
    "    w_q = w_q_vectors.reshape(m, n)\n",
    "    \n",
    "    return w_q, indices\n",
    "\n",
    "w_q, best_indices = vector_quantize(w, codebook)\n",
    "\n",
    "loss_fn(w_q, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3707, device='cuda:0', grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def quantize_with_constraints_vectorized(w, codebook, allowed_indices, vecdim):\n",
    "    \"\"\"\n",
    "    Векторизованная версия для лучшей производительности\n",
    "    \"\"\"\n",
    "    m, n = w.shape\n",
    "    n_vectors = m * n // vecdim\n",
    "    \n",
    "    # Разбиваем на векторы\n",
    "    w_vectors = w.reshape(-1, vecdim)  # (n_vectors, vecdim)\n",
    "    \n",
    "    # Получаем все разрешенные векторы для всех позиций\n",
    "    # allowed_indices: (n_layers, n_vectors)\n",
    "    # -> allowed_vectors: (n_layers, n_vectors, vecdim)\n",
    "    allowed_vectors = codebook[allowed_indices]\n",
    "    \n",
    "    # Вычисляем расстояния\n",
    "    # w_vectors: (n_vectors, vecdim) -> (1, n_vectors, vecdim)\n",
    "    # allowed_vectors: (n_layers, n_vectors, vecdim)\n",
    "    distances = F.mse_loss(\n",
    "        w_vectors.unsqueeze(0).expand_as(allowed_vectors),\n",
    "        allowed_vectors,\n",
    "        reduction='none'\n",
    "    ).mean(dim=2)  # (n_layers, n_vectors)\n",
    "    \n",
    "    # Находим лучшие индексы\n",
    "    best_layer_indices = torch.argmin(distances, dim=0)  # (n_vectors,)\n",
    "    best_indices = allowed_indices[best_layer_indices, torch.arange(n_vectors)]\n",
    "    \n",
    "    # Собираем результат\n",
    "    best_vectors = codebook[best_indices]  # (n_vectors, vecdim)\n",
    "    w_q = best_vectors.reshape(m, n)\n",
    "    \n",
    "    return w_q, best_indices\n",
    "\n",
    "w_q, best_indices = quantize_with_constraints_vectorized(w, codebook, indices, vecdim)\n",
    "\n",
    "loss_fn(w_q, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3707, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.2946, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mMSELoss()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m steps \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_steps):\n\u001b[1;32m      5\u001b[0m \t\u001b[38;5;66;03m#w_q, best_indices = vector_quantize(w, codebook)\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \tw_q, best_indices \u001b[38;5;241m=\u001b[39m \u001b[43mquantize_with_constraints_vectorized\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcodebook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvecdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \t\u001b[38;5;66;03m# w_q = torch.gather(\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \t\u001b[38;5;66;03m# \tcodebook,\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \t\u001b[38;5;66;03m# \t1,\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \t\u001b[38;5;66;03m# \tindices.unsqueeze(-1).expand(*indices.shape, vecdim)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \t\u001b[38;5;66;03m# ).reshape(n_layers, m, n)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \t\u001b[38;5;66;03m# w_q = (scales * w_q).sum(0)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \tloss \u001b[38;5;241m=\u001b[39m loss_fn(w, w_q)\n",
      "Cell \u001b[0;32mIn[3], line 31\u001b[0m, in \u001b[0;36mquantize_with_constraints_vectorized\u001b[0;34m(w, codebook, allowed_indices, vecdim)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Собираем результат\u001b[39;00m\n\u001b[1;32m     30\u001b[0m best_vectors \u001b[38;5;241m=\u001b[39m codebook[best_indices]  \u001b[38;5;66;03m# (n_vectors, vecdim)\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m w_q \u001b[38;5;241m=\u001b[39m \u001b[43mbest_vectors\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m w_q, best_indices\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_steps = 2500\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "for steps in range(n_steps):\n",
    "\t#w_q, best_indices = vector_quantize(w, codebook)\n",
    "\tw_q, best_indices = quantize_with_constraints_vectorized(w, codebook, indices, vecdim)\n",
    "\t\n",
    "\t# w_q = torch.gather(\n",
    "\t# \tcodebook,\n",
    "\t# \t1,\n",
    "\t# \tindices.unsqueeze(-1).expand(*indices.shape, vecdim)\n",
    "\t# ).reshape(n_layers, m, n)\n",
    "\t# w_q = (scales * w_q).sum(0)\n",
    "\t\n",
    "\tloss = loss_fn(w, w_q)\n",
    "\tloss.backward()\n",
    "\toptim.step()\n",
    "\toptim.zero_grad()\n",
    "\t\n",
    "\tif steps % (n_steps // 10) == 0:\n",
    "\t\tprint(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
