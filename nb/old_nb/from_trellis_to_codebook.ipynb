{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_quantize_batched(w, codebook, batch_size=1024):\n",
    "    m, n = w.shape\n",
    "    codebook_size, vecdim = codebook.shape\n",
    "    \n",
    "    \n",
    "    # if (m * n) % vecdim != 0:\n",
    "    #     raise ValueError(f\"Размер матрицы {m}x{n} не делится на vecdim={vecdim}\")\n",
    "    \n",
    "    # n_vectors = m * n // vecdim\n",
    "    # w_vectors = w.reshape(n_vectors, vecdim)  # (n_vectors, vecdim)\n",
    "    \n",
    "    w = w.reshape(-1)\n",
    "    w_pad = 0\n",
    "\n",
    "    if w.shape[0] % vecdim != 0:\n",
    "        w_pad = vecdim - w.shape[0] % vecdim\n",
    "        w = torch.cat([w, torch.zeros(w_pad).to(w.device)])\n",
    "    \n",
    "    n_vectors = w.shape[0] // vecdim\n",
    "    w_vectors = w.reshape(n_vectors, vecdim)  # (n_vectors, vecdim)\n",
    "\n",
    "\n",
    "    indices = torch.empty(n_vectors, dtype=torch.long, device=w.device)\n",
    "    \n",
    "    for start in range(0, n_vectors, batch_size):\n",
    "        end = min(start + batch_size, n_vectors)\n",
    "        batch = w_vectors[start:end]  # (batch_size, vecdim)\n",
    "        \n",
    "        distances = torch.cdist(batch, codebook, p=2)  # (batch_size, codebook_size)\n",
    "        indices[start:end] = torch.argmin(distances, dim=1)\n",
    "    \n",
    "    w_q_vectors = codebook[indices]  # (n_vectors, vecdim)\n",
    "    \n",
    "    if w_pad != 0:\n",
    "        w_q_vectors = w_q_vectors.reshape(-1)[:-w_pad]\n",
    "\n",
    "    w_q = w_q_vectors.reshape(m, n)\n",
    "    \n",
    "    return w_q, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fp4_s1e2m1_subnormal(x4):\n",
    "        s = (x4 >> 3) & 0b1\n",
    "        e = (x4 >> 1) & 0b11\n",
    "        m = x4 & 0b1\n",
    "        value = torch.where(e==0, 0.5 * m.float(), (1.0 + 0.5*m.float()) * (2.0 ** (e.float() - 1)))\n",
    "        value = value * torch.where(s==0, 1.0, -1.0)        \n",
    "        return value\n",
    "\n",
    "def fp6_s1e3m2_subnormal(x6):\n",
    "    s = (x6 >> 5) & 0b1       \n",
    "    e = (x6 >> 2) & 0b111     \n",
    "    m = x6 & 0b11    \n",
    "             \n",
    "    e_f = e.float()\n",
    "    m_f = m.float()\n",
    "\n",
    "    # Bias = 3 (2^(3-1)-1)\n",
    "    value = torch.where(\n",
    "        e == 0,\n",
    "        (m_f / (2**2)) * (2.0 ** (1 - 3)),\n",
    "        (1.0 + m_f / (2**2)) * (2.0 ** (e_f - 3))\n",
    "    )\n",
    "\n",
    "    value = value * torch.where(s == 0, 1.0, -1.0)\n",
    "    return value\n",
    "\n",
    "def fp8_s1e4m3_subnormal(x8):\n",
    "    # Разбор битов\n",
    "    s = (x8 >> 7) & 0b1       \n",
    "    e = (x8 >> 3) & 0b1111    \n",
    "    m = x8 & 0b111            \n",
    "\n",
    "    # Преобразуем к float\n",
    "    e_f = e.float()\n",
    "    m_f = m.float()\n",
    "\n",
    "    # Subnormal: e == 0\n",
    "    value = torch.where(\n",
    "        e == 0,\n",
    "        (m_f / (2**3)) * (2.0 ** (1 - 7)), \n",
    "        (1.0 + m_f / (2**3)) * (2.0 ** (e_f - 7))\n",
    "    )\n",
    "\n",
    "    value = value * torch.where(s == 0, 1.0, -1.0)\n",
    "    return value\n",
    "\n",
    "\n",
    "def int8_to_fp4_pair(x):\n",
    "    x = x.int() & 0xFF\n",
    "\n",
    "    x1 = (x >> 4) & 0b1111\n",
    "    x2 = x & 0b1111\n",
    "\n",
    "    return fp4_s1e2m1_subnormal(torch.stack([x1, x2], dim=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permute_16bits(x):\n",
    "    x = x.to(torch.int32) * 34038481 + 76625530\n",
    "    x = x * (x + 1)\n",
    "    x = (x >> 9) & ((1<<16) - 1)\n",
    "    return x\n",
    "\n",
    "\n",
    "def pack_bits(matrix, in_bits=4, out_bits=16):\n",
    "    \"\"\"\n",
    "    matrix: [M, N], значения в пределах (0..2^in_bits-1)\n",
    "    in_bits: число бит для каждого входного значения\n",
    "    out_bits: разрядность упаковки (8, 16, 32, 64)\n",
    "    \"\"\"\n",
    "    M, N = matrix.shape\n",
    "    total_bits = N * in_bits\n",
    "    num_out = (total_bits + out_bits - 1) // out_bits  # ceil\n",
    "\n",
    "    # Внутренний тип всегда int64 для сдвигов\n",
    "    packed = torch.zeros((M, num_out), dtype=torch.int64, device=matrix.device)\n",
    "\n",
    "    bit_position = 0\n",
    "    mask_in = (1 << in_bits) - 1\n",
    "\n",
    "    for col in range(N):\n",
    "        # Берём значения текущего столбца\n",
    "        values = (matrix[:, col].to(torch.int64) & mask_in) << (bit_position % out_bits)\n",
    "        idx_low = bit_position // out_bits\n",
    "        packed[:, idx_low] |= values & ((1 << out_bits) - 1)\n",
    "\n",
    "        # Если пересекли границу out_bits, записываем остаток в следующий элемент\n",
    "        if (bit_position % out_bits) + in_bits > out_bits:\n",
    "            packed[:, idx_low + 1] |= values >> out_bits\n",
    "\n",
    "        bit_position += in_bits\n",
    "\n",
    "    # Приводим к целевому типу\n",
    "    if out_bits == 8:\n",
    "        return packed.to(torch.uint8)\n",
    "    elif out_bits == 16:\n",
    "        return packed.to(torch.uint16)\n",
    "    elif out_bits == 32:\n",
    "        return packed.to(torch.int32)  # uint32 нет в PyTorch\n",
    "    elif out_bits == 64:\n",
    "        return packed.to(torch.int64)\n",
    "    else:\n",
    "        raise ValueError(\"out_bits должен быть 8, 16, 32 или 64\")\n",
    "    \n",
    "    \n",
    "def unpack_bits(packed, N, in_bits=4, out_bits=16):\n",
    "    M, num_out = packed.shape\n",
    "    packed64 = packed.to(torch.int64)\n",
    "    unpacked = torch.zeros((M, N), dtype=torch.int64, device=packed.device)\n",
    "\n",
    "    mask_in = (1 << in_bits) - 1\n",
    "    bit_position = 0\n",
    "\n",
    "    for col in range(N):\n",
    "        idx_low = bit_position // out_bits\n",
    "        shift = bit_position % out_bits\n",
    "        value = packed64[:, idx_low] >> shift\n",
    "        if shift + in_bits > out_bits:\n",
    "            value |= packed64[:, idx_low + 1] << (out_bits - shift)\n",
    "        unpacked[:, col] = value & mask_in\n",
    "        bit_position += in_bits\n",
    "\n",
    "    return unpacked\n",
    "\n",
    "# indices = torch.arange(1 << 16)\n",
    "# windows = sliding_bit_windows_16bit(indices, 4, 2)\n",
    "# #codebook = fp4_s1e2m1_subnormal(windows).cuda()\n",
    "\n",
    "# packed_windows = pack_bits(windows, in_bits=4, out_bits=16)\n",
    "# packed_windows = permute_16bits(packed_windows)\n",
    "# windows = unpack_bits(packed_windows, N=windows.shape[-1], in_bits=4, out_bits=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_bit_windows_16bit(X: torch.Tensor, bit_per_value: int, bit_step: int) -> torch.Tensor:\n",
    "    N = 16\n",
    "    assert bit_per_value <= 8  # output will be packed into int8\n",
    "    num_windows = (N + bit_step - 1) // bit_step\n",
    "    print(\"num windows:\", num_windows)\n",
    "    print(\"bit per window:\", 16 / num_windows)\n",
    "\n",
    "    # Convert to unsigned 32-bit for safe bit shifting and mask to 16 bits\n",
    "    X_u = X.to(torch.int32) & 0xFFFF  # (*X.shape)\n",
    "\n",
    "    # Extract all bits: shape (*X.shape, N)\n",
    "    bit_positions = torch.arange(N - 1, -1, -1, dtype=torch.int32, device=X.device)\n",
    "    bits = ((X_u.unsqueeze(-1) >> bit_positions) & 1).to(torch.uint8)\n",
    "\n",
    "    # Starting positions of windows and bit weights for packing\n",
    "    start = torch.arange(0, N, bit_step, device=X.device)[:num_windows]\n",
    "    pow2 = (1 << torch.arange(bit_per_value - 1, -1, -1, device=X.device)).to(torch.uint8)\n",
    "\n",
    "    # Collect window bits: shape (*X.shape, num_windows, bit_per_value)\n",
    "    idx = (start[:, None] + torch.arange(bit_per_value, device=X.device)) % N\n",
    "    selected_bits = bits[..., idx]\n",
    "\n",
    "    # Pack bits into uint8 values: shape (*X.shape, num_windows)\n",
    "    windows_uint8 = torch.sum(selected_bits * pow2, dim=-1)\n",
    "\n",
    "    return windows_uint8\n",
    "\n",
    "\n",
    "def sliding_bit_windows_8bit(X: torch.Tensor, bit_per_value: int, bit_step: int) -> torch.Tensor:\n",
    "    N = 8\n",
    "    assert bit_per_value <= 8  # output will be packed into int8\n",
    "    num_windows = (N + bit_step - 1) // bit_step\n",
    "    print(\"num windows:\", num_windows)\n",
    "    print(\"bit per window:\", 8 / num_windows)\n",
    "\n",
    "    # Convert to unsigned 32-bit for safe bit shifting and mask to 8 bits\n",
    "    X_u = X.to(torch.int32) & 0xFF  # (*X.shape)\n",
    "\n",
    "    # Extract all bits: shape (*X.shape, N)\n",
    "    bit_positions = torch.arange(N - 1, -1, -1, dtype=torch.int32, device=X.device)\n",
    "    bits = ((X_u.unsqueeze(-1) >> bit_positions) & 1).to(torch.uint8)\n",
    "\n",
    "    # Starting positions of windows and bit weights for packing\n",
    "    start = torch.arange(0, N, bit_step, device=X.device)[:num_windows]\n",
    "    pow2 = (1 << torch.arange(bit_per_value - 1, -1, -1, device=X.device)).to(torch.uint8)\n",
    "\n",
    "    # Collect window bits: shape (*X.shape, num_windows, bit_per_value)\n",
    "    idx = (start[:, None] + torch.arange(bit_per_value, device=X.device)) % N\n",
    "    selected_bits = bits[..., idx]\n",
    "\n",
    "    # Pack bits into uint8 values: shape (*X.shape, num_windows)\n",
    "    windows_uint8 = torch.sum(selected_bits * pow2, dim=-1)\n",
    "\n",
    "    return windows_uint8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scalars(bits):\n",
    "\tvalues = torch.linspace(-3, 3, steps=1<<bits)\n",
    "\treturn values[torch.randperm(1<<bits)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num windows: 8\n",
      "bit per window: 2.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([65536, 8])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = torch.arange(1 << 16)\n",
    "windows = sliding_bit_windows_16bit(indices, 4, 2)\n",
    "\n",
    "# packed_windows = pack_bits(windows, in_bits=4, out_bits=16)\n",
    "# packed_windows = permute_16bits(packed_windows)\n",
    "# windows = unpack_bits(packed_windows, N=windows.shape[-1], in_bits=4, out_bits=16)\n",
    "\n",
    "codebook = fp4_s1e2m1_subnormal(windows).cuda()\n",
    "\n",
    "# indices = torch.arange(1 << 16)\n",
    "# windows = sliding_bit_windows_16bit(indices, 6, 2)\n",
    "# codebook = fp6_s1e3m2_subnormal(windows).cuda()\n",
    "\n",
    "# indices = torch.arange(1 << 16)\n",
    "# windows = sliding_bit_windows_16bit(indices, 8, 3)\n",
    "# # codebook = fp8_s1e4m3_subnormal(windows).cuda()\n",
    "# codebook = torch.randn(1<<8)[windows].cuda()\n",
    "# #codebook = get_scalars(4)[windows].cuda()\n",
    "\n",
    "# indices = torch.arange(1 << 8)\n",
    "# windows = sliding_bit_windows_8bit(indices, 4, 2)\n",
    "# # packed_windows = pack_bits(windows, in_bits=4, out_bits=16)\n",
    "# # packed_windows = permute_16bits(packed_windows)\n",
    "# # windows = unpack_bits(packed_windows, N=windows.shape[-1], in_bits=4, out_bits=16)\n",
    "# codebook = fp4_s1e2m1_subnormal(windows).cuda()\n",
    "\n",
    "# indices = torch.arange(1 << 8)\n",
    "# windows = sliding_bit_windows_8bit(indices, 8, 2)\n",
    "# codebook = torch.randn(1<<8)[windows].cuda()\n",
    "\n",
    "# codebook = torch.randn(2**8, 8).cuda()\n",
    "\n",
    "codebook /= codebook.std()\n",
    "codebook.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse err: tensor(0.1423, device='cuda:0')\n",
      "zeros: tensor(0.0987, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "w = torch.randn(2048, 2048).cuda()\n",
    "\n",
    "w_q, best_indices = vector_quantize_batched(w, codebook, batch_size=1024)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "loss = loss_fn(w, w_q)\n",
    "print(\"mse err:\", loss)\n",
    "\n",
    "print(\"zeros:\", (w_q==0).sum() / (w_q.shape[0] * w_q.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
