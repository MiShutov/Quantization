{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"/home/msst/repo/Quantization\")\n",
    "import qlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "#@torch.compile\n",
    "def decode_compressed(L, K, V, m, n, compressed, expanded_lut):\n",
    "    if compressed.dtype != torch.uint16:\n",
    "        compressed = compressed.view(torch.uint16)\n",
    "\n",
    "    V_flag = int(math.log2(V))\n",
    "\n",
    "    #print(compressed.shape, (K * m * n // 16, ))\n",
    "\n",
    "    assert compressed.shape == (K * m * n // 16, )\n",
    "\n",
    "    BITS_PER_BLOCK = K * 16 * 16  # R bits * f16 mma tile A size\n",
    "\n",
    "    # unswizzle interleaved blocks\n",
    "\n",
    "    BLOCK_SIZE = 16 * 16\n",
    "\n",
    "    BITS_PER_BLOCK = K * 16 * 16  # R bits * f16 mma tile A size\n",
    "\n",
    "    compressed = (compressed.view(torch.uint8).reshape(\n",
    "        m // 16 // 2, n // 16 // 2, BLOCK_SIZE // 8, 2, 2,\n",
    "        K).permute(0, -2, 1, -3, 2, -1).flip(\n",
    "            (-1, )).reshape(m // 16, n // 16, BITS_PER_BLOCK // 16, 2).flip(\n",
    "                (-1, )).view(torch.uint16).reshape(m // 16, n // 16,\n",
    "                                                   BITS_PER_BLOCK // 16))\n",
    "    # decode block\n",
    "\n",
    "    assert L <= 16\n",
    "\n",
    "    blocked = compressed.reshape(K * m * n // BITS_PER_BLOCK,\n",
    "                                 BITS_PER_BLOCK // 16, 1)\n",
    "    blocked_roll = torch.roll(blocked.to(torch.int32), -1,\n",
    "                              -2).to(blocked.dtype)\n",
    "    blocked32 = torch.cat((blocked_roll, blocked),\n",
    "                          dim=-1).reshape(blocked.shape[0],\n",
    "                                          -1).contiguous().view(torch.uint32)\n",
    "    # blocked32 is 16bits[-1]||16bits[0] 16bits[0]||16bits[1] ... 16bits[-2]||16bits[-1]\n",
    "\n",
    "    expanded32 = blocked32.reshape(*blocked32.shape,\n",
    "                                   1).expand(*blocked32.shape,\n",
    "                                             16).view(torch.int32)\n",
    "    shifts = (torch.arange(0, 16, dtype=torch.int32,\n",
    "                           device=blocked.device)).to(torch.int32).reshape(\n",
    "                               1, 1, -1).expand(expanded32.shape)\n",
    "    shifted = expanded32 >> (16 - shifts)\n",
    "    indices = torch.bitwise_and(\n",
    "        shifted.reshape(shifted.shape[0], -1)[:, 16 - L::K << V_flag], (1 << L) - 1)\n",
    "\n",
    "    # decode lut\n",
    "    mma_swizzled = expanded_lut[indices]\n",
    "\n",
    "    # deswizzle m16n8k16 mma pattern\n",
    "    decompressed = (mma_swizzled.reshape(m // 16, n // 16, 16, 16).reshape(\n",
    "        m // 16, n // 16, 8, 4, 2, 2, 2).permute(0, -2, 2, 1, -3, 3,\n",
    "                                                 -1).reshape(m, n))\n",
    "    return decompressed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 4096 // 4\n",
    "#n = 11008 // 4\n",
    "n = 4096 // 4\n",
    "\n",
    "w = torch.randn(m, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "qparams = qlib.TrellisQuantizerParams(\n",
    "\tT = 256, L=16, V=2, K=2, decode_mode=\"Rand2d\"\n",
    ")\n",
    "\n",
    "quatizer = qlib.TrellisQuantizer(qparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:01<00:00,  8.25it/s]\n",
      "100%|██████████| 16/16 [00:01<00:00, 10.13it/s]\n"
     ]
    }
   ],
   "source": [
    "w = w.to('cuda')\n",
    "quatizer = quatizer.to('cuda')\n",
    "\n",
    "wq, state = quatizer.quantize(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "packed = quatizer.pack_trellis(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "packed = packed.view(torch.uint8).view(-1, 2).flip(\n",
    "                (-1, )).reshape(m // 16 // 2, 2, n // 16 // 2, 2, 16 * 16 // 8,\n",
    "                                qparams.K).permute(0, 2, 4, 3, 1, 5).flip(\n",
    "                                    (-1, )).contiguous().flatten().view(\n",
    "                                        torch.int16).reshape(packed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096, 32])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "packed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "unpacked = decode_compressed(\n",
    "\tL=qparams.L, \n",
    "\tK=qparams.K,\n",
    "\tV=qparams.V,\n",
    "\tm=m,\n",
    "\tn=n,\n",
    "\tcompressed=packed.flatten(),\n",
    "\texpanded_lut=quatizer.lut\n",
    "\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.7818e-01, -3.6281e-01,  1.3887e-01,  ..., -4.7790e-01,\n",
       "         -1.2870e+00, -1.2668e+00],\n",
       "        [-6.1352e-01, -1.6406e+00,  2.9137e-01,  ..., -4.6438e-01,\n",
       "          3.1621e-01,  8.1235e-01],\n",
       "        [ 8.0103e-01, -2.3225e+00,  3.6668e-01,  ...,  4.6468e-01,\n",
       "         -3.6813e-01, -1.3651e+00],\n",
       "        ...,\n",
       "        [-1.0677e+00,  1.6957e+00,  1.2529e-01,  ..., -1.8370e-01,\n",
       "         -1.7721e-03, -1.8435e-01],\n",
       "        [ 1.1563e+00, -8.6044e-01,  1.2589e+00,  ..., -2.7057e-01,\n",
       "         -2.8953e-01, -1.0476e-02],\n",
       "        [-1.4377e+00,  2.0027e-01, -2.6547e+00,  ...,  1.0195e+00,\n",
       "         -2.6182e-02, -4.5766e-01]], device='cuda:0')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2782, -0.3628,  0.7109,  ...,  0.1162, -0.3886, -0.9662],\n",
       "        [ 0.0505,  0.7295,  0.0432,  ..., -1.0157,  0.0133, -0.6968],\n",
       "        [ 2.9878, -0.4455, -0.6134,  ..., -1.0280, -0.3564,  0.0493],\n",
       "        ...,\n",
       "        [-0.2500,  0.6626,  0.4132,  ..., -0.4027, -0.2010, -1.3244],\n",
       "        [-0.1092, -0.6290,  1.3971,  ...,  0.2196,  1.0638,  0.2044],\n",
       "        [-0.4141, -0.7849, -1.3011,  ...,  1.3192, -0.0262, -0.4577]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unpacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11, 12, 13, 14, 15],\n",
      "        [16, 17, 18, 19, 20, 21, 22, 23],\n",
      "        [24, 25, 26, 27, 28, 29, 30, 31],\n",
      "        [32, 33, 34, 35, 36, 37, 38, 39],\n",
      "        [40, 41, 42, 43, 44, 45, 46, 47],\n",
      "        [48, 49, 50, 51, 52, 53, 54, 55],\n",
      "        [56, 57, 58, 59, 60, 61, 62, 63]])\n"
     ]
    }
   ],
   "source": [
    "size = 8\n",
    "m, n = size, size\n",
    "tp_rank = 2\n",
    "\n",
    "W = torch.arange(size**2).reshape(size, size)\n",
    "\n",
    "Wr = W\n",
    "print(Wr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 16])\n",
      "tensor([[ 0,  1,  2,  3,  8,  9, 10, 11, 16, 17, 18, 19, 24, 25, 26, 27],\n",
      "        [ 4,  5,  6,  7, 12, 13, 14, 15, 20, 21, 22, 23, 28, 29, 30, 31],\n",
      "        [32, 33, 34, 35, 40, 41, 42, 43, 48, 49, 50, 51, 56, 57, 58, 59],\n",
      "        [36, 37, 38, 39, 44, 45, 46, 47, 52, 53, 54, 55, 60, 61, 62, 63]])\n"
     ]
    }
   ],
   "source": [
    "td_x = td_y = 4\n",
    "\n",
    "Wr_ = Wr.reshape(m // td_x, td_x, n // td_y, td_y).transpose(1, 2).reshape(-1, td_x * td_y)\n",
    "\n",
    "print(Wr_.shape)\n",
    "print(Wr_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 8\u001b[0m\n\u001b[1;32m      3\u001b[0m has_kernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_kernel:\n\u001b[1;32m      6\u001b[0m     packed \u001b[38;5;241m=\u001b[39m packed\u001b[38;5;241m.\u001b[39mview(torch\u001b[38;5;241m.\u001b[39muint8)\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mflip(\n\u001b[1;32m      7\u001b[0m         (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, ))\u001b[38;5;241m.\u001b[39mreshape(m \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m16\u001b[39m \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, n \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m16\u001b[39m \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m16\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m16\u001b[39m \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m8\u001b[39m,\n\u001b[0;32m----> 8\u001b[0m                         \u001b[43margs\u001b[49m\u001b[38;5;241m.\u001b[39mK)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m5\u001b[39m)\u001b[38;5;241m.\u001b[39mflip(\n\u001b[1;32m      9\u001b[0m                             (\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, ))\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mflatten()\u001b[38;5;241m.\u001b[39mview(\n\u001b[1;32m     10\u001b[0m                                 torch\u001b[38;5;241m.\u001b[39mint16)\u001b[38;5;241m.\u001b[39mreshape(packed\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "# qtip/lib/algo/finetune.py\n",
    "\n",
    "has_kernel = True\n",
    "\n",
    "if has_kernel:\n",
    "    packed = packed.view(torch.uint8).view(-1, 2).flip(\n",
    "        (-1, )).reshape(m // 16 // 2, 2, n // 16 // 2, 2, 16 * 16 // 8,\n",
    "                        args.K).permute(0, 2, 4, 3, 1, 5).flip(\n",
    "                            (-1, )).contiguous().flatten().view(\n",
    "                                torch.int16).reshape(packed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "libc10.so: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mqtip_kernels\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcuda\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cuda\n",
      "\u001b[0;31mImportError\u001b[0m: libc10.so: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import qtip_kernels\n",
    "import torch\n",
    "from cuda import cuda\n",
    "\n",
    "#from tinygrad import Device\n",
    "\n",
    "kernels = {\n",
    "    2: {\n",
    "        (256, 1, 256): qtip_kernels.decompress_matvec_16_9_2_1_256_1_256,\n",
    "        (4096, 1, 4096): qtip_kernels.decompress_matvec_16_9_2_1_4096_1_4096,\n",
    "        (4096, 1, 11008): qtip_kernels.decompress_matvec_16_9_2_1_4096_1_11008,\n",
    "        (11008, 1, 4096): qtip_kernels.decompress_matvec_16_9_2_1_11008_1_4096,\n",
    "        (8192, 1, 8192): qtip_kernels.decompress_matvec_16_9_2_1_8192_1_8192,\n",
    "        (1024, 1, 8192): qtip_kernels.decompress_matvec_16_9_2_1_1024_1_8192,\n",
    "        (8192, 1, 28672): qtip_kernels.decompress_matvec_16_9_2_1_8192_1_28672,\n",
    "        (28672, 1, 8192): qtip_kernels.decompress_matvec_16_9_2_1_28672_1_8192,\n",
    "    },\n",
    "    3: {\n",
    "        (4096, 1, 4096): qtip_kernels.decompress_matvec_16_9_3_1_4096_1_4096,\n",
    "        (4096, 1, 11008): qtip_kernels.decompress_matvec_16_9_3_1_4096_1_11008,\n",
    "        (11008, 1, 4096): qtip_kernels.decompress_matvec_16_9_3_1_11008_1_4096,\n",
    "        (8192, 1, 8192): qtip_kernels.decompress_matvec_16_9_3_1_8192_1_8192,\n",
    "        (1024, 1, 8192): qtip_kernels.decompress_matvec_16_9_3_1_1024_1_8192,\n",
    "        (8192, 1, 28672): qtip_kernels.decompress_matvec_16_9_3_1_8192_1_28672,\n",
    "        (28672, 1, 8192): qtip_kernels.decompress_matvec_16_9_3_1_28672_1_8192,\n",
    "    },\n",
    "    4: {\n",
    "        (4096, 1, 4096): qtip_kernels.decompress_matvec_16_9_4_1_4096_1_4096,\n",
    "        (4096, 1, 11008): qtip_kernels.decompress_matvec_16_9_4_1_4096_1_11008,\n",
    "        (11008, 1, 4096): qtip_kernels.decompress_matvec_16_9_4_1_11008_1_4096,\n",
    "        (8192, 1, 8192): qtip_kernels.decompress_matvec_16_9_4_1_8192_1_8192,\n",
    "        (1024, 1, 8192): qtip_kernels.decompress_matvec_16_9_4_1_1024_1_8192,\n",
    "        (8192, 1, 28672): qtip_kernels.decompress_matvec_16_9_4_1_8192_1_28672,\n",
    "        (28672, 1, 8192): qtip_kernels.decompress_matvec_16_9_4_1_28672_1_8192,\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "#dev = Device[Device.DEFAULT]\n",
    "def time_kernel(kernel):\n",
    "    ITER = int(os.getenv(\"ITER\", \"100\"))\n",
    "    zero_buf = torch.empty(128 * (1024**2), dtype=torch.int8, device='cuda')\n",
    "    # capture CUDA graph\n",
    "    start_events = [cuda.cuEventCreate(0)[1] for _ in range(ITER)]\n",
    "    end_events = [cuda.cuEventCreate(0)[1] for _ in range(ITER)]\n",
    "    graph = torch.cuda.CUDAGraph()\n",
    "    with torch.cuda.graph(graph):\n",
    "        stream = torch.cuda.current_stream().cuda_stream\n",
    "        cuda.cuEventRecordWithFlags(start_events[0], stream, 1)\n",
    "        kernel()\n",
    "        cuda.cuEventRecordWithFlags(end_events[0], stream, 1)\n",
    "        zero_buf.zero_()\n",
    "\n",
    "    elapsed_time_ms = 0.0\n",
    "    for _ in range(ITER):\n",
    "        graph.replay()\n",
    "        torch.cuda.synchronize()\n",
    "        #        dev.invalidate_caches()\n",
    "        torch.cuda.synchronize()\n",
    "        elapsed_time_ms += cuda.cuEventElapsedTime(start_events[0],\n",
    "                                                   end_events[0])[1]\n",
    "\n",
    "    #elapsed_time_ms = sum(cuda.cuEventElapsedTime(se, ee)[1] for se, ee in zip(start_events, end_events))\n",
    "    return elapsed_time_ms / ITER\n",
    "\n",
    "\n",
    "def time_kernel_dual(kernel1, kernel2):\n",
    "    ITER = int(os.getenv(\"ITER\", \"100\"))\n",
    "    # capture CUDA graph\n",
    "    start_events = [cuda.cuEventCreate(0)[1] for _ in range(ITER)]\n",
    "    end_events = [cuda.cuEventCreate(0)[1] for _ in range(ITER)]\n",
    "    graph = torch.cuda.CUDAGraph()\n",
    "    with torch.cuda.graph(graph):\n",
    "        stream = torch.cuda.current_stream().cuda_stream\n",
    "        for i in range(ITER):\n",
    "            cuda.cuEventRecordWithFlags(start_events[i], stream, 1)\n",
    "            if i % 2 == 0:\n",
    "                kernel1()\n",
    "            else:\n",
    "                kernel2()\n",
    "            cuda.cuEventRecordWithFlags(end_events[i], stream, 1)\n",
    "\n",
    "    graph.replay()\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    elapsed_time_ms = sum(\n",
    "        cuda.cuEventElapsedTime(se, ee)[1]\n",
    "        for se, ee in zip(start_events, end_events))\n",
    "    return elapsed_time_ms / ITER\n",
    "\n",
    "\n",
    "def quipsharp_time(M, N, K):\n",
    "    import quiptools_cuda\n",
    "    x = torch.randn((K, ), dtype=torch.float16, device=\"cuda\")\n",
    "    x2 = torch.randn((K, ), dtype=torch.float16, device=\"cuda\")\n",
    "    Qidxs = torch.randint(0x7FFFFFFFFFFFFFFF, (M // 16, K // 64, 8, 4),\n",
    "                          dtype=torch.int64,\n",
    "                          device=\"cuda\")\n",
    "    Qidxs2 = torch.randint(0x7FFFFFFFFFFFFFFF, (M // 16, K // 64, 8, 4),\n",
    "                           dtype=torch.int64,\n",
    "                           device=\"cuda\")\n",
    "    codebook = torch.randint(0x7fffffff, (256, ),\n",
    "                             dtype=torch.int32,\n",
    "                             device=\"cuda\")\n",
    "    codebook2 = torch.randint(0x7fffffff, (256, ),\n",
    "                              dtype=torch.int32,\n",
    "                              device=\"cuda\")\n",
    "\n",
    "    elapsed_time_ms = time_kernel_dual(\n",
    "        lambda: quiptools_cuda.decode_matvec_e8p(x, Qidxs, codebook),\n",
    "        lambda: quiptools_cuda.decode_matvec_e8p(x2, Qidxs2, codebook2))\n",
    "    gbps = (Qidxs.nbytes / (10**9)) / (elapsed_time_ms / 1000)\n",
    "    print(f\"quip# {(M, N, K)}: {elapsed_time_ms * 1000:.2f}us {gbps:.1f} GBps\")\n",
    "\n",
    "\n",
    "def time_qs_kernels():\n",
    "    for M, N, K in kernels[2]:\n",
    "        quipsharp_time(M, N, K)\n",
    "\n",
    "\n",
    "def decompress_matvec_time(R, args1, args2):\n",
    "    out, compressed, x, codebook = args1\n",
    "    m, n = out.shape\n",
    "    k, n = x.shape\n",
    "    kernel = kernels[R][(m, n, k)]\n",
    "\n",
    "    elapsed_time_ms = time_kernel_dual(lambda: kernel(*args1),\n",
    "                                       lambda: kernel(*args2))\n",
    "\n",
    "    gbps = (compressed.nbytes / (10**9)) / (elapsed_time_ms / 1000)\n",
    "    print(\n",
    "        f\"{R}bit {(m, n, k)}: {elapsed_time_ms * 1000:.2f}us {gbps:.1f} GBps\")\n",
    "\n",
    "\n",
    "def decompress_matvec(R, out, compressed, x, codebook):\n",
    "    m, n = out.shape\n",
    "    k, n = x.shape\n",
    "    kernel = kernels[R][(m, n, k)]\n",
    "    kernel(out, compressed, x, codebook)\n",
    "\n",
    "\n",
    "def prepare_arguments_sanity(L, S, R, V, m, n, k):\n",
    "    out = torch.zeros((m, n), dtype=torch.float32,\n",
    "                      device=\"cuda\")  # we require zero-initialization\n",
    "    # NOTE: all zero so no top bit flips\n",
    "    compressed = torch.full((R * m * k // 32, ),\n",
    "                            0,\n",
    "                            dtype=torch.int32,\n",
    "                            device=\"cuda\")\n",
    "    \"\"\"\n",
    "    compressed = torch.randint(torch.iinfo(torch.int32).min,\n",
    "                               torch.iinfo(torch.int32).max+1,\n",
    "                               (R * m * k // 32,),\n",
    "                               dtype=torch.int32,\n",
    "                               device=\"cpu\").cuda()\n",
    "                               \"\"\"\n",
    "    x = torch.ones((k, n), dtype=torch.float16, device=\"cpu\").cuda()\n",
    "    codebook = torch.full((1 << (S + V), ),\n",
    "                          1 / k / n,\n",
    "                          dtype=torch.float16,\n",
    "                          device=\"cpu\").cuda()\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    return out, compressed, x, codebook\n",
    "\n",
    "\n",
    "def sanity_check(L, S, R, V):\n",
    "    for m, n, k in kernels[R]:\n",
    "        out, compressed, x, codebook = prepare_arguments_sanity(\n",
    "            L, S, R, V, m, n, k)\n",
    "        decompressed = torch.ones(m, k, dtype=torch.float16,\n",
    "                                  device=\"cpu\").cuda() / k / n\n",
    "        decompress_matvec(R, out, compressed, x, codebook)\n",
    "        print(\"sanity check\", m, n, k,\n",
    "              torch.sum(out).item(), \"=\",\n",
    "              torch.sum(decompressed @ x).item())\n",
    "\n",
    "\n",
    "def quantlut_sym(tlut, L, nbits):\n",
    "    with torch.no_grad():\n",
    "        lut = torch.arange(1 << L, device=tlut.device)\n",
    "        lut = (lut + 1) * lut\n",
    "        sflp = 1 - ((lut >> 15) & 1) * 2\n",
    "        lut = (lut >> (16 - nbits - 1)) & ((1 << nbits) - 1)\n",
    "    lut = tlut[lut]\n",
    "    lut[:, 0] = lut[:, 0] * sflp\n",
    "    return lut\n",
    "\n",
    "\n",
    "def decode_compressed(L, S, R, V, m, k, compressed, codebook):\n",
    "    if compressed.dtype != torch.uint16:\n",
    "        compressed = compressed.view(torch.uint16)\n",
    "\n",
    "    assert compressed.shape == (R * m * k // 16, )\n",
    "\n",
    "    BLOCK_SIZE = 16 * 16\n",
    "\n",
    "    BITS_PER_BLOCK = R * 16 * 16  # R bits * f16 mma tile A size\n",
    "\n",
    "    compressed = (\n",
    "        compressed.view(torch.uint8).reshape(m // 16 // 2, k // 16 // 2,\n",
    "                                             BLOCK_SIZE // 8, 2, 2,\n",
    "                                             R).permute(0, -2, 1, -3, 2, -1)\n",
    "        # big endian across words, little endian within words ...\n",
    "        .flip((-1, )).reshape(m // 16, k // 16, BITS_PER_BLOCK // 16, 2).flip(\n",
    "            (-1, )).view(torch.uint16).reshape(m // 16, k // 16,\n",
    "                                               BITS_PER_BLOCK // 16))\n",
    "    '''\n",
    "    # unswizzle interleaved blocks\n",
    "\n",
    "    compressed = (compressed.reshape(m // 16 // 2, k // 16 // 2,\n",
    "                                     BITS_PER_BLOCK // 16, 2, 2).permute(\n",
    "                                         0, -1, 1, -2,\n",
    "                                         2).reshape(m // 16, k // 16,\n",
    "                                                    BITS_PER_BLOCK // 16))\n",
    "    '''\n",
    "    # decode block\n",
    "\n",
    "    assert L == 16\n",
    "\n",
    "    blocked = compressed.reshape(R * m * k // BITS_PER_BLOCK,\n",
    "                                 BITS_PER_BLOCK // 16, 1)\n",
    "    blocked_roll = torch.roll(blocked.cpu(), -1, -2).cuda()\n",
    "    blocked32 = torch.cat((blocked_roll, blocked),\n",
    "                          dim=-1).reshape(blocked.shape[0],\n",
    "                                          -1).contiguous().view(torch.uint32)\n",
    "    # blocked32 is 16bits[-1]||16bits[0] 16bits[0]||16bits[1] ... 16bits[-2]||16bits[-1]\n",
    "\n",
    "    expanded32 = blocked32.reshape(*blocked32.shape,\n",
    "                                   1).expand(*blocked32.shape,\n",
    "                                             16).view(torch.int32)\n",
    "    shifts = (torch.arange(0, 16, dtype=torch.int32, device=\"cuda\")).to(\n",
    "        torch.int32).reshape(1, 1, -1).expand(expanded32.shape)\n",
    "    shifted = expanded32 >> (16 - shifts)\n",
    "    indices = torch.bitwise_and(\n",
    "        shifted.reshape(shifted.shape[0], -1)[:, 16 - L::R << V], (1 << L) - 1)\n",
    "\n",
    "    # decode lut\n",
    "    expanded_lut = quantlut_sym(codebook, L, S)\n",
    "    mma_swizzled = expanded_lut[indices]\n",
    "\n",
    "    # deswizzle m16n8k16 mma pattern\n",
    "    decompressed = (mma_swizzled.reshape(m // 16, k // 16, 16, 16).reshape(\n",
    "        m // 16, k // 16, 8, 4, 2, 2, 2).permute(0, -2, 2, 1, -3, 3,\n",
    "                                                 -1).reshape(m, k))\n",
    "    return decompressed\n",
    "\n",
    "\n",
    "def prepare_arguments(L, S, R, V, m, n, k):\n",
    "    out = torch.zeros((m, n), dtype=torch.float32,\n",
    "                      device=\"cuda\")  # we require zero-initialization\n",
    "    #codebook = torch.full((1<<(S+V),), 1, dtype=torch.float16, device=\"cpu\").cuda()\n",
    "    codebook = (torch.randn(\n",
    "        (1 << (S + V)), dtype=torch.float16, device=\"cpu\").cuda() / 16).clamp(\n",
    "            -1, 1)\n",
    "    compressed = torch.randint(torch.iinfo(torch.int32).min,\n",
    "                               torch.iinfo(torch.int32).max + 1,\n",
    "                               (R * m * k // 32, ),\n",
    "                               dtype=torch.int32,\n",
    "                               device=\"cpu\").cuda()\n",
    "    x = (torch.randn(\n",
    "        (k, n), dtype=torch.float16, device=\"cpu\").cuda() / 16).clamp(-1, 1)\n",
    "    #x = torch.zeros((k, n), dtype=torch.float16, device=\"cuda\")\n",
    "    #x[4,0] = 1.0\n",
    "    x = x.contiguous()\n",
    "\n",
    "    decompressed = decode_compressed(L, S, R, V, m, k, compressed,\n",
    "                                     codebook.reshape(1 << S, 1 << V))\n",
    "\n",
    "    return out, compressed, x, codebook, decompressed\n",
    "\n",
    "\n",
    "def test_kernels(L, S, R, V):\n",
    "    torch.set_printoptions(threshold=10_000)\n",
    "    for m, n, k in kernels[R]:\n",
    "        out, compressed, x, codebook, decompressed = prepare_arguments(\n",
    "            L, S, R, V, m, n, k)\n",
    "        if not os.getenv(\"TIMING\"):\n",
    "            decompress_matvec(R, out, compressed, x, codebook)\n",
    "        else:\n",
    "            decompress_matvec_time(R, (out, compressed, x, codebook),\n",
    "                                   prepare_arguments(L, S, R, V, m, n, k)[:4])\n",
    "        if not os.getenv(\"NOCHECK\"):\n",
    "            ref = (decompressed @ x)\n",
    "            allclose = torch.allclose(out.half(), ref, atol=1e-5, rtol=0.01)\n",
    "            if not allclose:\n",
    "                print(torch.stack((ref[:16], out[:16]), dim=-1))\n",
    "            try:\n",
    "                torch.testing.assert_allclose(out.half(),\n",
    "                                              ref,\n",
    "                                              atol=1e-5,\n",
    "                                              rtol=0.01)\n",
    "            except:\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "                exit()\n",
    "            print(\"real test\", m, n, k,\n",
    "                  torch.sum(out).item(), \"=\",\n",
    "                  torch.sum(decompressed @ x).item(), \"allclose\", allclose)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    torch.manual_seed(42)\n",
    "    L, S, V = 16, 9, 1\n",
    "    for R in range(2, 5):\n",
    "        print(R)\n",
    "        #sanity_check(L, S, R, V)\n",
    "        test_kernels(L, S, R, V)\n",
    "    if os.getenv(\"QS\"): time_qs_kernels()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
