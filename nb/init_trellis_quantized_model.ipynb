{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import torch\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/home/msst/repo/Quantization\")\n",
    "import qlib\n",
    "\n",
    "model_name='Llama2-7b-hf'\n",
    "#model_name='Llama-3.2-1B'\n",
    "\n",
    "DTYPE = torch.float16\n",
    "model = qlib.load_custom_llama(\n",
    "\tmodel_name=model_name, \n",
    "\tdevice_map='cpu', \n",
    "\ttorch_dtype=DTYPE, \n",
    "\t#use_flash_attn=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/16 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 11.64 GiB of which 9.28 GiB is free. Including non-PyTorch memory, this process has 2.34 GiB memory in use. Of the allocated memory 1.22 GiB is allocated by PyTorch, and 1.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 30\u001b[0m\n\u001b[1;32m     11\u001b[0m incoh_proc_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqtip\u001b[39m\u001b[38;5;124m'\u001b[39m \n\u001b[1;32m     13\u001b[0m wrapper \u001b[38;5;241m=\u001b[39m qlib\u001b[38;5;241m.\u001b[39mHomeQuantWrapper(\n\u001b[1;32m     14\u001b[0m     wrap_rule\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m     15\u001b[0m         torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mLinear : qlib\u001b[38;5;241m.\u001b[39mqlayers\u001b[38;5;241m.\u001b[39mVQ2SQ\u001b[38;5;241m.\u001b[39mTrellisLinear(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     }\n\u001b[1;32m     29\u001b[0m )\n\u001b[0;32m---> 30\u001b[0m \u001b[43mwrapper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m model_checkpoints_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/msst/repo/Quantization/logs/checkpoints_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/trellis\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     33\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(model_checkpoints_path, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/repo/Quantization/qlib/wrappers/homequant_wrapper.py:25\u001b[0m, in \u001b[0;36mHomeQuantWrapper.wrap_model\u001b[0;34m(self, current_module, prefix)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[38;5;28msetattr\u001b[39m(current_module, module_name, new_module)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repo/Quantization/qlib/wrappers/homequant_wrapper.py:25\u001b[0m, in \u001b[0;36mHomeQuantWrapper.wrap_model\u001b[0;34m(self, current_module, prefix)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[38;5;28msetattr\u001b[39m(current_module, module_name, new_module)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping similar frames: HomeQuantWrapper.wrap_model at line 25 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/repo/Quantization/qlib/wrappers/homequant_wrapper.py:25\u001b[0m, in \u001b[0;36mHomeQuantWrapper.wrap_model\u001b[0;34m(self, current_module, prefix)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[38;5;28msetattr\u001b[39m(current_module, module_name, new_module)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repo/Quantization/qlib/wrappers/homequant_wrapper.py:22\u001b[0m, in \u001b[0;36mHomeQuantWrapper.wrap_model\u001b[0;34m(self, current_module, prefix)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap_rule[module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 22\u001b[0m             new_module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_rule\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrap_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m             \u001b[38;5;28msetattr\u001b[39m(current_module, module_name, new_module)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/qenv/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repo/Quantization/qlib/qlayers/VQ2SQ.py:69\u001b[0m, in \u001b[0;36mTrellisLinear.wrap_module\u001b[0;34m(self, module, verbose, *args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSV \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt(w_std)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# Quantize\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m reco, states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m     71\u001b[0m     err \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean((reco \u001b[38;5;241m-\u001b[39m w)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/repo/Quantization/qlib/qlayers/trellis_quantizer.py:239\u001b[0m, in \u001b[0;36mtrellis_quantizer.quantize\u001b[0;34m(self, X, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;66;03m# # Set vector as 16 x 16 patch\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;66;03m# patch_size = 16\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;66;03m# X = X.unfold(0, patch_size, patch_size).unfold(1, patch_size, patch_size)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    236\u001b[0m \n\u001b[1;32m    237\u001b[0m \u001b[38;5;66;03m# Fisrt fase\u001b[39;00m\n\u001b[1;32m    238\u001b[0m roll_X \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mroll(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mV) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mV, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 239\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquantize_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroll_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverlap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m overlap \u001b[38;5;241m=\u001b[39m state[:, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mT \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mV)] \u001b[38;5;241m>>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mK \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mV\n\u001b[1;32m    241\u001b[0m \u001b[38;5;66;03m# Second fase\u001b[39;00m\n",
      "File \u001b[0;32m~/repo/Quantization/qlib/qlayers/trellis_quantizer.py:221\u001b[0m, in \u001b[0;36mtrellis_quantizer.quantize_seq\u001b[0;34m(self, X, overlap, **kwargs)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X))):\n\u001b[1;32m    220\u001b[0m     overlap_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m overlap \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m overlap[i]\n\u001b[0;32m--> 221\u001b[0m     Qidxs[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mviterbi\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverlap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverlap_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    222\u001b[0m Qidxs \u001b[38;5;241m=\u001b[39m Qidxs\u001b[38;5;241m.\u001b[39mreshape(n_seq_padded, T \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mV)[:n_seq]\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Qidxs\n",
      "File \u001b[0;32m~/repo/Quantization/qlib/qlayers/trellis_quantizer.py:178\u001b[0m, in \u001b[0;36mtrellis_quantizer.viterbi\u001b[0;34m(self, X, overlap)\u001b[0m\n\u001b[1;32m    175\u001b[0m     cost \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmin(cost \u001b[38;5;241m+\u001b[39m mask, fakeinf)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# Time-major storage for efficient backtrace\u001b[39;00m\n\u001b[0;32m--> 178\u001b[0m from_state \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43mT_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mL\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mK\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mV\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlong\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, T_v):\n\u001b[1;32m    182\u001b[0m     obs \u001b[38;5;241m=\u001b[39m X[:, i\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mV:(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mV]\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 16.00 GiB. GPU 0 has a total capacity of 11.64 GiB of which 9.28 GiB is free. Including non-PyTorch memory, this process has 2.34 GiB memory in use. Of the allocated memory 1.22 GiB is allocated by PyTorch, and 1.00 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import qlib.qlayers\n",
    "import qlib.qlayers.VQ2SQ\n",
    "\n",
    "\n",
    "L=16\n",
    "K=2\n",
    "V=2\n",
    "T=256\n",
    "tlut_bits=10\n",
    "decode_mode='LowBitSym' \n",
    "incoh_proc_mode='qtip' \n",
    "\n",
    "wrapper = qlib.HomeQuantWrapper(\n",
    "    wrap_rule={\n",
    "        torch.nn.Linear : qlib.qlayers.VQ2SQ.TrellisLinear(\n",
    "\t\t\tT=T, \n",
    "            L=L, \n",
    "            V=V, \n",
    "            K=K,\n",
    "\t\t\ttlut_bits=tlut_bits,\n",
    "            decode_mode=decode_mode, \n",
    "            incoh_proc_mode=incoh_proc_mode,\n",
    "\t\t\t#viterby_bs=4096,\n",
    "\t\t),\n",
    "    },\n",
    "    exceptions = {\n",
    "        'lm_head' : None,\n",
    "    }\n",
    ")\n",
    "wrapper.wrap_model(model)\n",
    "\n",
    "model_checkpoints_path = f'/home/msst/repo/Quantization/logs/checkpoints_{model_name}/trellis'\n",
    "os.makedirs(model_checkpoints_path, exist_ok=True)\n",
    "torch.save(model, os.path.join(model_checkpoints_path, f'T{T}_L{L}_V{V}_K{K}_lbits{tlut_bits}_{decode_mode}_{incoh_proc_mode}.pth'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
