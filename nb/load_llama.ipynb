{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20f31df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/msst/Utils/miniconda3/envs/qenv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17ed0fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:39<00:00, 19.54s/it]\n"
     ]
    }
   ],
   "source": [
    "# FP\n",
    "path_to_model = \"/media/msst/ssd_storage1/ml/llm/pretrained_models/Llama2-7B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(path_to_model)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    path_to_model,\n",
    "    device_map=\"cuda\",\n",
    "    dtype=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c80d39c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = '''\n",
    "User prompt:\n",
    "\n",
    "Please write a small story about boy, who start his way in LLM quantization. It should be funny, but you must use a lot of ML terminology.\n",
    "\n",
    "Answer:\n",
    "'''\n",
    "max_new_tokens = 1024\n",
    "model_inputs = tokenizer([prompt,], return_tensors=\"pt\").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77c6aed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User prompt:\n",
      "\n",
      "Please write a small story about boy, who start his way in LLM quantization. It should be funny, but you must use a lot of ML terminology.\n",
      "\n",
      "Answer:\n",
      "\n",
      "The boy was excited to start his way in LLM quantization. He had been studying for months and was finally ready to put his knowledge to the test.\n",
      "\n",
      "He started by reading the documentation and learning about the different types of quantization. He then began to experiment with different algorithms and techniques to see which ones worked best for his data.\n",
      "\n",
      "As he continued to work on his project, he began to see some promising results. He was able to reduce the size of his data while still maintaining its accuracy. He was also able to improve the speed of his algorithm by using more efficient techniques.\n",
      "\n",
      "The boy was thrilled with his progress and was excited to continue working on his project. He knew that with more practice and experience, he would be able to create even better results.\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n",
      "\n",
      "###\n"
     ]
    }
   ],
   "source": [
    "generated_ids = model.generate(**model_inputs, max_new_tokens=max_new_tokens)\n",
    "model_outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(model_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7477af8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17748b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quant\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/home/msst/repo/Quantization\")\n",
    "import qlib\n",
    "\n",
    "path_to_model = \"/home/msst/repo/Quantization/logs/Llama2-7B_w8a8\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(path_to_model)\n",
    "\n",
    "# model = qlib.QuantizedLlamaForCausalLM.from_pretrained(\n",
    "#     path_to_model,\n",
    "#     device_map=\"cuda\",\n",
    "#     dtype=\"auto\"\n",
    "# )\n",
    "\n",
    "\n",
    "# model = AutoModel.from_pretrained(\n",
    "#     path_to_model,\n",
    "#     device_map=\"cuda\",\n",
    "#     dtype=\"auto\"\n",
    "# )\n",
    "\n",
    "config = AutoConfig.from_pretrained(path_to_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d133ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96f8c8cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User prompt:\n",
      "\n",
      "Please write a small story about boy, who start his way in LLM quantization. It should be funny, but you must use a lot of ML terminology.\n",
      "\n",
      "Answer:\n",
      "\n",
      "The boy was excited to start his way in LLM quantization. He had been studying for months and was finally ready to put his knowledge to use.\n",
      "\n",
      "The first step was to choose the right model. There were many different options, but the boy decided to go with a transformer-based model. He spent hours training the model and fine-tuning the hyperparameters.\n",
      "\n",
      "Once the model was trained, the boy was ready to start quantizing it. He used a technique called quantization-aware training to ensure that the model would be able to run on a low-power device.\n",
      "\n",
      "The boy was thrilled with the results. His model was able to run on a low-power device and still perform well. He was excited to see what he could do with his newfound skills.\n",
      "\n",
      "The boy was excited to start his way in LLM quantization. He had been studying for months and was finally ready to put his knowledge to use.\n",
      "\n",
      "The first step was to choose the right model. There were many different options, but the boy decided to go with a transformer-based model. He spent hours training the model and fine-tuning the hyperparameters.\n",
      "\n",
      "Once the model was trained, the boy was ready to start quantizing it. He used a technique called quantization-aware training to ensure that the model would be able to run on a low-power device.\n",
      "\n",
      "The boy was thrilled with the results. His model was able to run on a low-power device and still perform well. He was excited to see what he could do with his newfound skills.\n",
      "\n",
      "The boy was excited to start his way in LLM quantization. He had been studying for months and was finally ready to put his knowledge to use.\n",
      "\n",
      "The first step was to choose the right model. There were many different options, but the boy decided to go with a transformer-based model. He spent hours training the model and fine-tuning the hyperparameters.\n",
      "\n",
      "Once the model was trained, the boy was ready to start quantizing it. He used a technique called quantization-aware training to ensure that the model would be able to run on a low-power device.\n",
      "\n",
      "The boy was thrilled with the results. His model was able to run on a low-power device and still perform well. He was excited to see what he could do with his newfound skills.\n",
      "\n",
      "The boy was excited to start his way in LLM quantization. He had been studying for months and was finally ready to put his knowledge to use.\n",
      "\n",
      "The first step was to choose the right model. There were many different options, but the boy decided to go with a transformer-based model. He spent hours training the model and fine-tuning the hyperparameters.\n",
      "\n",
      "Once the model was trained, the boy was ready to start quantizing it. He used a technique called quantization-aware training to ensure that the model would be able to run on a low-power device.\n",
      "\n",
      "The boy was thrilled with the results. His model was able to run on a low-power device and still perform well. He was excited to see what he could do with his newfound skills.\n",
      "\n",
      "The boy was excited to start his way in LLM quantization. He had been studying for months and was finally ready to put his knowledge to use.\n",
      "\n",
      "The first step was to choose the right model. There were many different options, but the boy decided to go with a transformer-based model. He spent hours training the model and fine-tuning the hyperparameters.\n",
      "\n",
      "Once the model was trained, the boy was ready to start quantizing it. He used a technique called quantization-aware training to ensure that the model would be able to run on a low-power device.\n",
      "\n",
      "The boy was thrilled with the results. His model was able to run on a low-power device and still perform well. He was excited to see what he could do with his newfound skills.\n",
      "\n",
      "The boy was excited to start his way in LLM quantization. He had been studying for months and was finally ready to put his knowledge to use.\n",
      "\n",
      "The first step was to choose the right model. There were many different options, but the boy decided to go with a transformer-based model. He spent hours training the model and fine-tuning the hyperparameters.\n",
      "\n",
      "Once the model was trained, the boy was ready to start quantizing it. He used a technique called quantization-aware training to ensure that the model would be able to run on a low-power device.\n",
      "\n",
      "The boy was thrilled with the results. His model was able to run on a low-power device and still perform well. He was excited to see what he could do with his newfound skills.\n",
      "\n",
      "The boy was excited to start his way in LLM quantization. He\n"
     ]
    }
   ],
   "source": [
    "generated_ids = model.generate(**model_inputs, max_new_tokens=max_new_tokens)\n",
    "model_outputs = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(model_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e76f0721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 m 52s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab96f433",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
