{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import nip\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"/home/msst/repo/Quantization\")\n",
    "import qlib\n",
    "\n",
    "DEVICE = 'cuda:0'\n",
    "\n",
    "# # T=256 \n",
    "# # L=12\n",
    "# # V=1\n",
    "# # K=2\n",
    "# # tlut_bits=10\n",
    "# # decode_mode='1mad'\n",
    "# # incoh_proc_mode='skip'\n",
    "# # viterby_batch_size=256#1024\n",
    "\n",
    "# T=256 \n",
    "# L=16\n",
    "# V=2\n",
    "# K=2\n",
    "# tlut_bits=9\n",
    "# decode_mode='quantlut_sym'\n",
    "# incoh_proc_mode='skip'\n",
    "# viterby_batch_size=1024\n",
    "\n",
    "# qmodule_test = qlib.TrellisLinear(\n",
    "# \t\t\tT=T, \n",
    "#             L=L, \n",
    "#             V=V, \n",
    "#             K=K,\n",
    "# \t\t\ttlut_bits=tlut_bits,\n",
    "#             decode_mode=decode_mode, \n",
    "#             incoh_proc_mode=incoh_proc_mode, \n",
    "#             viterby_batch_size=viterby_batch_size\n",
    "# \t\t)\n",
    "\n",
    "# module_to_wrap = torch.nn.Linear(4096, 128).cuda() #model.get_submodule(module_name).cuda()\n",
    "# module_weight = module_to_wrap.weight.data.clone()\n",
    "# # module_to_wrap.weight.data = torch.randn(4096, 4096).cuda()\n",
    "# qmodule_test = qmodule_test.wrap_module(module_to_wrap)\n",
    "\n",
    "# w_scaled = module_weight / module_weight.std()\n",
    "# wq_scaled = qmodule_test.weight / module_weight.std()\n",
    "\n",
    "# err = torch.mean((wq_scaled - w_scaled)**2, axis=-1)\n",
    "# print(f'error: {err.mean().item():.3f} +- {err.std().item():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOL, Im custom!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc36ab6134774bf9b6ae5f4983265f2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name='Llama2-7b-hf'\n",
    "#model_name='Llama-3.2-1B'\n",
    "\n",
    "DTYPE = torch.float16\n",
    "model = qlib.load_custom_llama(\n",
    "\tmodel_name=model_name, \n",
    "\tdevice_map='cpu', \n",
    "\ttorch_dtype=DTYPE, \n",
    "\t#use_flash_attn=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_193187/2858948352.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  qmodel = torch.load(cpnt_path)\n"
     ]
    }
   ],
   "source": [
    "#cpnt_path = '/home/msst/repo/Quantization/logs/checkpoints_Llama2-7b-hf/trellis/T256_L12_V1_K2_1mad_qtip.pth'\n",
    "cpnt_path = '/home/msst/repo/Quantization/logs/checkpoints_Llama2-7b-hf/trellis/T256_L16_V2_K2_quantlut_sym_2d_qtip.pth'\n",
    "qmodel = torch.load(cpnt_path)\n",
    "qmodel = qmodel.to(DEVICE)\n",
    "tokenizer=qlib.load_tokenizer('Llama2-7b-hf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trellis_layer = qmodel.get_submodule('model.layers.7.self_attn.q_proj')\n",
    "# test_input = torch.randn(4, 4096, 4096).half().to('cuda:0')\n",
    "\n",
    "# %%timeit\n",
    "# with torch.amp.autocast('cuda', dtype=torch.float16):\n",
    "#     with torch.no_grad():\n",
    "#         trellis_layer(test_input)\n",
    "\n",
    "# full 48.8 ms\n",
    "# wo incoh 44.8 ms\n",
    "# wo linear 26.8 ms\n",
    "# wo linear and incoh 22.8 ms\n",
    "# full 1mad 61.6 ms\n",
    "# full 1mad_short 73.3 ms\n",
    "# full 1mad_mid 62.1 ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 2/21 [00:36<05:46, 18.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53.879631382783934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 3/21 [00:54<05:27, 18.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53.23798870333791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 4/21 [01:12<05:08, 18.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56.96323627715897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 5/21 [01:31<04:50, 18.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57.77230420172647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 6/21 [01:49<04:32, 18.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54.538511722748524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 7/21 [02:07<04:14, 18.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56.43514044062202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 8/21 [02:25<03:56, 18.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.319813841102565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 9/21 [02:43<03:38, 18.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.411158931703206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 10/21 [03:01<03:20, 18.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.52449337402075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 11/21 [03:20<03:01, 18.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.13975740748911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 12/21 [03:38<02:43, 18.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54.48786258642959\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 13/21 [03:56<02:25, 18.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.11318190902971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 14/21 [04:14<02:07, 18.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56.453415541530845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 15/21 [04:32<01:49, 18.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.90724836495527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 16/21 [04:51<01:30, 18.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.32450942099588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████  | 17/21 [05:09<01:12, 18.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.72551908095811\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 18/21 [05:27<00:54, 18.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.404183936452846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 19/21 [05:45<00:36, 18.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54.77897151625316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 20/21 [06:03<00:18, 18.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54.47452254610681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [06:19<00:00, 18.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54.97963127495153\n",
      "54.97963127495153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataloader = qlib.QATDataset(\n",
    "    config=nip.load('/home/msst/repo/Quantization/configs/data/wikitext_test_seqlen4096.yaml'),\n",
    "    tokenizer=tokenizer\n",
    ").get_dataloader()\n",
    "\n",
    "\n",
    "with torch.amp.autocast('cuda', dtype=torch.float16):\n",
    "    res = qlib.evaluate(qmodel, dataloader, print_times=25)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
