{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import nip\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"/home/msst/repo/Quantization\")\n",
    "import qlib\n",
    "\n",
    "DEVICE = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_to_model = '/home/msst/repo/Quantization/ml/llm/pretrained_models/Llama2-7b-trellis'\n",
    "# qmodel = AutoModelForCausalLM.from_pretrained(\n",
    "# \tpath_to_model, \n",
    "# \ttrust_remote_code=True,\n",
    "#     torch_dtype=torch.float16,\n",
    "# ).to(DEVICE)\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(path_to_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOL, Im custom!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of QuantizedLlamaForCausalLM were not initialized from the model checkpoint at /home/msst/repo/Quantization/logs/checkpoints_Llama2-7b-hf/trellis/T256_L16_V2_K2_cbs10_LowBitSym_qtip_ptq_bs5 and are newly initialized: ['model.layers.0.mlp.down_proj.input_quantizer.act_scale', 'model.layers.0.mlp.gate_proj.input_quantizer.act_scale', 'model.layers.0.mlp.up_proj.input_quantizer.act_scale', 'model.layers.0.self_attn.k_proj.input_quantizer.act_scale', 'model.layers.0.self_attn.o_proj.input_quantizer.act_scale', 'model.layers.0.self_attn.q_proj.input_quantizer.act_scale', 'model.layers.0.self_attn.v_proj.input_quantizer.act_scale', 'model.layers.1.mlp.down_proj.input_quantizer.act_scale', 'model.layers.1.mlp.gate_proj.input_quantizer.act_scale', 'model.layers.1.mlp.up_proj.input_quantizer.act_scale', 'model.layers.1.self_attn.k_proj.input_quantizer.act_scale', 'model.layers.1.self_attn.o_proj.input_quantizer.act_scale', 'model.layers.1.self_attn.q_proj.input_quantizer.act_scale', 'model.layers.1.self_attn.v_proj.input_quantizer.act_scale', 'model.layers.10.mlp.down_proj.input_quantizer.act_scale', 'model.layers.10.mlp.gate_proj.input_quantizer.act_scale', 'model.layers.10.mlp.up_proj.input_quantizer.act_scale', 'model.layers.10.self_attn.k_proj.input_quantizer.act_scale', 'model.layers.10.self_attn.o_proj.input_quantizer.act_scale', 'model.layers.10.self_attn.q_proj.input_quantizer.act_scale', 'model.layers.10.self_attn.v_proj.input_quantizer.act_scale', 'model.layers.11.mlp.down_proj.input_quantizer.act_scale', 'model.layers.11.mlp.gate_proj.input_quantizer.act_scale', 'model.layers.11.mlp.up_proj.input_quantizer.act_scale', 'model.layers.11.self_attn.k_proj.input_quantizer.act_scale', 'model.layers.11.self_attn.o_proj.input_quantizer.act_scale', 'model.layers.11.self_attn.q_proj.input_quantizer.act_scale', 'model.layers.11.self_attn.v_proj.input_quantizer.act_scale', 'model.layers.12.mlp.down_proj.input_quantizer.act_scale', 'model.layers.12.mlp.gate_proj.input_quantizer.act_scale', 'model.layers.12.mlp.up_proj.input_quantizer.act_scale', 'model.layers.12.self_attn.k_proj.input_quantizer.act_scale', 'model.layers.12.self_attn.o_proj.input_quantizer.act_scale', 'model.layers.12.self_attn.q_proj.input_quantizer.act_scale', 'model.layers.12.self_attn.v_proj.input_quantizer.act_scale', 'model.layers.13.mlp.down_proj.input_quantizer.act_scale', 'model.layers.13.mlp.gate_proj.input_quantizer.act_scale', 'model.layers.13.mlp.up_proj.input_quantizer.act_scale', 'model.layers.13.self_attn.k_proj.input_quantizer.act_scale', 'model.layers.13.self_attn.o_proj.input_quantizer.act_scale', 'model.layers.13.self_attn.q_proj.input_quantizer.act_scale', 'model.layers.13.self_attn.v_proj.input_quantizer.act_scale', 'model.layers.14.mlp.down_proj.input_quantizer.act_scale', 'model.layers.14.mlp.gate_proj.input_quantizer.act_scale', 'model.layers.14.mlp.up_proj.input_quantizer.act_scale', 'model.layers.14.self_attn.k_proj.input_quantizer.act_scale', 'model.layers.14.self_attn.o_proj.input_quantizer.act_scale', 'model.layers.14.self_attn.q_proj.input_quantizer.act_scale', 'model.layers.14.self_attn.v_proj.input_quantizer.act_scale', 'model.layers.15.mlp.down_proj.input_quantizer.act_scale', 'model.layers.15.mlp.gate_proj.input_quantizer.act_scale', 'model.layers.15.mlp.up_proj.input_quantizer.act_scale', 'model.layers.15.self_attn.k_proj.input_quantizer.act_scale', 'model.layers.15.self_attn.o_proj.input_quantizer.act_scale', 'model.layers.15.self_attn.q_proj.input_quantizer.act_scale', 'model.layers.15.self_attn.v_proj.input_quantizer.act_scale', 'model.layers.16.mlp.down_proj.input_quantizer.act_scale', 'model.layers.16.mlp.gate_proj.input_quantizer.act_scale', 'model.layers.16.mlp.up_proj.input_quantizer.act_scale', 'model.layers.16.self_attn.k_proj.input_quantizer.act_scale', 'model.layers.16.self_attn.o_proj.input_quantizer.act_scale', 'model.layers.16.self_attn.q_proj.input_quantizer.act_scale', 'model.layers.16.self_attn.v_proj.input_quantizer.act_scale', 'model.layers.17.mlp.down_proj.input_quantizer.act_scale', 'model.layers.17.mlp.gate_proj.input_quantizer.act_scale', 'model.layers.17.mlp.up_proj.input_quantizer.act_scale', 'model.layers.17.self_attn.k_proj.input_quantizer.act_scale', 'model.layers.17.self_attn.o_proj.input_quantizer.act_scale', 'model.layers.17.self_attn.q_proj.input_quantizer.act_scale', 'model.layers.17.self_attn.v_proj.input_quantizer.act_scale', 'model.layers.18.mlp.down_proj.input_quantizer.act_scale', 'model.layers.18.mlp.gate_proj.input_quantizer.act_scale', 'model.layers.18.mlp.up_proj.input_quantizer.act_scale', 'model.layers.18.self_attn.k_proj.input_quantizer.act_scale', 'model.layers.18.self_attn.o_proj.input_quantizer.act_scale', 'model.layers.18.self_attn.q_proj.input_quantizer.act_scale', 'model.layers.18.self_attn.v_proj.input_quantizer.act_scale', 'model.layers.19.mlp.down_proj.input_quantizer.act_scale', 'model.layers.19.mlp.gate_proj.input_quantizer.act_scale', 'model.layers.19.mlp.up_proj.input_quantizer.act_scale', 'model.layers.19.self_attn.k_proj.input_quantizer.act_scale', 'model.layers.19.self_attn.o_proj.input_quantizer.act_scale', 'model.layers.19.self_attn.q_proj.input_quantizer.act_scale', 'model.layers.19.self_attn.v_proj.input_quantizer.act_scale', 'model.layers.2.mlp.down_proj.input_quantizer.act_scale', 'model.layers.2.mlp.gate_proj.input_quantizer.act_scale', 'model.layers.2.mlp.up_proj.input_quantizer.act_scale', 'model.layers.2.self_attn.k_proj.input_quantizer.act_scale', 'model.layers.2.self_attn.o_proj.input_quantizer.act_scale', 'model.layers.2.self_attn.q_proj.input_quantizer.act_scale', 'model.layers.2.self_attn.v_proj.input_quantizer.act_scale', 'model.layers.20.mlp.down_proj.input_quantizer.act_scale', 'model.layers.20.mlp.gate_proj.input_quantizer.act_scale', 'model.layers.20.mlp.up_proj.input_quantizer.act_scale', 'model.layers.20.self_attn.k_proj.input_quantizer.act_scale', 'model.layers.20.self_attn.o_proj.input_quantizer.act_scale', 'model.layers.20.self_attn.q_proj.input_quantizer.act_scale', 'model.layers.20.self_attn.v_proj.input_quantizer.act_scale', 'model.layers.21.mlp.down_proj.input_quantizer.act_scale', 'model.layers.21.mlp.gate_proj.input_quantizer.act_scale', 'model.layers.21.mlp.up_proj.input_quantizer.act_scale', 'model.layers.21.self_attn.k_proj.input_quantizer.act_scale', 'model.layers.21.self_attn.o_proj.input_quantizer.act_scale', 'model.layers.21.self_attn.q_proj.input_quantizer.act_scale', 'model.layers.21.self_attn.v_proj.input_quantizer.act_scale', 'model.layers.22.mlp.down_proj.input_quantizer.act_scale', 'model.layers.22.mlp.gate_proj.input_quantizer.act_scale', 'model.layers.22.mlp.up_proj.input_quantizer.act_scale', 'model.layers.22.self_attn.k_proj.input_quantizer.act_scale', 'model.layers.22.self_attn.o_proj.input_quantizer.act_scale', 'model.layers.22.self_attn.q_proj.input_quantizer.act_scale', 'model.layers.22.self_attn.v_proj.input_quantizer.act_scale', 'model.layers.23.mlp.down_proj.input_quantizer.act_scale', 'model.layers.23.mlp.gate_proj.input_quantizer.act_scale', 'model.layers.23.mlp.up_proj.input_quantizer.act_scale', 'model.layers.23.self_attn.k_proj.input_quantizer.act_scale', 'model.layers.23.self_attn.o_proj.input_quantizer.act_scale', 'model.layers.23.self_attn.q_proj.input_quantizer.act_scale', 'model.layers.23.self_attn.v_proj.input_quantizer.act_scale', 'model.layers.24.mlp.down_proj.input_quantizer.act_scale', 'model.layers.24.mlp.gate_proj.input_quantizer.act_scale', 'model.layers.24.mlp.up_proj.input_quantizer.act_scale', 'model.layers.24.self_attn.k_proj.input_quantizer.act_scale', 'model.layers.24.self_attn.o_proj.input_quantizer.act_scale', 'model.layers.24.self_attn.q_proj.input_quantizer.act_scale', 'model.layers.24.self_attn.v_proj.input_quantizer.act_scale', 'model.layers.25.mlp.down_proj.input_quantizer.act_scale', 'model.layers.25.mlp.gate_proj.input_quantizer.act_scale', 'model.layers.25.mlp.up_proj.input_quantizer.act_scale', 'model.layers.25.self_attn.k_proj.input_quantizer.act_scale', 'model.layers.25.self_attn.o_proj.input_quantizer.act_scale', 'model.layers.25.self_attn.q_proj.input_quantizer.act_scale', 'model.layers.25.self_attn.v_proj.input_quantizer.act_scale', 'model.layers.26.mlp.down_proj.input_quantizer.act_scale', 'model.layers.26.mlp.gate_proj.input_quantizer.act_scale', 'model.layers.26.mlp.up_proj.input_quantizer.act_scale', 'model.layers.26.self_attn.k_proj.input_quantizer.act_scale', 'model.layers.26.self_attn.o_proj.input_quantizer.act_scale', 'model.layers.26.self_attn.q_proj.input_quantizer.act_scale', 'model.layers.26.self_attn.v_proj.input_quantizer.act_scale', 'model.layers.27.mlp.down_proj.input_quantizer.act_scale', 'model.layers.27.mlp.gate_proj.input_quantizer.act_scale', 'model.layers.27.mlp.up_proj.input_quantizer.act_scale', 'model.layers.27.self_attn.k_proj.input_quantizer.act_scale', 'model.layers.27.self_attn.o_proj.input_quantizer.act_scale', 'model.layers.27.self_attn.q_proj.input_quantizer.act_scale', 'model.layers.27.self_attn.v_proj.input_quantizer.act_scale', 'model.layers.28.mlp.down_proj.input_quantizer.act_scale', 'model.layers.28.mlp.gate_proj.input_quantizer.act_scale', 'model.layers.28.mlp.up_proj.input_quantizer.act_scale', 'model.layers.28.self_attn.k_proj.input_quantizer.act_scale', 'model.layers.28.self_attn.o_proj.input_quantizer.act_scale', 'model.layers.28.self_attn.q_proj.input_quantizer.act_scale', 'model.layers.28.self_attn.v_proj.input_quantizer.act_scale', 'model.layers.29.mlp.down_proj.input_quantizer.act_scale', 'model.layers.29.mlp.gate_proj.input_quantizer.act_scale', 'model.layers.29.mlp.up_proj.input_quantizer.act_scale', 'model.layers.29.self_attn.k_proj.input_quantizer.act_scale', 'model.layers.29.self_attn.o_proj.input_quantizer.act_scale', 'model.layers.29.self_attn.q_proj.input_quantizer.act_scale', 'model.layers.29.self_attn.v_proj.input_quantizer.act_scale', 'model.layers.3.mlp.down_proj.input_quantizer.act_scale', 'model.layers.3.mlp.gate_proj.input_quantizer.act_scale', 'model.layers.3.mlp.up_proj.input_quantizer.act_scale', 'model.layers.3.self_attn.k_proj.input_quantizer.act_scale', 'model.layers.3.self_attn.o_proj.input_quantizer.act_scale', 'model.layers.3.self_attn.q_proj.input_quantizer.act_scale', 'model.layers.3.self_attn.v_proj.input_quantizer.act_scale', 'model.layers.30.mlp.down_proj.input_quantizer.act_scale', 'model.layers.30.mlp.gate_proj.input_quantizer.act_scale', 'model.layers.30.mlp.up_proj.input_quantizer.act_scale', 'model.layers.30.self_attn.k_proj.input_quantizer.act_scale', 'model.layers.30.self_attn.o_proj.input_quantizer.act_scale', 'model.layers.30.self_attn.q_proj.input_quantizer.act_scale', 'model.layers.30.self_attn.v_proj.input_quantizer.act_scale', 'model.layers.31.mlp.down_proj.input_quantizer.act_scale', 'model.layers.31.mlp.gate_proj.input_quantizer.act_scale', 'model.layers.31.mlp.up_proj.input_quantizer.act_scale', 'model.layers.31.self_attn.k_proj.input_quantizer.act_scale', 'model.layers.31.self_attn.o_proj.input_quantizer.act_scale', 'model.layers.31.self_attn.q_proj.input_quantizer.act_scale', 'model.layers.31.self_attn.v_proj.input_quantizer.act_scale', 'model.layers.4.mlp.down_proj.input_quantizer.act_scale', 'model.layers.4.mlp.gate_proj.input_quantizer.act_scale', 'model.layers.4.mlp.up_proj.input_quantizer.act_scale', 'model.layers.4.self_attn.k_proj.input_quantizer.act_scale', 'model.layers.4.self_attn.o_proj.input_quantizer.act_scale', 'model.layers.4.self_attn.q_proj.input_quantizer.act_scale', 'model.layers.4.self_attn.v_proj.input_quantizer.act_scale', 'model.layers.5.mlp.down_proj.input_quantizer.act_scale', 'model.layers.5.mlp.gate_proj.input_quantizer.act_scale', 'model.layers.5.mlp.up_proj.input_quantizer.act_scale', 'model.layers.5.self_attn.k_proj.input_quantizer.act_scale', 'model.layers.5.self_attn.o_proj.input_quantizer.act_scale', 'model.layers.5.self_attn.q_proj.input_quantizer.act_scale', 'model.layers.5.self_attn.v_proj.input_quantizer.act_scale', 'model.layers.6.mlp.down_proj.input_quantizer.act_scale', 'model.layers.6.mlp.gate_proj.input_quantizer.act_scale', 'model.layers.6.mlp.up_proj.input_quantizer.act_scale', 'model.layers.6.self_attn.k_proj.input_quantizer.act_scale', 'model.layers.6.self_attn.o_proj.input_quantizer.act_scale', 'model.layers.6.self_attn.q_proj.input_quantizer.act_scale', 'model.layers.6.self_attn.v_proj.input_quantizer.act_scale', 'model.layers.7.mlp.down_proj.input_quantizer.act_scale', 'model.layers.7.mlp.gate_proj.input_quantizer.act_scale', 'model.layers.7.mlp.up_proj.input_quantizer.act_scale', 'model.layers.7.self_attn.k_proj.input_quantizer.act_scale', 'model.layers.7.self_attn.o_proj.input_quantizer.act_scale', 'model.layers.7.self_attn.q_proj.input_quantizer.act_scale', 'model.layers.7.self_attn.v_proj.input_quantizer.act_scale', 'model.layers.8.mlp.down_proj.input_quantizer.act_scale', 'model.layers.8.mlp.gate_proj.input_quantizer.act_scale', 'model.layers.8.mlp.up_proj.input_quantizer.act_scale', 'model.layers.8.self_attn.k_proj.input_quantizer.act_scale', 'model.layers.8.self_attn.o_proj.input_quantizer.act_scale', 'model.layers.8.self_attn.q_proj.input_quantizer.act_scale', 'model.layers.8.self_attn.v_proj.input_quantizer.act_scale', 'model.layers.9.mlp.down_proj.input_quantizer.act_scale', 'model.layers.9.mlp.gate_proj.input_quantizer.act_scale', 'model.layers.9.mlp.up_proj.input_quantizer.act_scale', 'model.layers.9.self_attn.k_proj.input_quantizer.act_scale', 'model.layers.9.self_attn.o_proj.input_quantizer.act_scale', 'model.layers.9.self_attn.q_proj.input_quantizer.act_scale', 'model.layers.9.self_attn.v_proj.input_quantizer.act_scale']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "path_to_checkpoints = \"/home/msst/repo/Quantization/logs/checkpoints_Llama2-7b-hf/trellis/\"\n",
    "chpnt_name = 'T256_L16_V2_K2_cbs10_LowBitSym_qtip_ptq_bs5'\n",
    "\n",
    "qmodel = qlib.QuantizedLlamaForCausalLM.from_pretrained(\n",
    "\tos.path.join(path_to_checkpoints, chpnt_name),\n",
    "    torch_dtype=torch.float16,\n",
    ").to(DEVICE)\n",
    "\n",
    "model_name = 'Llama2-7b-hf'\n",
    "tokenizer = qlib.load_tokenizer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(32):\n",
    "\n",
    "#print(qmodel.get_decoder().layers[0].self_attn.q_proj.weight_scales[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 2/21 [00:30<04:37, 14.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.63577275968245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 2/21 [00:32<05:06, 16.15s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m qlib\u001b[38;5;241m.\u001b[39mQATDataset(\n\u001b[1;32m      2\u001b[0m     config\u001b[38;5;241m=\u001b[39mnip\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/msst/repo/Quantization/configs/data/wikitext_test_seqlen4096.yaml\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m      3\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer\n\u001b[1;32m      4\u001b[0m )\u001b[38;5;241m.\u001b[39mget_dataloader()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16):\n\u001b[0;32m----> 7\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mqlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhalf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(res)\n",
      "File \u001b[0;32m~/miniconda3/envs/qenv/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repo/Quantization/qlib/utils/evaluation.py:72\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, dataloader, print_times)\u001b[0m\n\u001b[1;32m     70\u001b[0m batch \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     71\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 72\u001b[0m neg_log_likelihood \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m     74\u001b[0m loss \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m n_processed_samples \u001b[38;5;241m/\u001b[39m (n_processed_samples \u001b[38;5;241m+\u001b[39m n_samples)\n\u001b[1;32m     75\u001b[0m n_processed_samples \u001b[38;5;241m=\u001b[39m n_processed_samples \u001b[38;5;241m+\u001b[39m n_samples\n",
      "File \u001b[0;32m~/miniconda3/envs/qenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/qenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/repo/Quantization/qlib/modeling/modeling_qtrellis_llama.py:901\u001b[0m, in \u001b[0;36mQuantizedLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    898\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m    900\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 901\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    911\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    912\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    913\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    915\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    916\u001b[0m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/qenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/qenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/repo/Quantization/qlib/modeling/modeling_qtrellis_llama.py:659\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **flash_attn_kwargs)\u001b[0m\n\u001b[1;32m    647\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    648\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    649\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    656\u001b[0m         position_embeddings,\n\u001b[1;32m    657\u001b[0m     )\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 659\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    664\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    666\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mflash_attn_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    671\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/miniconda3/envs/qenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/qenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/repo/Quantization/qlib/modeling/modeling_qtrellis_llama.py:412\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[1;32m    411\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm(hidden_states)\n\u001b[0;32m--> 412\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    413\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    415\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (hidden_states,)\n",
      "File \u001b[0;32m~/miniconda3/envs/qenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/qenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/repo/Quantization/qlib/modeling/modeling_qtrellis_llama.py:218\u001b[0m, in \u001b[0;36mLlamaMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 218\u001b[0m     down_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgate_proj(x)) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "File \u001b[0;32m~/miniconda3/envs/qenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/qenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/repo/Quantization/qlib/qlayers/VQ2SQ.py:175\u001b[0m, in \u001b[0;36mTrellisLinear.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mincoh_proc_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqtip\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m#x = matmul_hadUt_cuda((x * self.SU).float()).to(x.dtype)\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     x \u001b[38;5;241m=\u001b[39m matmul_hadUt_cuda(x \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSU)\n\u001b[0;32m--> 175\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_quantizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m w \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(weight\u001b[38;5;241m=\u001b[39mw, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mx)\n",
      "File \u001b[0;32m~/miniconda3/envs/qenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/qenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/repo/Quantization/qlib/qlayers/input_quantizer.py:51\u001b[0m, in \u001b[0;36mInputQuantizer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     46\u001b[0m     x_scale \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmaximum(\n\u001b[1;32m     47\u001b[0m         x\u001b[38;5;241m.\u001b[39mamin(dim\u001b[38;5;241m=\u001b[39mreduce_dims) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mN, \n\u001b[1;32m     48\u001b[0m         x\u001b[38;5;241m.\u001b[39mamax(dim\u001b[38;5;241m=\u001b[39mreduce_dims) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mP, \n\u001b[1;32m     49\u001b[0m     )\n\u001b[1;32m     50\u001b[0m     replace_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_scale\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m<\u001b[39m x_scale\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact_scale\u001b[49m\u001b[38;5;241m.\u001b[39mdata[replace_indices] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.1\u001b[39m \u001b[38;5;241m*\u001b[39m x_scale[replace_indices]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/qenv/lib/python3.10/site-packages/torch/nn/modules/module.py:1918\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1909\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;241m=\u001b[39m OrderedDict()\n\u001b[1;32m   1911\u001b[0m \u001b[38;5;66;03m# On the return type:\u001b[39;00m\n\u001b[1;32m   1912\u001b[0m \u001b[38;5;66;03m# We choose to return `Any` in the `__getattr__` type signature instead of a more strict `Union[Tensor, Module]`.\u001b[39;00m\n\u001b[1;32m   1913\u001b[0m \u001b[38;5;66;03m# This is done for better interop with various type checkers for the end users.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[38;5;66;03m# See full discussion on the problems with returning `Union` here\u001b[39;00m\n\u001b[1;32m   1917\u001b[0m \u001b[38;5;66;03m# https://github.com/microsoft/pyright/issues/4213\u001b[39;00m\n\u001b[0;32m-> 1918\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m   1919\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[1;32m   1920\u001b[0m         _parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_parameters\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataloader = qlib.QATDataset(\n",
    "    config=nip.load('/home/msst/repo/Quantization/configs/data/wikitext_test_seqlen4096.yaml'),\n",
    "    tokenizer=tokenizer\n",
    ").get_dataloader()\n",
    "\n",
    "with torch.amp.autocast('cuda', dtype=torch.float16):\n",
    "    res = qlib.evaluate(qmodel.half(), dataloader, print_times=25)\n",
    "    print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_to_save = os.path.join(\n",
    "# \tpath_to_checkpoints, \n",
    "# \tf'{chpnt_name}_act8bit'\n",
    "# )\n",
    "# qmodel.half().save_pretrained(path_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 25.07 ppl init\n",
    "## 7.016 ptq (ns=128, bs=2, ne=1)\n",
    "## 7.545 ptq (ns=128, bs=2, ne=1, fp acts)\n",
    "\n",
    "## 7.058 ptq (ns=128, bs=2, ne=2)\n",
    "# 6.654 ptq + qat (KD+CE, 25 steps)\n",
    "# 6.598 ptq + qat (CE, 25 steps)\n",
    "# 6.547 ptq + qat (KD, 10 steps)\n",
    "# 6.511 ptq + qat (KD, 5 steps)\n",
    "\n",
    "## 6.797 ptq (ns=320, bs=5, ne=4)\n",
    "## 6.779 ptq (ns=320, bs=5, ne=2)\n",
    "# 6.579 ptq + qat (KD, 2 steps 1 ga Adam)\n",
    "# 6.493 ptq + qat (KD, 5 steps 1 ga Adam) # PiQA 0.7584 # WINO 0.6606 # ArcE 0.7071 # ArcC 0.3712\n",
    "# 7.372 ptq + qat (KD, 25 steps 1 ga Adam)\n",
    "# 7.236 ptq + qat (KD, 10 steps 4 ga Adafactor)\n",
    "# > 7.5 ptq + qat (CE, 10 steps 4 ga Adafactor)\n",
    "# 6.567 ptq + qat (CE, 10 steps 4 ga Adam)\n",
    "# 6.612 ptq + qat (CE, 10 steps 4 ga Adam small lr)\n",
    "# 6.609 ptq + qat (CE, 25 steps 4 ga Adam)\n",
    "# 7.449 ptq + qat (CE, 100 steps 2 ga Adam)\n",
    "# 6.464 ptq + qat (CE, 20 steps 10 ga Adam)\n",
    "# 6.390 ptq + qat (CE, 20 steps 20 ga Adam)\n",
    "# 6.325 ptq + qat (CE, 125 steps 20 ga Adam)\n",
    "\n",
    "### 39 ppl init\n",
    "# 6.778 ptq (ns=128, bs=2, ne=2)\n",
    "# 6.565 ptq + qat (CE, 10 steps 1 ga Adafactor)\n",
    "# 6.478 ptq + qat (KD, 25 steps 1 ga Adafactor)\n",
    "# 6.420 ptq + qat (KD+CE, 25 steps 1 ga Adafactor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6675"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.435 + 0.763 + 0.780 + 0.692) / 4\n",
    "\n",
    "# 0.435\n",
    "# 0763\n",
    "# 0.780\n",
    "# 0.692\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
