{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fad3c4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/msst/Utils/miniconda3/envs/qenv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/home/msst/repo/Quantization\")\n",
    "from qlib._modeling.modeling_llama import LlamaForCausalLM\n",
    "from qlib import evaluate, QATDataset\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from utils import init_quant_model_base, init_quant_model_hessian, QuantLinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09793695",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 112.62it/s]\n"
     ]
    }
   ],
   "source": [
    "# FP\n",
    "\n",
    "path_to_model = \"/media/msst/ssd_storage1/ml/llm/pretrained_models/Llama2-7B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(path_to_model)\n",
    "model_fp = LlamaForCausalLM.from_pretrained(\n",
    "    path_to_model,\n",
    "    device_map=\"cpu\",\n",
    "    dtype=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f89dbd18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 110.63it/s]\n"
     ]
    }
   ],
   "source": [
    "BITS = 2\n",
    "GROUP_SIZE = 64\n",
    "\n",
    "model_q = LlamaForCausalLM.from_pretrained(\n",
    "    path_to_model,\n",
    "    device_map=\"cpu\",\n",
    "    dtype=\"auto\"\n",
    ")\n",
    "\n",
    "def wrap_model(current_module, prefix=''):\n",
    "    for module_name, module in current_module.named_children():\n",
    "        full_name = f\"{prefix}.{module_name}\" if prefix else module_name\n",
    "        \n",
    "        if \"proj\" in module_name:\n",
    "            weight_shape = module.weight.data.shape\n",
    "            setattr(current_module, module_name, QuantLinear(weight_shape, GROUP_SIZE, BITS))\n",
    "        else:\n",
    "            wrap_model(module, full_name)\n",
    "\n",
    "wrap_model(model_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69246002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4820) model.layers.0.self_attn.q_proj\n",
      "tensor(0.4102) model.layers.0.self_attn.k_proj\n",
      "tensor(0.2148) model.layers.0.self_attn.v_proj\n",
      "tensor(0.2834) model.layers.0.self_attn.o_proj\n",
      "tensor(0.2047) model.layers.0.mlp.gate_proj\n",
      "tensor(0.2044) model.layers.0.mlp.up_proj\n",
      "tensor(0.2075) model.layers.0.mlp.down_proj\n",
      "tensor(0.2335) model.layers.1.self_attn.q_proj\n",
      "tensor(0.2376) model.layers.1.self_attn.k_proj\n",
      "tensor(0.2192) model.layers.1.self_attn.v_proj\n",
      "tensor(0.2485) model.layers.1.self_attn.o_proj\n",
      "tensor(0.2049) model.layers.1.mlp.gate_proj\n",
      "tensor(0.2046) model.layers.1.mlp.up_proj\n",
      "tensor(0.2054) model.layers.1.mlp.down_proj\n",
      "tensor(0.2113) model.layers.2.self_attn.q_proj\n",
      "tensor(0.2099) model.layers.2.self_attn.k_proj\n",
      "tensor(0.2057) model.layers.2.self_attn.v_proj\n",
      "tensor(0.2063) model.layers.2.self_attn.o_proj\n",
      "tensor(0.2037) model.layers.2.mlp.gate_proj\n",
      "tensor(0.2040) model.layers.2.mlp.up_proj\n",
      "tensor(0.2037) model.layers.2.mlp.down_proj\n",
      "tensor(0.2101) model.layers.3.self_attn.q_proj\n",
      "tensor(0.2073) model.layers.3.self_attn.k_proj\n",
      "tensor(0.2052) model.layers.3.self_attn.v_proj\n",
      "tensor(0.2058) model.layers.3.self_attn.o_proj\n",
      "tensor(0.2037) model.layers.3.mlp.gate_proj\n",
      "tensor(0.2037) model.layers.3.mlp.up_proj\n",
      "tensor(0.2041) model.layers.3.mlp.down_proj\n",
      "tensor(0.2096) model.layers.4.self_attn.q_proj\n",
      "tensor(0.2077) model.layers.4.self_attn.k_proj\n",
      "tensor(0.2047) model.layers.4.self_attn.v_proj\n",
      "tensor(0.2040) model.layers.4.self_attn.o_proj\n",
      "tensor(0.2040) model.layers.4.mlp.gate_proj\n",
      "tensor(0.2041) model.layers.4.mlp.up_proj\n",
      "tensor(0.2047) model.layers.4.mlp.down_proj\n",
      "tensor(0.2071) model.layers.5.self_attn.q_proj\n",
      "tensor(0.2070) model.layers.5.self_attn.k_proj\n",
      "tensor(0.2046) model.layers.5.self_attn.v_proj\n",
      "tensor(0.2035) model.layers.5.self_attn.o_proj\n",
      "tensor(0.2034) model.layers.5.mlp.gate_proj\n",
      "tensor(0.2041) model.layers.5.mlp.up_proj\n",
      "tensor(0.2044) model.layers.5.mlp.down_proj\n",
      "tensor(0.2077) model.layers.6.self_attn.q_proj\n",
      "tensor(0.2056) model.layers.6.self_attn.k_proj\n",
      "tensor(0.2055) model.layers.6.self_attn.v_proj\n",
      "tensor(0.2037) model.layers.6.self_attn.o_proj\n",
      "tensor(0.2034) model.layers.6.mlp.gate_proj\n",
      "tensor(0.2042) model.layers.6.mlp.up_proj\n",
      "tensor(0.2046) model.layers.6.mlp.down_proj\n",
      "tensor(0.2066) model.layers.7.self_attn.q_proj\n",
      "tensor(0.2056) model.layers.7.self_attn.k_proj\n",
      "tensor(0.2058) model.layers.7.self_attn.v_proj\n",
      "tensor(0.2040) model.layers.7.self_attn.o_proj\n",
      "tensor(0.2033) model.layers.7.mlp.gate_proj\n",
      "tensor(0.2041) model.layers.7.mlp.up_proj\n",
      "tensor(0.2047) model.layers.7.mlp.down_proj\n",
      "tensor(0.2066) model.layers.8.self_attn.q_proj\n",
      "tensor(0.2058) model.layers.8.self_attn.k_proj\n",
      "tensor(0.2054) model.layers.8.self_attn.v_proj\n",
      "tensor(0.2034) model.layers.8.self_attn.o_proj\n",
      "tensor(0.2035) model.layers.8.mlp.gate_proj\n",
      "tensor(0.2037) model.layers.8.mlp.up_proj\n",
      "tensor(0.2048) model.layers.8.mlp.down_proj\n",
      "tensor(0.2059) model.layers.9.self_attn.q_proj\n",
      "tensor(0.2057) model.layers.9.self_attn.k_proj\n",
      "tensor(0.2054) model.layers.9.self_attn.v_proj\n",
      "tensor(0.2028) model.layers.9.self_attn.o_proj\n",
      "tensor(0.2033) model.layers.9.mlp.gate_proj\n",
      "tensor(0.2037) model.layers.9.mlp.up_proj\n",
      "tensor(0.2054) model.layers.9.mlp.down_proj\n",
      "tensor(0.2054) model.layers.10.self_attn.q_proj\n",
      "tensor(0.2059) model.layers.10.self_attn.k_proj\n",
      "tensor(0.2052) model.layers.10.self_attn.v_proj\n",
      "tensor(0.2022) model.layers.10.self_attn.o_proj\n",
      "tensor(0.2034) model.layers.10.mlp.gate_proj\n",
      "tensor(0.2039) model.layers.10.mlp.up_proj\n",
      "tensor(0.2056) model.layers.10.mlp.down_proj\n",
      "tensor(0.2062) model.layers.11.self_attn.q_proj\n",
      "tensor(0.2062) model.layers.11.self_attn.k_proj\n",
      "tensor(0.2060) model.layers.11.self_attn.v_proj\n",
      "tensor(0.2027) model.layers.11.self_attn.o_proj\n",
      "tensor(0.2036) model.layers.11.mlp.gate_proj\n",
      "tensor(0.2038) model.layers.11.mlp.up_proj\n",
      "tensor(0.2056) model.layers.11.mlp.down_proj\n",
      "tensor(0.2059) model.layers.12.self_attn.q_proj\n",
      "tensor(0.2057) model.layers.12.self_attn.k_proj\n",
      "tensor(0.2054) model.layers.12.self_attn.v_proj\n",
      "tensor(0.2027) model.layers.12.self_attn.o_proj\n",
      "tensor(0.2034) model.layers.12.mlp.gate_proj\n",
      "tensor(0.2034) model.layers.12.mlp.up_proj\n",
      "tensor(0.2054) model.layers.12.mlp.down_proj\n",
      "tensor(0.2061) model.layers.13.self_attn.q_proj\n",
      "tensor(0.2058) model.layers.13.self_attn.k_proj\n",
      "tensor(0.2050) model.layers.13.self_attn.v_proj\n",
      "tensor(0.2030) model.layers.13.self_attn.o_proj\n",
      "tensor(0.2033) model.layers.13.mlp.gate_proj\n",
      "tensor(0.2035) model.layers.13.mlp.up_proj\n",
      "tensor(0.2058) model.layers.13.mlp.down_proj\n",
      "tensor(0.2049) model.layers.14.self_attn.q_proj\n",
      "tensor(0.2060) model.layers.14.self_attn.k_proj\n",
      "tensor(0.2052) model.layers.14.self_attn.v_proj\n",
      "tensor(0.2027) model.layers.14.self_attn.o_proj\n",
      "tensor(0.2033) model.layers.14.mlp.gate_proj\n",
      "tensor(0.2035) model.layers.14.mlp.up_proj\n",
      "tensor(0.2053) model.layers.14.mlp.down_proj\n",
      "tensor(0.2064) model.layers.15.self_attn.q_proj\n",
      "tensor(0.2056) model.layers.15.self_attn.k_proj\n",
      "tensor(0.2050) model.layers.15.self_attn.v_proj\n",
      "tensor(0.2026) model.layers.15.self_attn.o_proj\n",
      "tensor(0.2032) model.layers.15.mlp.gate_proj\n",
      "tensor(0.2034) model.layers.15.mlp.up_proj\n",
      "tensor(0.2060) model.layers.15.mlp.down_proj\n",
      "tensor(0.2068) model.layers.16.self_attn.q_proj\n",
      "tensor(0.2062) model.layers.16.self_attn.k_proj\n",
      "tensor(0.2050) model.layers.16.self_attn.v_proj\n",
      "tensor(0.2024) model.layers.16.self_attn.o_proj\n",
      "tensor(0.2030) model.layers.16.mlp.gate_proj\n",
      "tensor(0.2031) model.layers.16.mlp.up_proj\n",
      "tensor(0.2056) model.layers.16.mlp.down_proj\n",
      "tensor(0.2055) model.layers.17.self_attn.q_proj\n",
      "tensor(0.2060) model.layers.17.self_attn.k_proj\n",
      "tensor(0.2051) model.layers.17.self_attn.v_proj\n",
      "tensor(0.2025) model.layers.17.self_attn.o_proj\n",
      "tensor(0.2028) model.layers.17.mlp.gate_proj\n",
      "tensor(0.2031) model.layers.17.mlp.up_proj\n",
      "tensor(0.2044) model.layers.17.mlp.down_proj\n",
      "tensor(0.2059) model.layers.18.self_attn.q_proj\n",
      "tensor(0.2055) model.layers.18.self_attn.k_proj\n",
      "tensor(0.2042) model.layers.18.self_attn.v_proj\n",
      "tensor(0.2025) model.layers.18.self_attn.o_proj\n",
      "tensor(0.2027) model.layers.18.mlp.gate_proj\n",
      "tensor(0.2029) model.layers.18.mlp.up_proj\n",
      "tensor(0.2041) model.layers.18.mlp.down_proj\n",
      "tensor(0.2067) model.layers.19.self_attn.q_proj\n",
      "tensor(0.2063) model.layers.19.self_attn.k_proj\n",
      "tensor(0.2044) model.layers.19.self_attn.v_proj\n",
      "tensor(0.2026) model.layers.19.self_attn.o_proj\n",
      "tensor(0.2028) model.layers.19.mlp.gate_proj\n",
      "tensor(0.2028) model.layers.19.mlp.up_proj\n",
      "tensor(0.2039) model.layers.19.mlp.down_proj\n",
      "tensor(0.2063) model.layers.20.self_attn.q_proj\n",
      "tensor(0.2067) model.layers.20.self_attn.k_proj\n",
      "tensor(0.2040) model.layers.20.self_attn.v_proj\n",
      "tensor(0.2021) model.layers.20.self_attn.o_proj\n",
      "tensor(0.2027) model.layers.20.mlp.gate_proj\n",
      "tensor(0.2028) model.layers.20.mlp.up_proj\n",
      "tensor(0.2039) model.layers.20.mlp.down_proj\n",
      "tensor(0.2073) model.layers.21.self_attn.q_proj\n",
      "tensor(0.2071) model.layers.21.self_attn.k_proj\n",
      "tensor(0.2039) model.layers.21.self_attn.v_proj\n",
      "tensor(0.2024) model.layers.21.self_attn.o_proj\n",
      "tensor(0.2028) model.layers.21.mlp.gate_proj\n",
      "tensor(0.2030) model.layers.21.mlp.up_proj\n",
      "tensor(0.2035) model.layers.21.mlp.down_proj\n",
      "tensor(0.2067) model.layers.22.self_attn.q_proj\n",
      "tensor(0.2070) model.layers.22.self_attn.k_proj\n",
      "tensor(0.2038) model.layers.22.self_attn.v_proj\n",
      "tensor(0.2015) model.layers.22.self_attn.o_proj\n",
      "tensor(0.2029) model.layers.22.mlp.gate_proj\n",
      "tensor(0.2030) model.layers.22.mlp.up_proj\n",
      "tensor(0.2030) model.layers.22.mlp.down_proj\n",
      "tensor(0.2065) model.layers.23.self_attn.q_proj\n",
      "tensor(0.2066) model.layers.23.self_attn.k_proj\n",
      "tensor(0.2038) model.layers.23.self_attn.v_proj\n",
      "tensor(0.2025) model.layers.23.self_attn.o_proj\n",
      "tensor(0.2027) model.layers.23.mlp.gate_proj\n",
      "tensor(0.2030) model.layers.23.mlp.up_proj\n",
      "tensor(0.2031) model.layers.23.mlp.down_proj\n",
      "tensor(0.2066) model.layers.24.self_attn.q_proj\n",
      "tensor(0.2079) model.layers.24.self_attn.k_proj\n",
      "tensor(0.2041) model.layers.24.self_attn.v_proj\n",
      "tensor(0.2021) model.layers.24.self_attn.o_proj\n",
      "tensor(0.2029) model.layers.24.mlp.gate_proj\n",
      "tensor(0.2029) model.layers.24.mlp.up_proj\n",
      "tensor(0.2030) model.layers.24.mlp.down_proj\n",
      "tensor(0.2065) model.layers.25.self_attn.q_proj\n",
      "tensor(0.2063) model.layers.25.self_attn.k_proj\n",
      "tensor(0.2038) model.layers.25.self_attn.v_proj\n",
      "tensor(0.2028) model.layers.25.self_attn.o_proj\n",
      "tensor(0.2028) model.layers.25.mlp.gate_proj\n",
      "tensor(0.2030) model.layers.25.mlp.up_proj\n",
      "tensor(0.2032) model.layers.25.mlp.down_proj\n",
      "tensor(0.2075) model.layers.26.self_attn.q_proj\n",
      "tensor(0.2075) model.layers.26.self_attn.k_proj\n",
      "tensor(0.2038) model.layers.26.self_attn.v_proj\n",
      "tensor(0.2022) model.layers.26.self_attn.o_proj\n",
      "tensor(0.2026) model.layers.26.mlp.gate_proj\n",
      "tensor(0.2031) model.layers.26.mlp.up_proj\n",
      "tensor(0.2035) model.layers.26.mlp.down_proj\n",
      "tensor(0.2074) model.layers.27.self_attn.q_proj\n",
      "tensor(0.2052) model.layers.27.self_attn.k_proj\n",
      "tensor(0.2036) model.layers.27.self_attn.v_proj\n",
      "tensor(0.2023) model.layers.27.self_attn.o_proj\n",
      "tensor(0.2029) model.layers.27.mlp.gate_proj\n",
      "tensor(0.2029) model.layers.27.mlp.up_proj\n",
      "tensor(0.2041) model.layers.27.mlp.down_proj\n",
      "tensor(0.2084) model.layers.28.self_attn.q_proj\n",
      "tensor(0.2056) model.layers.28.self_attn.k_proj\n",
      "tensor(0.2040) model.layers.28.self_attn.v_proj\n",
      "tensor(0.2026) model.layers.28.self_attn.o_proj\n",
      "tensor(0.2027) model.layers.28.mlp.gate_proj\n",
      "tensor(0.2032) model.layers.28.mlp.up_proj\n",
      "tensor(0.2046) model.layers.28.mlp.down_proj\n",
      "tensor(0.2098) model.layers.29.self_attn.q_proj\n",
      "tensor(0.2070) model.layers.29.self_attn.k_proj\n",
      "tensor(0.2043) model.layers.29.self_attn.v_proj\n",
      "tensor(0.2025) model.layers.29.self_attn.o_proj\n",
      "tensor(0.2032) model.layers.29.mlp.gate_proj\n",
      "tensor(0.2036) model.layers.29.mlp.up_proj\n",
      "tensor(0.2065) model.layers.29.mlp.down_proj\n",
      "tensor(0.2123) model.layers.30.self_attn.q_proj\n",
      "tensor(0.2064) model.layers.30.self_attn.k_proj\n",
      "tensor(0.2041) model.layers.30.self_attn.v_proj\n",
      "tensor(0.2028) model.layers.30.self_attn.o_proj\n",
      "tensor(0.2027) model.layers.30.mlp.gate_proj\n",
      "tensor(0.2028) model.layers.30.mlp.up_proj\n",
      "tensor(0.2105) model.layers.30.mlp.down_proj\n",
      "tensor(0.2109) model.layers.31.self_attn.q_proj\n",
      "tensor(0.2092) model.layers.31.self_attn.k_proj\n",
      "tensor(0.2061) model.layers.31.self_attn.v_proj\n",
      "tensor(0.2016) model.layers.31.self_attn.o_proj\n",
      "tensor(0.2044) model.layers.31.mlp.gate_proj\n",
      "tensor(0.2049) model.layers.31.mlp.up_proj\n",
      "tensor(0.2173) model.layers.31.mlp.down_proj\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = f\"/home/msst/repo/Quantization/nb/adaptive_rounding/init_w{BITS}gs{GROUP_SIZE}.pth\"\n",
    "\n",
    "# init_quant_model_base(model_q, model_fp)\n",
    "# torch.save(model_q.state_dict(), path)\n",
    "model_q.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a765722d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/32 [00:00<?, ?it/s]/home/msst/Utils/miniconda3/envs/qenv/lib/python3.13/site-packages/torch/backends/cuda/__init__.py:131: UserWarning: Please use the new API settings to control TF32 behavior, such as torch.backends.cudnn.conv.fp32_precision = 'tf32' or torch.backends.cuda.matmul.fp32_precision = 'ieee'. Old settings, e.g, torch.backends.cuda.matmul.allow_tf32 = True, torch.backends.cudnn.allow_tf32 = True, allowTF32CuDNN() and allowTF32CuBLAS() will be deprecated after Pytorch 2.9. Please see https://pytorch.org/docs/main/notes/cuda.html#tensorfloat-32-tf32-on-ampere-and-later-devices (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:80.)\n",
      "  return torch._C._get_cublas_allow_tf32()\n",
      "/home/msst/Utils/miniconda3/envs/qenv/lib/python3.13/site-packages/torch/_inductor/compile_fx.py:312: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312.4642028808594 -> 25.386293411254883\n",
      "384.5408020019531 -> 27.397750854492188\n",
      "226.7843017578125 -> 9.038747787475586\n",
      "2.4171228408813477 -> 0.43604612350463867\n",
      "434.210205078125 -> 240.4371795654297\n",
      "433.3564453125 -> 230.09152221679688\n",
      "1.9809553623199463 -> 1.0955758094787598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/32 [01:26<44:53, 86.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5276.001953125 -> 991.803466796875\n",
      "5321.421875 -> 1011.8199462890625\n",
      "1067.2955322265625 -> 110.42594909667969\n",
      "12.065034866333008 -> 6.955783843994141\n",
      "1827.060546875 -> 1414.498779296875\n",
      "1490.5360107421875 -> 1100.546142578125\n",
      "7141.8779296875 -> 183.28485107421875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 2/32 [02:49<42:07, 84.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12063.65625 -> 4457.0283203125\n",
      "12613.712890625 -> 4867.1748046875\n",
      "3904.09326171875 -> 1170.191162109375\n",
      "45.68397521972656 -> 16.34051513671875\n",
      "3797.11865234375 -> 3130.78076171875\n",
      "3159.1015625 -> 2495.4951171875\n",
      "23.85717010498047 -> 20.610610961914062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 3/32 [04:11<40:27, 83.71s/it]\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"dataset_name\" : \"slim_pajama\",\n",
    "    \"split\": \"train[:250]\",\n",
    "    \"seq_length\": 4096,\n",
    "    \"batch_size\": 8,\n",
    "    \"random_seed\": 'no_rand'\n",
    "}\n",
    "\n",
    "dataloader = QATDataset(\n",
    "    config=config,\n",
    "    tokenizer=tokenizer\n",
    ").get_dataloader()\n",
    "\n",
    "init_quant_model_hessian(\n",
    "    model_q, \n",
    "    model_fp, \n",
    "    dataloader, \n",
    "    advanced=True, \n",
    "    # init_blocks=3, \n",
    "    use_cholesky=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3aa9b6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/166 [00:00<?, ?it/s]W0106 13:57:55.692000 31607 site-packages/torch/_dynamo/convert_frame.py:1358] [0/8] torch._dynamo hit config.recompile_limit (8)\n",
      "W0106 13:57:55.692000 31607 site-packages/torch/_dynamo/convert_frame.py:1358] [0/8]    function: 'reconstruct_weight' (/home/msst/repo/Quantization/nb/adaptive_rounding/utils.py:32)\n",
      "W0106 13:57:55.692000 31607 site-packages/torch/_dynamo/convert_frame.py:1358] [0/8]    last reason: 0/7: tensor 'self._buffers['compressed_weight']' size mismatch at index 0. expected 11008, actual 4096\n",
      "W0106 13:57:55.692000 31607 site-packages/torch/_dynamo/convert_frame.py:1358] [0/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "W0106 13:57:55.692000 31607 site-packages/torch/_dynamo/convert_frame.py:1358] [0/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html\n",
      " 10%|█         | 17/166 [00:09<01:38,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155.07044677221742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 33/166 [00:18<01:28,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172.97263319046485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 49/166 [00:27<01:17,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143.01810211709636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▊      | 64/166 [00:36<00:58,  1.74it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# model_q = model_q.cuda()\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     res = \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_q\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_times\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m     \u001b[38;5;28mprint\u001b[39m(res)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Utils/miniconda3/envs/qenv/lib/python3.13/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/repo/Quantization/qlib/utils/evaluation.py:26\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(model, dataloader, print_times)\u001b[39m\n\u001b[32m     23\u001b[39m     loss += neg_log_likelihood * n_samples / n_processed_samples\n\u001b[32m     25\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m step!=\u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m step%(\u001b[38;5;28mmax\u001b[39m(\u001b[32m1\u001b[39m, n_steps//print_times))==\u001b[32m0\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m         \u001b[38;5;28mprint\u001b[39m(math.exp(\u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[32m     28\u001b[39m ppl = math.exp(loss.item())\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ppl\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"dataset_name\" : \"wiki\",\n",
    "    \"split\": \"test\",\n",
    "    \"seq_length\": 2048,\n",
    "    \"batch_size\": 1,\n",
    "    \"random_seed\": 'no_rand'\n",
    "}\n",
    "\n",
    "dataloader = QATDataset(\n",
    "    config=config,\n",
    "    tokenizer=tokenizer\n",
    ").get_dataloader()\n",
    "\n",
    "model_q = model_q.cuda()\n",
    "# model_q = model_q.cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    res = evaluate(model_q, dataloader, print_times=10)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579a63a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting: w3gs128\n",
    "# wiki ppl seqlen=2048\n",
    "\n",
    "# 8.76 - base\n",
    "# 6.41 - advanced=False, init_blocks=3 \n",
    "# 6.39 - advanced=True, init_blocks=3\n",
    "# 6.42 - advanced=True, init_blocks=3, use_cholesky=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b79e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting: w3gs128\n",
    "# wiki ppl seqlen=2048\n",
    "\n",
    "#  - base\n",
    "#  - advanced=False, init_blocks=3 \n",
    "#  - advanced=True, init_blocks=3\n",
    "#  - advanced=True, init_blocks=3, use_cholesky=True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
