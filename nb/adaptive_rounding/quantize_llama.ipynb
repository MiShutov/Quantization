{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fad3c4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/msst/Utils/miniconda3/envs/qenv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/home/msst/repo/Quantization\")\n",
    "import qlib\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09793695",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 85.01it/s]\n"
     ]
    }
   ],
   "source": [
    "# FP\n",
    "path_to_model = \"/media/msst/ssd_storage1/ml/llm/pretrained_models/Llama2-7B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(path_to_model)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "model = qlib._modeling.modeling_llama.LlamaForCausalLM.from_pretrained(\n",
    "    path_to_model,\n",
    "    device_map=\"cpu\",\n",
    "    dtype=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0027db01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantLinear(nn.Module):\n",
    "    def __init__(self, weight_shape, group_size):\n",
    "        super().__init__()\n",
    "        self.weight_shape = weight_shape\n",
    "        self.group_size = group_size\n",
    "        self.scale_size = [weight_shape[0], weight_shape[1] // group_size]\n",
    "        self.scale = nn.Parameter(torch.empty(self.scale_size))\n",
    "        self.offset = nn.Parameter(torch.empty(self.scale_size))\n",
    "        self.register_buffer(\n",
    "            \"compressed_weight\",\n",
    "            torch.empty(\n",
    "                weight_shape,\n",
    "                dtype=torch.uint8,\n",
    "                requires_grad=False\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def reshape_weight_for_scaling(self, w):\n",
    "        return w.reshape(\n",
    "            self.weight_shape[0], self.weight_shape[1] // self.group_size, -1\n",
    "        )\n",
    "\n",
    "\n",
    "    @torch.compile()\n",
    "    def reconstruct_weight(self):\n",
    "        w = self.reshape_weight_for_scaling(self.compressed_weight)\n",
    "        w = w * self.scale[..., None] - self.offset[..., None]\n",
    "        w = w.reshape(self.weight_shape)\n",
    "        return w\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        w = self.reconstruct_weight()\n",
    "        return torch.nn.functional.linear(x, w.to(x.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac58bd9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 104.14it/s]\n"
     ]
    }
   ],
   "source": [
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "qmodel = qlib._modeling.modeling_llama.LlamaForCausalLM.from_pretrained(\n",
    "    path_to_model,\n",
    "    device_map=\"cpu\",\n",
    "    dtype=\"auto\"\n",
    ")\n",
    "\n",
    "def wrap_model(current_module, prefix=''):\n",
    "    for module_name, module in current_module.named_children():\n",
    "        full_name = f\"{prefix}.{module_name}\" if prefix else module_name\n",
    "        \n",
    "        if \"proj\" in module_name:\n",
    "            weight_shape = module.weight.data.shape\n",
    "            setattr(current_module, module_name, QuantLinear(weight_shape, 128))\n",
    "        else:\n",
    "            wrap_model(module, full_name)\n",
    "\n",
    "wrap_model(qmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21b0364c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinMaxInitializer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(self, x_grouped, negative_clip, positive_clip):\n",
    "        x_min = x_grouped.min(axis=-1)[0].unsqueeze(-1).float()\n",
    "        x_max = x_grouped.max(axis=-1)[0].unsqueeze(-1).float()\n",
    "\n",
    "        offset = (x_max * negative_clip - x_min * positive_clip) / (positive_clip - negative_clip)\n",
    "        scale = (x_max + offset) / positive_clip\n",
    "        scale = torch.abs(scale)\n",
    "\n",
    "        scale = scale.reshape(x_grouped.shape[0], x_grouped.shape[1])\n",
    "        offset = offset.reshape(x_grouped.shape[0], x_grouped.shape[1])\n",
    "\n",
    "        return scale.contiguous(), offset.contiguous()\n",
    "\n",
    "initializer = MinMaxInitializer()\n",
    "\n",
    "@torch.no_grad()\n",
    "def configure_single_layer(qlayer, layer, bits, C=None):\n",
    "    max_int_val = 2**bits - 1\n",
    "\n",
    "    orig_weight = layer.weight.data\n",
    "    if C is not None:\n",
    "        orig_weight = C.float() @ orig_weight.float()\n",
    "\n",
    "    orig_weight_reshaped = qlayer.reshape_weight_for_scaling(orig_weight)\n",
    "    scale, offset = initializer(orig_weight_reshaped, negative_clip=0, positive_clip=max_int_val)\n",
    "    \n",
    "    quant_weight = (orig_weight_reshaped + offset[..., None]) / scale[..., None]\n",
    "    quant_weight = quant_weight.reshape_as(orig_weight)\n",
    "\n",
    "    quant_weight = torch.clamp(torch.round(quant_weight), 0, max_int_val).to(torch.uint8)\n",
    "\n",
    "    qlayer.compressed_weight.copy_(quant_weight)\n",
    "    qlayer.scale.copy_(scale)\n",
    "    qlayer.offset.copy_(offset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def init_quant_model(qmodel, model, bits):\n",
    "    for qmodule_name, qmodule in qmodel.named_modules():\n",
    "        if isinstance(qmodule, QuantLinear):\n",
    "            orig_module = model.get_submodule(qmodule_name)\n",
    "            configure_single_layer(qmodule, orig_module, bits)\n",
    "\n",
    "            err = torch.mean(((orig_module.weight.data.cpu() - qmodule.reconstruct_weight().cpu()) / (orig_module.weight.data.cpu().std() + 1e-8))**2)\n",
    "            print(err, qmodule_name)\n",
    "\n",
    "\n",
    "# init_quant_model(qmodel, model, bits=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69246002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.save(qmodel.state_dict(), \"/home/msst/repo/Quantization/nb/adaptive_rounding/init_base.pth\")\n",
    "qmodel.load_state_dict(torch.load(\"/home/msst/repo/Quantization/nb/adaptive_rounding/init_base.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f4440d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.91085815429688 -> 8.156818389892578\n",
      "94.28813171386719 -> 8.093609809875488\n",
      "26.162555694580078 -> 2.408036231994629\n"
     ]
    }
   ],
   "source": [
    "from transformers.masking_utils import create_causal_mask\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def prepare_hessian(activations):\n",
    "    hidden_size = activations[0].shape[-1]\n",
    "    H = torch.zeros(hidden_size, hidden_size).cuda()\n",
    "    for act in activations:\n",
    "        act = act.cuda().view(-1, act.shape[-1]).float() / hidden_size ** 0.5\n",
    "        H += act.T @ act\n",
    "    return H\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def prepare_hessian_q(activations, activations_q):\n",
    "    hidden_size = activations[0].shape[-1]\n",
    "    H_q = torch.zeros(hidden_size, hidden_size).cuda()\n",
    "    for act_id in range(len(activations)):\n",
    "        act = activations[act_id].cuda().view(-1, hidden_size).float() / hidden_size ** 0.5\n",
    "        act_q = activations_q[act_id].cuda().view(-1, hidden_size).float() / hidden_size ** 0.5\n",
    "        H_q += act.T @ act_q\n",
    "    return H_q\n",
    "\n",
    "\n",
    "def prepare_C(H, Hq):\n",
    "    I = torch.eye(H.shape[0]).cuda() * 0 #* H.std()\n",
    "    C = torch.linalg.inv(H + I) @ Hq\n",
    "    return C\n",
    "\n",
    "\n",
    "def hessian_loss(layer_q, layer_fp, H, C):\n",
    "    w_q = layer_q.reconstruct_weight()\n",
    "    if C is not None:\n",
    "        w_q = w_q @ C.T\n",
    "    w = layer_fp.weight\n",
    "    delta_w = layer_q.reconstruct_weight() - layer_fp.weight\n",
    "    return torch.trace(delta_w @ H @ delta_w.T)\n",
    "\n",
    "\n",
    "def optimize_quant_params(\n",
    "        layer_q,\n",
    "        layer_fp,\n",
    "        bits,\n",
    "        H,\n",
    "        C=None,\n",
    "    ):\n",
    "    trainable_params = [layer_q.scale, layer_q.offset]    \n",
    "    optim = torch.optim.Adam(trainable_params, lr=1e-3)\n",
    "    n_steps = 100\n",
    "    \n",
    "    with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "        for i in range(n_steps):\n",
    "            optim.zero_grad()\n",
    "            loss = hessian_loss(layer_q, layer_fp, H, C)\n",
    "            if i == 0:\n",
    "                init_loss =  loss.item()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "        print(f\"{init_loss} -> {loss}\")\n",
    "\n",
    "\n",
    "def hessian_loss_ste(layer_q, layer_fp, H, bits):\n",
    "    max_int_val = 2**bits - 1\n",
    "\n",
    "    latent_weight_reshaped = layer_q.reshape_weight_for_scaling(layer_fp.weight + layer_q.weight_addition)\n",
    "    latent_weight_scaled = (latent_weight_reshaped + layer_q.offset[..., None]) / layer_q.scale[..., None]\n",
    "\n",
    "    quant_weight = torch.clamp(torch.round(latent_weight_scaled), 0, max_int_val).to(torch.uint8)\n",
    "    quant_weight_ste = quant_weight + latent_weight_scaled\n",
    "\n",
    "    layer_q.compressed_weight.copy_(quant_weight.reshape_as(layer_fp.weight))\n",
    "\n",
    "    weight_reco = quant_weight_ste * layer_q.scale[..., None] - layer_q.offset[..., None]\n",
    "    weight_reco = weight_reco.reshape_as(layer_fp.weight)\n",
    "\n",
    "    delta_w = weight_reco - layer_fp.weight\n",
    "\n",
    "    C = 1e-6\n",
    "    return torch.trace(delta_w @ H @ delta_w.T) + C * torch.sum(layer_q.weight_addition ** 2)\n",
    "\n",
    "\n",
    "def optimize_quant_params_ste(\n",
    "        layer_q,\n",
    "        layer_fp,\n",
    "        bits,\n",
    "        H\n",
    "    ):\n",
    "    layer_q.weight_addition = nn.Parameter(torch.zeros_like(layer_fp.weight.data).float())\n",
    "\n",
    "    # trainable_params = [layer_q.scale, layer_q.offset, layer_q.latent_weight]        \n",
    "    # trainable_params = [layer_q.scale, layer_q.offset]\n",
    "    trainable_params = [layer_q.weight_addition]        \n",
    "    optim = torch.optim.Adam(trainable_params, lr=1e-4)\n",
    "    n_steps = 100\n",
    "\n",
    "    with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "        for i in range(n_steps):\n",
    "            optim.zero_grad()\n",
    "\n",
    "            loss = hessian_loss_ste(layer_q, layer_fp, H, bits)\n",
    "            if i == 0:\n",
    "                init_loss =  loss.item()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "        print(f\"{init_loss} -> {loss}\")\n",
    "\n",
    "    del layer_q.weight_addition\n",
    "\n",
    "\n",
    "def init_quant_block_hessian(\n",
    "        block_q,\n",
    "        block_fp,\n",
    "        bits,\n",
    "        activations,\n",
    "        causal_mask,\n",
    "        position_embeddings,\n",
    "        with_opt=True,\n",
    "        ):\n",
    "\n",
    "    ##### Attention #####\n",
    "\n",
    "    # Copy activations for the residual stream\n",
    "    residual_activations = [x.clone() for x in activations]\n",
    "\n",
    "    # Collect activations after input_layernorm\n",
    "    with torch.no_grad():\n",
    "        for act_id, act in enumerate(activations):\n",
    "            act = act.cuda()\n",
    "            act = block_fp.input_layernorm(act)\n",
    "            activations[act_id] = act.cpu()\n",
    "\n",
    "    # Initialize q,k,v-projs\n",
    "    H = prepare_hessian(activations)\n",
    "    block_q_attn = block_q.self_attn\n",
    "    block_fp_attn = block_fp.self_attn\n",
    "    for layer_name in [\"q_proj\", \"k_proj\", \"v_proj\"]:\n",
    "        layer_q = getattr(block_q_attn, layer_name)\n",
    "        layer_fp = getattr(block_fp_attn, layer_name)\n",
    "        configure_single_layer(layer_q, layer_fp, bits)\n",
    "        if with_opt:\n",
    "            optimize_quant_params(layer_q, layer_fp, bits, H)\n",
    "\n",
    "    # Collect attention-out activations\n",
    "    with torch.no_grad():\n",
    "        for act_id, act in enumerate(activations):\n",
    "            act = act.cuda()\n",
    "            act = block_fp_attn.compute_attention(\n",
    "                hidden_states=act, \n",
    "                position_embeddings=position_embeddings,\n",
    "                attention_mask=causal_mask\n",
    "            )[0]\n",
    "            activations[act_id] = act.cpu()\n",
    "\n",
    "    # Initialize o_proj\n",
    "    layer_q = block_q.self_attn.o_proj\n",
    "    layer_fp = block_fp.self_attn.o_proj\n",
    "    H = prepare_hessian(activations)\n",
    "    configure_single_layer(layer_q, layer_fp, bits)\n",
    "    if with_opt:\n",
    "        optimize_quant_params(layer_q, layer_fp, bits, H)\n",
    "\n",
    "    # Collect self_attn outs\n",
    "    with torch.no_grad():\n",
    "        for act_id, act in enumerate(activations):\n",
    "            act = act.cuda()\n",
    "            res_act = residual_activations[act_id].cuda()\n",
    "            act = block_fp.self_attn.o_proj(act)\n",
    "            activations[act_id] = (act + res_act).cpu()\n",
    "\n",
    "    ##### MLP #####\n",
    "\n",
    "    # Copy activations for the residual stream\n",
    "    residual_activations = [x.clone() for x in activations]\n",
    "\n",
    "    # Collect activations after post_attention_layernorm\n",
    "    with torch.no_grad():\n",
    "        for act_id, act in enumerate(activations):\n",
    "            act = act.cuda()\n",
    "            act = block_fp.post_attention_layernorm(act)\n",
    "            activations[act_id] = act.cpu()\n",
    "\n",
    "    # Initialize gate_proj and up_proj\n",
    "    H = prepare_hessian(activations)\n",
    "    block_q_mlp = block_q.mlp\n",
    "    block_fp_mlp = block_fp.mlp\n",
    "    for layer_name in [\"gate_proj\", \"up_proj\"]:\n",
    "        layer_q = getattr(block_q_mlp, layer_name)\n",
    "        layer_fp = getattr(block_fp_mlp, layer_name)\n",
    "        configure_single_layer(layer_q, layer_fp, bits)\n",
    "        if with_opt:\n",
    "            optimize_quant_params(layer_q, layer_fp, bits, H)\n",
    "\n",
    "    # Collect internal mlp activations\n",
    "    with torch.no_grad():\n",
    "        for act_id, act in enumerate(activations):\n",
    "            act = act.cuda()\n",
    "            act = block_fp_mlp.act_fn(block_fp.mlp.gate_proj(act)) * block_fp.mlp.up_proj(act)\n",
    "            activations[act_id] = act.cpu()\n",
    "\n",
    "    # Initialize down_proj\n",
    "    layer_q = block_q.mlp.down_proj\n",
    "    layer_fp = block_fp.mlp.down_proj\n",
    "    H = prepare_hessian(activations)\n",
    "    configure_single_layer(layer_q, layer_fp, bits)\n",
    "    if with_opt:\n",
    "        optimize_quant_params(layer_q, layer_fp, bits, H)\n",
    "\n",
    "    # Collect mlp outs\n",
    "    with torch.no_grad():\n",
    "        for act_id, act in enumerate(activations):\n",
    "            act = act.cuda()\n",
    "            res_act = residual_activations[act_id].cuda()\n",
    "            act = block_fp.mlp.down_proj(act)\n",
    "            activations[act_id] = (act + res_act).cpu()\n",
    "\n",
    "\n",
    "def init_quant_block_hessian_2(\n",
    "        block_q,\n",
    "        block_fp,\n",
    "        bits,\n",
    "        activations,\n",
    "        activations_q,\n",
    "        causal_mask,\n",
    "        position_embeddings,\n",
    "        with_opt=True,\n",
    "        ):\n",
    "\n",
    "    ##### Attention #####\n",
    "\n",
    "    # Copy activations for the residual stream\n",
    "    residual_activations = [x.clone() for x in activations]\n",
    "    residual_activations_q = [x.clone() for x in activations_q]\n",
    "\n",
    "    # Collect activations after input_layernorm\n",
    "    with torch.no_grad():\n",
    "        for act_id in range(len(activations)):\n",
    "            act = block_fp.input_layernorm(activations[act_id].cuda())            \n",
    "            activations[act_id] = act.cpu()\n",
    "            \n",
    "            act_q = block_q.input_layernorm(activations_q[act_id].cuda())            \n",
    "            activations_q[act_id] = act_q.cpu()\n",
    "\n",
    "    # Initialize q,k,v-projs\n",
    "    H = prepare_hessian(activations)\n",
    "    Hq = prepare_hessian_q(activations, activations_q)\n",
    "    C = prepare_C(H, Hq)\n",
    "\n",
    "    block_q_attn = block_q.self_attn\n",
    "    block_fp_attn = block_fp.self_attn\n",
    "    for layer_name in [\"q_proj\", \"k_proj\", \"v_proj\"]:\n",
    "        layer_q = getattr(block_q_attn, layer_name)\n",
    "        layer_fp = getattr(block_fp_attn, layer_name)\n",
    "        configure_single_layer(layer_q, layer_fp, bits)\n",
    "        if with_opt:\n",
    "            optimize_quant_params(layer_q, layer_fp, bits, H, C)\n",
    "\n",
    "    # Collect attention-out activations\n",
    "    with torch.no_grad():\n",
    "        for act_id in range(len(activations)):\n",
    "            act = block_fp_attn.compute_attention(\n",
    "                hidden_states=act.cuda(), \n",
    "                position_embeddings=position_embeddings,\n",
    "                attention_mask=causal_mask\n",
    "            )[0]\n",
    "            activations[act_id] = act.cpu()\n",
    "            \n",
    "            act_q = block_q_attn.compute_attention(\n",
    "                hidden_states=act_q.cuda(), \n",
    "                position_embeddings=position_embeddings,\n",
    "                attention_mask=causal_mask\n",
    "            )[0]\n",
    "            activations_q[act_id] = act_q.cpu()\n",
    "\n",
    "    # Initialize o_proj\n",
    "    layer_q = block_q.self_attn.o_proj\n",
    "    layer_fp = block_fp.self_attn.o_proj\n",
    "    \n",
    "    H = prepare_hessian(activations)\n",
    "    Hq = prepare_hessian_q(activations, activations_q)\n",
    "    C = prepare_C(H, Hq)\n",
    "\n",
    "    configure_single_layer(layer_q, layer_fp, bits)\n",
    "    if with_opt:\n",
    "        optimize_quant_params(layer_q, layer_fp, bits, H, C)\n",
    "\n",
    "    # Collect self_attn outs\n",
    "    with torch.no_grad():\n",
    "        for act_id in range(len(activations)):\n",
    "            act = activations[act_id].cuda()\n",
    "            res_act = residual_activations[act_id].cuda()\n",
    "            activations[act_id] = (block_fp.self_attn.o_proj(act) + res_act).cpu()\n",
    "\n",
    "            act_q = activations_q[act_id].cuda()\n",
    "            res_act_q = residual_activations_q[act_id].cuda()\n",
    "            activations_q[act_id] = (block_q.self_attn.o_proj(act_q) + res_act_q).cpu()\n",
    "\n",
    "\n",
    "    ##### MLP #####\n",
    "\n",
    "    # Copy activations for the residual stream\n",
    "    residual_activations = [x.clone() for x in activations]\n",
    "    residual_activations_q = [x.clone() for x in activations_q]\n",
    "\n",
    "    # Collect activations after post_attention_layernorm\n",
    "    with torch.no_grad():\n",
    "        for act_id in range(len(activations)):\n",
    "            act = block_fp.post_attention_layernorm(activations[act_id].cuda())\n",
    "            activations[act_id] = act.cpu()\n",
    "\n",
    "            act_q = block_q.post_attention_layernorm(activations_q[act_id].cuda())\n",
    "            activations_q[act_id] = act_q.cpu()\n",
    "\n",
    "    # Initialize gate_proj and up_proj\n",
    "    H = prepare_hessian(activations)\n",
    "    Hq = prepare_hessian_q(activations, activations_q)\n",
    "    C = prepare_C(H, Hq)\n",
    "\n",
    "    block_q_mlp = block_q.mlp\n",
    "    block_fp_mlp = block_fp.mlp\n",
    "    for layer_name in [\"gate_proj\", \"up_proj\"]:\n",
    "        layer_q = getattr(block_q_mlp, layer_name)\n",
    "        layer_fp = getattr(block_fp_mlp, layer_name)\n",
    "        configure_single_layer(layer_q, layer_fp, bits)\n",
    "        if with_opt:\n",
    "            optimize_quant_params(layer_q, layer_fp, bits, H, C)\n",
    "\n",
    "    # Collect internal mlp activations\n",
    "    with torch.no_grad():\n",
    "        for act_id in range(len(activations)):\n",
    "            act = activations[act_id].cuda()\n",
    "            act = block_fp_mlp.act_fn(block_fp.mlp.gate_proj(act)) * block_fp.mlp.up_proj(act)\n",
    "            activations[act_id] = act.cpu()\n",
    "\n",
    "            act_q = activations_q[act_id].cuda()\n",
    "            act_q = block_q_mlp.act_fn(block_q.mlp.gate_proj(act_q)) * block_q.mlp.up_proj(act_q)\n",
    "            activations_q[act_id] = act_q.cpu()\n",
    "\n",
    "    # Initialize down_proj\n",
    "    layer_q = block_q.mlp.down_proj\n",
    "    layer_fp = block_fp.mlp.down_proj\n",
    "    \n",
    "    H = prepare_hessian(activations)\n",
    "    Hq = prepare_hessian_q(activations, activations_q)\n",
    "    C = prepare_C(H, Hq)\n",
    "\n",
    "    configure_single_layer(layer_q, layer_fp, bits)\n",
    "    if with_opt:\n",
    "        optimize_quant_params(layer_q, layer_fp, bits, H, C)\n",
    "\n",
    "    # Collect mlp outs\n",
    "    with torch.no_grad():\n",
    "        for act_id in range(len(activations)):\n",
    "            act = activations[act_id].cuda()\n",
    "            res_act = residual_activations[act_id].cuda()\n",
    "            activations[act_id] = (block_fp.mlp.down_proj(act) + res_act).cpu()\n",
    "\n",
    "            act_q = activations_q[act_id].cuda()\n",
    "            res_act_q = residual_activations_q[act_id].cuda()\n",
    "            activations_q[act_id] = (block_q.mlp.down_proj(act) + res_act_q).cpu()\n",
    "\n",
    "\n",
    "def init_quant_model_hessian(model_q, model_fp, bits, dataloader):\n",
    "    embed_tokens = model_fp.get_decoder().embed_tokens.cuda()\n",
    "    embed_tokens_device = embed_tokens.weight.device\n",
    "\n",
    "    _batch = next(iter(dataloader))\n",
    "    _inputs_embeds = embed_tokens(_batch.to(embed_tokens_device))\n",
    "\n",
    "    cache_position = torch.arange(_inputs_embeds.shape[1], device=_inputs_embeds.device)\n",
    "    position_ids = cache_position.unsqueeze(0)\n",
    "    causal_mask = create_causal_mask(\n",
    "        config=model_fp.config,\n",
    "        input_embeds=_inputs_embeds,\n",
    "        attention_mask=None,\n",
    "        cache_position=cache_position,\n",
    "        past_key_values=None,\n",
    "        position_ids=position_ids,\n",
    "    )\n",
    "\n",
    "    position_embeddings = model_fp.get_decoder().rotary_emb(_inputs_embeds, position_ids)\n",
    "\n",
    "    # Prepare activations\n",
    "    activations = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            activations.append(embed_tokens(batch.to(embed_tokens_device)).cpu())\n",
    "    activations_q = [a.clone() for a in activations]\n",
    "\n",
    "    for decoder_layer_id in tqdm(range(len(model_q.get_decoder().layers))):\n",
    "        # if decoder_layer_id > 2:\n",
    "        #     break\n",
    "\n",
    "        block_q = model_q.get_decoder().layers[decoder_layer_id].cuda()\n",
    "        block_fp = model_fp.get_decoder().layers[decoder_layer_id].cuda()\n",
    "\n",
    "        # init_quant_block_hessian(\n",
    "        #     block_q,\n",
    "        #     block_fp,\n",
    "        #     bits,\n",
    "        #     activations,\n",
    "        #     causal_mask,\n",
    "        #     position_embeddings,\n",
    "        #     with_opt=True\n",
    "        # )\n",
    "\n",
    "        init_quant_block_hessian_2(\n",
    "            block_q,\n",
    "            block_fp,\n",
    "            bits,\n",
    "            activations,\n",
    "            activations_q,\n",
    "            causal_mask,\n",
    "            position_embeddings,\n",
    "            with_opt=True\n",
    "        )\n",
    "\n",
    "        block_q = block_q.cpu()\n",
    "        block_fp = block_fp.cpu()\n",
    "\n",
    "\n",
    "config = {\n",
    "    # \"dataset_name\" : \"slim_pajama\",\n",
    "    # \"split\": \"train[:10000]\",\n",
    "    \"dataset_name\" : \"wiki\",\n",
    "    \"split\": \"train[:5000]\",\n",
    "    \"seq_length\": 4096,\n",
    "    \"n_seq\" : 64, #128,\n",
    "    \"batch_size\": 8,\n",
    "    \"random_seed\": 'no_rand'\n",
    "}\n",
    "dataloader = qlib.QATDataset(\n",
    "    config=config,\n",
    "    tokenizer=tokenizer\n",
    ").get_dataloader()\n",
    "print(len(dataloader))\n",
    "\n",
    "init_quant_model_hessian(qmodel, model, 3, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa9b6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 7/166 [00:04<01:36,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.779180533633582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 13/166 [00:07<01:29,  1.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.480178949563514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 15/166 [00:07<01:12,  2.07it/s]"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"dataset_name\" : \"wiki\",\n",
    "    \"split\": \"test\",\n",
    "    \"seq_length\": 2048,\n",
    "    \"batch_size\": 1,\n",
    "    \"random_seed\": 'no_rand'\n",
    "}\n",
    "\n",
    "dataloader = qlib.QATDataset(\n",
    "    config=config,\n",
    "    tokenizer=tokenizer\n",
    ").get_dataloader()\n",
    "\n",
    "qmodel = qmodel.cuda()\n",
    "# qmodel = model.cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    with torch.amp.autocast('cuda', dtype=torch.float16):\n",
    "        res = qlib.evaluate(qmodel, dataloader, print_times=25)\n",
    "        print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392e0e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base: 8.76\n",
    "# H: 6.39\n",
    "# H + x_q: 6.43"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
