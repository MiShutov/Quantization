{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fad3c4f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/msst/Utils/miniconda3/envs/qenv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/home/msst/repo/Quantization\")\n",
    "import qlib\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "185f0198",
   "metadata": {},
   "outputs": [],
   "source": [
    "BITS = 2\n",
    "GROUP_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09793695",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 85.13it/s]\n"
     ]
    }
   ],
   "source": [
    "# FP\n",
    "path_to_model = \"/media/msst/ssd_storage1/ml/llm/pretrained_models/Llama2-7B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(path_to_model)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "model = qlib._modeling.modeling_llama.LlamaForCausalLM.from_pretrained(\n",
    "    path_to_model,\n",
    "    device_map=\"cpu\",\n",
    "    dtype=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0027db01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantLinear(nn.Module):\n",
    "    def __init__(self, weight_shape, group_size):\n",
    "        super().__init__()\n",
    "        self.weight_shape = weight_shape\n",
    "        self.group_size = group_size\n",
    "        self.scale_size = [weight_shape[0], weight_shape[1] // group_size]\n",
    "        self.scale = nn.Parameter(torch.empty(self.scale_size))\n",
    "        self.offset = nn.Parameter(torch.empty(self.scale_size))\n",
    "        self.register_buffer(\n",
    "            \"compressed_weight\",\n",
    "            torch.empty(\n",
    "                weight_shape,\n",
    "                dtype=torch.uint8,\n",
    "                requires_grad=False\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def reshape_weight_for_scaling(self, w):\n",
    "        return w.reshape(\n",
    "            self.weight_shape[0], self.weight_shape[1] // self.group_size, -1\n",
    "        )\n",
    "\n",
    "\n",
    "    @torch.compile()\n",
    "    def reconstruct_weight(self):\n",
    "        w = self.reshape_weight_for_scaling(self.compressed_weight)\n",
    "        w = w * self.scale[..., None] - self.offset[..., None]\n",
    "        w = w.reshape(self.weight_shape)\n",
    "        return w\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        w = self.reconstruct_weight()\n",
    "        return torch.nn.functional.linear(x, w.to(x.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac58bd9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 109.44it/s]\n"
     ]
    }
   ],
   "source": [
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "qmodel = qlib._modeling.modeling_llama.LlamaForCausalLM.from_pretrained(\n",
    "    path_to_model,\n",
    "    device_map=\"cpu\",\n",
    "    dtype=\"auto\"\n",
    ")\n",
    "\n",
    "def wrap_model(current_module, prefix=''):\n",
    "    for module_name, module in current_module.named_children():\n",
    "        full_name = f\"{prefix}.{module_name}\" if prefix else module_name\n",
    "        \n",
    "        if \"proj\" in module_name:\n",
    "            weight_shape = module.weight.data.shape\n",
    "            setattr(current_module, module_name, QuantLinear(weight_shape, GROUP_SIZE))\n",
    "        else:\n",
    "            wrap_model(module, full_name)\n",
    "\n",
    "wrap_model(qmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21b0364c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinMaxInitializer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(self, x_grouped, negative_clip, positive_clip):\n",
    "        x_min = x_grouped.min(axis=-1)[0].unsqueeze(-1).float()\n",
    "        x_max = x_grouped.max(axis=-1)[0].unsqueeze(-1).float()\n",
    "\n",
    "        offset = (x_max * negative_clip - x_min * positive_clip) / (positive_clip - negative_clip)\n",
    "        scale = (x_max + offset) / positive_clip\n",
    "        scale = torch.abs(scale)\n",
    "\n",
    "        scale = scale.reshape(x_grouped.shape[0], x_grouped.shape[1])\n",
    "        offset = offset.reshape(x_grouped.shape[0], x_grouped.shape[1])\n",
    "\n",
    "        return scale.contiguous(), offset.contiguous()\n",
    "\n",
    "initializer = MinMaxInitializer()\n",
    "\n",
    "@torch.no_grad()\n",
    "def configure_single_layer(qlayer, layer, bits, C=None):\n",
    "    max_int_val = 2**bits - 1\n",
    "\n",
    "    orig_weight = layer.weight.data\n",
    "    if C is not None:\n",
    "        orig_weight = C.float() @ orig_weight.float()\n",
    "\n",
    "    orig_weight_reshaped = qlayer.reshape_weight_for_scaling(orig_weight)\n",
    "    scale, offset = initializer(orig_weight_reshaped, negative_clip=0, positive_clip=max_int_val)\n",
    "    \n",
    "    quant_weight = (orig_weight_reshaped + offset[..., None]) / scale[..., None]\n",
    "    quant_weight = quant_weight.reshape_as(orig_weight)\n",
    "\n",
    "    quant_weight = torch.clamp(torch.round(quant_weight), 0, max_int_val).to(torch.uint8)\n",
    "\n",
    "    qlayer.compressed_weight.copy_(quant_weight)\n",
    "    qlayer.scale.copy_(scale)\n",
    "    qlayer.offset.copy_(offset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def init_quant_model(qmodel, model, bits):\n",
    "    for qmodule_name, qmodule in qmodel.named_modules():\n",
    "        if isinstance(qmodule, QuantLinear):\n",
    "            orig_module = model.get_submodule(qmodule_name)\n",
    "            configure_single_layer(qmodule, orig_module, bits)\n",
    "\n",
    "            err = torch.mean(((orig_module.weight.data.cpu() - qmodule.reconstruct_weight().cpu()) / (orig_module.weight.data.cpu().std() + 1e-8))**2)\n",
    "            print(err, qmodule_name)\n",
    "\n",
    "\n",
    "# init_quant_model(qmodel, model, bits=BITS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69246002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = f\"/home/msst/repo/Quantization/nb/adaptive_rounding/init_w{BITS}gs{GROUP_SIZE}.pth\"\n",
    "# torch.save(qmodel.state_dict(), path)\n",
    "qmodel.load_state_dict(torch.load(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0f4440d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220.8610076904297 -> 16.006084442138672\n",
      "270.2167053222656 -> 17.31378936767578\n",
      "158.06936645507812 -> 5.9089813232421875\n",
      "0.3702833652496338 -> 0.03462900221347809\n",
      "571.605224609375 -> 234.9871826171875\n",
      "557.256103515625 -> 226.6083984375\n",
      "5.793052673339844 -> 1.3856124877929688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/32 [00:52<27:15, 52.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2806.46923828125 -> 742.6286010742188\n",
      "2754.2802734375 -> 750.1689453125\n",
      "506.09649658203125 -> 89.96592712402344\n",
      "2.8629579544067383 -> 0.28932833671569824\n",
      "1511.875 -> 805.513671875\n",
      "1342.026611328125 -> 710.8385009765625\n",
      "2970.34033203125 -> 8.761390686035156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 2/32 [01:43<25:43, 51.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5471.4736328125 -> 2247.2216796875\n",
      "5874.99609375 -> 2489.27880859375\n",
      "1697.52685546875 -> 627.808837890625\n",
      "142.06756591796875 -> 5.227794647216797\n",
      "3214.8486328125 -> 1136.830810546875\n",
      "2784.61083984375 -> 985.773193359375\n",
      "24.076446533203125 -> 8.934795379638672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 3/32 [02:33<24:37, 50.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11847.787109375 -> 4335.6171875\n",
      "12579.689453125 -> 4693.1611328125\n",
      "3501.369140625 -> 1208.107666015625\n",
      "649.212646484375 -> 9.239688873291016\n",
      "5669.966796875 -> 1388.126953125\n",
      "4854.236328125 -> 1169.9423828125\n",
      "43.729705810546875 -> 12.924774169921875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 4/32 [03:23<23:38, 50.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10924.50390625 -> 2956.976806640625\n",
      "11276.0791015625 -> 3084.78564453125\n",
      "3292.0634765625 -> 828.7548828125\n",
      "858.423828125 -> 9.749809265136719\n",
      "7359.904296875 -> 1426.61376953125\n",
      "5979.35546875 -> 1132.34130859375\n",
      "79.19644165039062 -> 13.4339599609375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 5/32 [04:14<22:43, 50.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11384.982421875 -> 2455.312744140625\n",
      "12361.57421875 -> 2681.96044921875\n",
      "3615.353515625 -> 715.705810546875\n",
      "1402.65478515625 -> 13.901622772216797\n",
      "8288.8447265625 -> 1329.260986328125\n",
      "6643.712890625 -> 1042.3768310546875\n",
      "71.44436645507812 -> 11.808341979980469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 6/32 [05:04<21:48, 50.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15521.015625 -> 2752.78271484375\n",
      "15990.3837890625 -> 2813.870361328125\n",
      "4686.244140625 -> 760.693359375\n",
      "1144.681640625 -> 11.400745391845703\n",
      "9771.666015625 -> 1471.75\n",
      "7529.671875 -> 1105.85595703125\n",
      "85.7706298828125 -> 12.789131164550781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 7/32 [05:54<20:55, 50.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16341.0390625 -> 2730.373779296875\n",
      "16529.10546875 -> 2723.862060546875\n",
      "5180.05859375 -> 783.30126953125\n",
      "1229.0068359375 -> 12.0103759765625\n",
      "10990.603515625 -> 1559.81005859375\n",
      "8555.4326171875 -> 1183.5902099609375\n",
      "97.76409912109375 -> 13.63885498046875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 8/32 [06:44<20:04, 50.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16469.703125 -> 2562.67724609375\n",
      "16481.388671875 -> 2558.045166015625\n",
      "5285.39453125 -> 751.55810546875\n",
      "1547.0048828125 -> 15.452003479003906\n",
      "11029.259765625 -> 1458.03125\n",
      "9076.654296875 -> 1176.7666015625\n",
      "115.91778564453125 -> 14.345466613769531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 9/32 [07:34<19:12, 50.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17014.119140625 -> 2457.2744140625\n",
      "17623.171875 -> 2579.177978515625\n",
      "5697.400390625 -> 757.6943359375\n",
      "1881.4873046875 -> 16.124359130859375\n",
      "11402.88671875 -> 1403.650146484375\n",
      "9688.4482421875 -> 1171.519287109375\n",
      "126.17559814453125 -> 14.72317886352539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 10/32 [08:25<18:28, 50.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17100.52734375 -> 2307.962890625\n",
      "18084.234375 -> 2482.820068359375\n",
      "5681.75390625 -> 705.1201171875\n",
      "1785.65771484375 -> 16.426197052001953\n",
      "11669.6103515625 -> 1378.6876220703125\n",
      "10200.80078125 -> 1180.32666015625\n",
      "148.6103515625 -> 15.987907409667969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 11/32 [09:16<17:42, 50.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18698.392578125 -> 2483.412109375\n",
      "18449.69140625 -> 2449.148193359375\n",
      "7440.83984375 -> 909.1630859375\n",
      "3432.060546875 -> 28.09123992919922\n",
      "12391.56640625 -> 1309.3408203125\n",
      "11052.015625 -> 1150.602783203125\n",
      "163.071044921875 -> 15.855094909667969\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 12/32 [10:07<16:54, 50.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19516.361328125 -> 2326.371337890625\n",
      "20551.869140625 -> 2487.814697265625\n",
      "7211.8828125 -> 798.41357421875\n",
      "3238.806640625 -> 23.90423583984375\n",
      "12879.123046875 -> 1285.11083984375\n",
      "11936.880859375 -> 1169.932373046875\n",
      "166.0516357421875 -> 16.424331665039062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████      | 13/32 [10:58<16:06, 50.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19559.25390625 -> 2212.381103515625\n",
      "20295.169921875 -> 2290.927978515625\n",
      "7935.2734375 -> 825.95068359375\n",
      "4940.703125 -> 31.362037658691406\n",
      "13569.0478515625 -> 1197.6319580078125\n",
      "12871.7001953125 -> 1118.5137939453125\n",
      "195.8963623046875 -> 17.583099365234375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 14/32 [11:50<15:21, 51.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20364.998046875 -> 2039.711669921875\n",
      "21315.78125 -> 2135.101806640625\n",
      "8068.9921875 -> 733.38525390625\n",
      "4177.5986328125 -> 21.258939743041992\n",
      "14640.392578125 -> 1228.99462890625\n",
      "13993.69140625 -> 1160.153564453125\n",
      "204.26043701171875 -> 19.37676239013672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 15/32 [12:40<14:25, 50.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18993.72265625 -> 1810.953857421875\n",
      "20283.794921875 -> 1951.0511474609375\n",
      "8332.15625 -> 724.45361328125\n",
      "6175.5859375 -> 28.501867294311523\n",
      "15894.060546875 -> 1222.3604736328125\n",
      "15165.75 -> 1152.9384765625\n",
      "247.1981201171875 -> 22.501449584960938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 16/32 [13:31<13:32, 50.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20036.75 -> 1763.216064453125\n",
      "20949.505859375 -> 1867.35888671875\n",
      "9619.2890625 -> 774.4091796875\n",
      "10937.03125 -> 37.9432373046875\n",
      "18312.52734375 -> 1168.333984375\n",
      "17294.361328125 -> 1087.67529296875\n",
      "332.896484375 -> 22.882461547851562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 17/32 [14:21<12:42, 50.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20344.9296875 -> 1491.923583984375\n",
      "21329.31640625 -> 1578.27099609375\n",
      "9869.3359375 -> 663.63330078125\n",
      "10867.7578125 -> 24.037832260131836\n",
      "21006.328125 -> 1124.6241455078125\n",
      "19100.66796875 -> 1009.219970703125\n",
      "365.771240234375 -> 22.487564086914062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 18/32 [15:13<11:54, 51.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21710.22265625 -> 1302.7371826171875\n",
      "22536.431640625 -> 1367.510986328125\n",
      "12120.8828125 -> 670.5576171875\n",
      "22723.90625 -> 38.2092399597168\n",
      "23807.05859375 -> 971.755615234375\n",
      "21402.03515625 -> 852.85546875\n",
      "472.50927734375 -> 21.730560302734375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▉    | 19/32 [16:05<11:07, 51.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20980.580078125 -> 951.0987548828125\n",
      "21921.119140625 -> 995.94140625\n",
      "12025.26953125 -> 503.921875\n",
      "26103.046875 -> 33.33537673950195\n",
      "26000.9375 -> 836.9398193359375\n",
      "22899.6875 -> 728.218505859375\n",
      "546.87255859375 -> 19.435256958007812\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 20/32 [16:58<10:20, 51.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21817.52734375 -> 772.7890625\n",
      "22411.013671875 -> 806.9041137695312\n",
      "12655.40625 -> 416.0865478515625\n",
      "34459.53125 -> 15.12813663482666\n",
      "28130.76953125 -> 692.50830078125\n",
      "24724.9140625 -> 596.1787109375\n",
      "657.082763671875 -> 16.89883041381836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 21/32 [17:49<09:27, 51.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23452.341796875 -> 635.1666259765625\n",
      "23839.126953125 -> 650.615234375\n",
      "15137.421875 -> 384.658447265625\n",
      "55336.0625 -> -13.445680618286133\n",
      "30928.73828125 -> 609.432373046875\n",
      "26665.62890625 -> 515.1119384765625\n",
      "767.855712890625 -> 15.301387786865234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 22/32 [18:40<08:34, 51.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25539.44921875 -> 544.8697509765625\n",
      "26104.306640625 -> 560.6552734375\n",
      "15865.9453125 -> 318.125732421875\n",
      "62410.375 -> -20.11383056640625\n",
      "33891.34765625 -> 560.614013671875\n",
      "28638.53125 -> 463.918701171875\n",
      "944.241455078125 -> 14.788333892822266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 23/32 [19:32<07:44, 51.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28693.119140625 -> 508.0504455566406\n",
      "29195.3125 -> 520.4442749023438\n",
      "19973.1796875 -> 338.010498046875\n",
      "140265.3125 -> -30.648534774780273\n",
      "36024.203125 -> 453.1529541015625\n",
      "30902.08203125 -> 382.8240966796875\n",
      "1145.337646484375 -> 13.142143249511719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 24/32 [20:23<06:51, 51.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26355.951171875 -> 366.8974914550781\n",
      "26654.15625 -> 374.9798583984375\n",
      "19038.296875 -> 252.9283447265625\n",
      "119058.5625 -> -61.53163528442383\n",
      "38638.0703125 -> 414.4991455078125\n",
      "33331.78125 -> 349.9017333984375\n",
      "1322.242431640625 -> 12.711406707763672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 25/32 [21:15<05:59, 51.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31968.38671875 -> 376.3715515136719\n",
      "31903.12109375 -> 377.45013427734375\n",
      "24034.265625 -> 273.262939453125\n",
      "268874.75 -> 122.5596694946289\n",
      "41490.9921875 -> 376.34515380859375\n",
      "35833.921875 -> 318.4627685546875\n",
      "1582.1494140625 -> 11.454811096191406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 81%|████████▏ | 26/32 [22:05<05:07, 51.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29312.84765625 -> 281.92181396484375\n",
      "29392.5625 -> 289.034423828125\n",
      "23519.8046875 -> 221.40872192382812\n",
      "336784.125 -> 278.508056640625\n",
      "44568.890625 -> 330.1917724609375\n",
      "38590.3671875 -> 280.58514404296875\n",
      "1888.81494140625 -> 11.51028823852539\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 27/32 [22:56<04:15, 51.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33550.84375 -> 271.3127746582031\n",
      "33743.01171875 -> 277.2298889160156\n",
      "25171.5625 -> 195.06182861328125\n",
      "376967.125 -> 365.7149963378906\n",
      "47412.3984375 -> 306.5296325683594\n",
      "41452.8125 -> 260.11358642578125\n",
      "2260.80078125 -> 11.16352653503418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 28/32 [23:47<03:23, 50.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32692.255859375 -> 227.25924682617188\n",
      "32868.18359375 -> 230.88858032226562\n",
      "27709.0703125 -> 187.49642944335938\n",
      "636257.5 -> 930.4278564453125\n",
      "49137.2421875 -> 263.0664978027344\n",
      "44037.6953125 -> 231.2512969970703\n",
      "2766.7646484375 -> 11.112064361572266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 29/32 [24:37<02:32, 50.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29108.828125 -> 175.52877807617188\n",
      "29412.76953125 -> 179.634033203125\n",
      "26255.0859375 -> 156.2301025390625\n",
      "609797.5 -> 1212.5782470703125\n",
      "52327.2890625 -> 248.03414916992188\n",
      "47561.96875 -> 223.94113159179688\n",
      "4589.9091796875 -> 13.34028434753418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 30/32 [25:28<01:41, 50.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32930.0859375 -> 167.1314697265625\n",
      "32648.41015625 -> 167.80740356445312\n",
      "29722.390625 -> 151.41702270507812\n",
      "922815.5 -> 2012.63427734375\n",
      "55767.578125 -> 283.00445556640625\n",
      "50153.7734375 -> 244.27688598632812\n",
      "579165056.0 -> -360064.375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 31/32 [26:23<00:51, 51.08s/it]\n"
     ]
    },
    {
     "ename": "_LinAlgError",
     "evalue": "linalg.inv: The diagonal element 2534 is zero, the inversion could not be completed because the input matrix is singular.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31m_LinAlgError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 438\u001b[39m\n\u001b[32m    433\u001b[39m dataloader = qlib.QATDataset(\n\u001b[32m    434\u001b[39m     config=config,\n\u001b[32m    435\u001b[39m     tokenizer=tokenizer\n\u001b[32m    436\u001b[39m ).get_dataloader()\n\u001b[32m    437\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dataloader))\n\u001b[32m--> \u001b[39m\u001b[32m438\u001b[39m \u001b[43minit_quant_model_hessian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBITS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 408\u001b[39m, in \u001b[36minit_quant_model_hessian\u001b[39m\u001b[34m(model_q, model_fp, bits, dataloader)\u001b[39m\n\u001b[32m    396\u001b[39m block_fp = model_fp.get_decoder().layers[decoder_layer_id].cuda()\n\u001b[32m    398\u001b[39m \u001b[38;5;66;03m# init_quant_block_hessian(\u001b[39;00m\n\u001b[32m    399\u001b[39m \u001b[38;5;66;03m#     block_q,\u001b[39;00m\n\u001b[32m    400\u001b[39m \u001b[38;5;66;03m#     block_fp,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    405\u001b[39m \u001b[38;5;66;03m#     with_opt=True\u001b[39;00m\n\u001b[32m    406\u001b[39m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m408\u001b[39m \u001b[43minit_quant_block_hessian_2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    409\u001b[39m \u001b[43m    \u001b[49m\u001b[43mblock_q\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[43m    \u001b[49m\u001b[43mblock_fp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    411\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    412\u001b[39m \u001b[43m    \u001b[49m\u001b[43mactivations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    413\u001b[39m \u001b[43m    \u001b[49m\u001b[43mactivations_q\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    414\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    416\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwith_opt\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    417\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    419\u001b[39m block_q = block_q.cpu()\n\u001b[32m    420\u001b[39m block_fp = block_fp.cpu()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 248\u001b[39m, in \u001b[36minit_quant_block_hessian_2\u001b[39m\u001b[34m(block_q, block_fp, bits, activations, activations_q, causal_mask, position_embeddings, with_opt)\u001b[39m\n\u001b[32m    246\u001b[39m H = prepare_hessian(activations)\n\u001b[32m    247\u001b[39m Hq = prepare_hessian_q(activations, activations_q)\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m C = \u001b[43mprepare_C\u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mHq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    250\u001b[39m block_q_attn = block_q.self_attn\n\u001b[32m    251\u001b[39m block_fp_attn = block_fp.self_attn\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mprepare_C\u001b[39m\u001b[34m(H, Hq)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprepare_C\u001b[39m(H, Hq):\n\u001b[32m     26\u001b[39m     I = torch.eye(H.shape[\u001b[32m0\u001b[39m]).cuda() * \u001b[32m0\u001b[39m \u001b[38;5;66;03m#* H.std()\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     C = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinalg\u001b[49m\u001b[43m.\u001b[49m\u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mI\u001b[49m\u001b[43m)\u001b[49m @ Hq\n\u001b[32m     28\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m C\n",
      "\u001b[31m_LinAlgError\u001b[39m: linalg.inv: The diagonal element 2534 is zero, the inversion could not be completed because the input matrix is singular."
     ]
    }
   ],
   "source": [
    "from transformers.masking_utils import create_causal_mask\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def prepare_hessian(activations):\n",
    "    hidden_size = activations[0].shape[-1]\n",
    "    H = torch.zeros(hidden_size, hidden_size).cuda()\n",
    "    for act in activations:\n",
    "        act = act.cuda().view(-1, act.shape[-1]).float() / hidden_size ** 0.5\n",
    "        H += act.T @ act\n",
    "    return H\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def prepare_hessian_q(activations, activations_q):\n",
    "    hidden_size = activations[0].shape[-1]\n",
    "    H_q = torch.zeros(hidden_size, hidden_size).cuda()\n",
    "    for act_id in range(len(activations)):\n",
    "        act = activations[act_id].cuda().view(-1, hidden_size).float() / hidden_size ** 0.5\n",
    "        act_q = activations_q[act_id].cuda().view(-1, hidden_size).float() / hidden_size ** 0.5\n",
    "        H_q += act.T @ act_q\n",
    "    return H_q\n",
    "\n",
    "\n",
    "def prepare_C(H, Hq):\n",
    "    I = torch.eye(H.shape[0]).cuda() * 0 #* H.std()\n",
    "    C = torch.linalg.inv(H + I) @ Hq\n",
    "    return C\n",
    "\n",
    "\n",
    "def hessian_loss(layer_q, layer_fp, H, C):\n",
    "    w_q = layer_q.reconstruct_weight()\n",
    "    if C is not None:\n",
    "        w_q = w_q @ C.T\n",
    "    w = layer_fp.weight\n",
    "    delta_w = layer_q.reconstruct_weight() - layer_fp.weight\n",
    "    return torch.trace(delta_w @ H @ delta_w.T)\n",
    "\n",
    "\n",
    "def optimize_quant_params(\n",
    "        layer_q,\n",
    "        layer_fp,\n",
    "        bits,\n",
    "        H,\n",
    "        C=None,\n",
    "    ):\n",
    "    trainable_params = [layer_q.scale, layer_q.offset]    \n",
    "    optim = torch.optim.Adam(trainable_params, lr=1e-3)\n",
    "    n_steps = 100\n",
    "    \n",
    "    with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "        for i in range(n_steps):\n",
    "            optim.zero_grad()\n",
    "            loss = hessian_loss(layer_q, layer_fp, H, C)\n",
    "            if i == 0:\n",
    "                init_loss =  loss.item()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "        print(f\"{init_loss} -> {loss}\")\n",
    "\n",
    "\n",
    "def hessian_loss_ste(layer_q, layer_fp, H, bits):\n",
    "    max_int_val = 2**bits - 1\n",
    "\n",
    "    latent_weight_reshaped = layer_q.reshape_weight_for_scaling(layer_fp.weight + layer_q.weight_addition)\n",
    "    latent_weight_scaled = (latent_weight_reshaped + layer_q.offset[..., None]) / layer_q.scale[..., None]\n",
    "\n",
    "    quant_weight = torch.clamp(torch.round(latent_weight_scaled), 0, max_int_val).to(torch.uint8)\n",
    "    quant_weight_ste = quant_weight + latent_weight_scaled\n",
    "\n",
    "    layer_q.compressed_weight.copy_(quant_weight.reshape_as(layer_fp.weight))\n",
    "\n",
    "    weight_reco = quant_weight_ste * layer_q.scale[..., None] - layer_q.offset[..., None]\n",
    "    weight_reco = weight_reco.reshape_as(layer_fp.weight)\n",
    "\n",
    "    delta_w = weight_reco - layer_fp.weight\n",
    "\n",
    "    C = 1e-6\n",
    "    return torch.trace(delta_w @ H @ delta_w.T) + C * torch.sum(layer_q.weight_addition ** 2)\n",
    "\n",
    "\n",
    "def optimize_quant_params_ste(\n",
    "        layer_q,\n",
    "        layer_fp,\n",
    "        bits,\n",
    "        H\n",
    "    ):\n",
    "    layer_q.weight_addition = nn.Parameter(torch.zeros_like(layer_fp.weight.data).float())\n",
    "\n",
    "    # trainable_params = [layer_q.scale, layer_q.offset, layer_q.latent_weight]        \n",
    "    # trainable_params = [layer_q.scale, layer_q.offset]\n",
    "    trainable_params = [layer_q.weight_addition]        \n",
    "    optim = torch.optim.Adam(trainable_params, lr=1e-4)\n",
    "    n_steps = 100\n",
    "\n",
    "    with torch.amp.autocast('cuda', dtype=torch.bfloat16):\n",
    "        for i in range(n_steps):\n",
    "            optim.zero_grad()\n",
    "\n",
    "            loss = hessian_loss_ste(layer_q, layer_fp, H, bits)\n",
    "            if i == 0:\n",
    "                init_loss =  loss.item()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "        print(f\"{init_loss} -> {loss}\")\n",
    "\n",
    "    del layer_q.weight_addition\n",
    "\n",
    "\n",
    "def init_quant_block_hessian(\n",
    "        block_q,\n",
    "        block_fp,\n",
    "        bits,\n",
    "        activations,\n",
    "        causal_mask,\n",
    "        position_embeddings,\n",
    "        with_opt=True,\n",
    "        ):\n",
    "\n",
    "    ##### Attention #####\n",
    "\n",
    "    # Copy activations for the residual stream\n",
    "    residual_activations = [x.clone() for x in activations]\n",
    "\n",
    "    # Collect activations after input_layernorm\n",
    "    with torch.no_grad():\n",
    "        for act_id, act in enumerate(activations):\n",
    "            act = act.cuda()\n",
    "            act = block_fp.input_layernorm(act)\n",
    "            activations[act_id] = act.cpu()\n",
    "\n",
    "    # Initialize q,k,v-projs\n",
    "    H = prepare_hessian(activations)\n",
    "    block_q_attn = block_q.self_attn\n",
    "    block_fp_attn = block_fp.self_attn\n",
    "    for layer_name in [\"q_proj\", \"k_proj\", \"v_proj\"]:\n",
    "        layer_q = getattr(block_q_attn, layer_name)\n",
    "        layer_fp = getattr(block_fp_attn, layer_name)\n",
    "        configure_single_layer(layer_q, layer_fp, bits)\n",
    "        if with_opt:\n",
    "            optimize_quant_params(layer_q, layer_fp, bits, H)\n",
    "\n",
    "    # Collect attention-out activations\n",
    "    with torch.no_grad():\n",
    "        for act_id, act in enumerate(activations):\n",
    "            act = act.cuda()\n",
    "            act = block_fp_attn.compute_attention(\n",
    "                hidden_states=act, \n",
    "                position_embeddings=position_embeddings,\n",
    "                attention_mask=causal_mask\n",
    "            )[0]\n",
    "            activations[act_id] = act.cpu()\n",
    "\n",
    "    # Initialize o_proj\n",
    "    layer_q = block_q.self_attn.o_proj\n",
    "    layer_fp = block_fp.self_attn.o_proj\n",
    "    H = prepare_hessian(activations)\n",
    "    configure_single_layer(layer_q, layer_fp, bits)\n",
    "    if with_opt:\n",
    "        optimize_quant_params(layer_q, layer_fp, bits, H)\n",
    "\n",
    "    # Collect self_attn outs\n",
    "    with torch.no_grad():\n",
    "        for act_id, act in enumerate(activations):\n",
    "            act = act.cuda()\n",
    "            res_act = residual_activations[act_id].cuda()\n",
    "            act = block_fp.self_attn.o_proj(act)\n",
    "            activations[act_id] = (act + res_act).cpu()\n",
    "\n",
    "    ##### MLP #####\n",
    "\n",
    "    # Copy activations for the residual stream\n",
    "    residual_activations = [x.clone() for x in activations]\n",
    "\n",
    "    # Collect activations after post_attention_layernorm\n",
    "    with torch.no_grad():\n",
    "        for act_id, act in enumerate(activations):\n",
    "            act = act.cuda()\n",
    "            act = block_fp.post_attention_layernorm(act)\n",
    "            activations[act_id] = act.cpu()\n",
    "\n",
    "    # Initialize gate_proj and up_proj\n",
    "    H = prepare_hessian(activations)\n",
    "    block_q_mlp = block_q.mlp\n",
    "    block_fp_mlp = block_fp.mlp\n",
    "    for layer_name in [\"gate_proj\", \"up_proj\"]:\n",
    "        layer_q = getattr(block_q_mlp, layer_name)\n",
    "        layer_fp = getattr(block_fp_mlp, layer_name)\n",
    "        configure_single_layer(layer_q, layer_fp, bits)\n",
    "        if with_opt:\n",
    "            optimize_quant_params(layer_q, layer_fp, bits, H)\n",
    "\n",
    "    # Collect internal mlp activations\n",
    "    with torch.no_grad():\n",
    "        for act_id, act in enumerate(activations):\n",
    "            act = act.cuda()\n",
    "            act = block_fp_mlp.act_fn(block_fp.mlp.gate_proj(act)) * block_fp.mlp.up_proj(act)\n",
    "            activations[act_id] = act.cpu()\n",
    "\n",
    "    # Initialize down_proj\n",
    "    layer_q = block_q.mlp.down_proj\n",
    "    layer_fp = block_fp.mlp.down_proj\n",
    "    H = prepare_hessian(activations)\n",
    "    configure_single_layer(layer_q, layer_fp, bits)\n",
    "    if with_opt:\n",
    "        optimize_quant_params(layer_q, layer_fp, bits, H)\n",
    "\n",
    "    # Collect mlp outs\n",
    "    with torch.no_grad():\n",
    "        for act_id, act in enumerate(activations):\n",
    "            act = act.cuda()\n",
    "            res_act = residual_activations[act_id].cuda()\n",
    "            act = block_fp.mlp.down_proj(act)\n",
    "            activations[act_id] = (act + res_act).cpu()\n",
    "\n",
    "\n",
    "def init_quant_block_hessian_2(\n",
    "        block_q,\n",
    "        block_fp,\n",
    "        bits,\n",
    "        activations,\n",
    "        activations_q,\n",
    "        causal_mask,\n",
    "        position_embeddings,\n",
    "        with_opt=True,\n",
    "        ):\n",
    "\n",
    "    ##### Attention #####\n",
    "\n",
    "    # Copy activations for the residual stream\n",
    "    residual_activations = [x.clone() for x in activations]\n",
    "    residual_activations_q = [x.clone() for x in activations_q]\n",
    "\n",
    "    # Collect activations after input_layernorm\n",
    "    with torch.no_grad():\n",
    "        for act_id in range(len(activations)):\n",
    "            act = block_fp.input_layernorm(activations[act_id].cuda())            \n",
    "            activations[act_id] = act.cpu()\n",
    "            \n",
    "            act_q = block_q.input_layernorm(activations_q[act_id].cuda())            \n",
    "            activations_q[act_id] = act_q.cpu()\n",
    "\n",
    "    # Initialize q,k,v-projs\n",
    "    H = prepare_hessian(activations)\n",
    "    Hq = prepare_hessian_q(activations, activations_q)\n",
    "    C = prepare_C(H, Hq)\n",
    "\n",
    "    block_q_attn = block_q.self_attn\n",
    "    block_fp_attn = block_fp.self_attn\n",
    "    for layer_name in [\"q_proj\", \"k_proj\", \"v_proj\"]:\n",
    "        layer_q = getattr(block_q_attn, layer_name)\n",
    "        layer_fp = getattr(block_fp_attn, layer_name)\n",
    "        configure_single_layer(layer_q, layer_fp, bits)\n",
    "        if with_opt:\n",
    "            optimize_quant_params(layer_q, layer_fp, bits, H, C)\n",
    "\n",
    "    # Collect attention-out activations\n",
    "    with torch.no_grad():\n",
    "        for act_id in range(len(activations)):\n",
    "            act = block_fp_attn.compute_attention(\n",
    "                hidden_states=act.cuda(), \n",
    "                position_embeddings=position_embeddings,\n",
    "                attention_mask=causal_mask\n",
    "            )[0]\n",
    "            activations[act_id] = act.cpu()\n",
    "            \n",
    "            act_q = block_q_attn.compute_attention(\n",
    "                hidden_states=act_q.cuda(), \n",
    "                position_embeddings=position_embeddings,\n",
    "                attention_mask=causal_mask\n",
    "            )[0]\n",
    "            activations_q[act_id] = act_q.cpu()\n",
    "\n",
    "    # Initialize o_proj\n",
    "    layer_q = block_q.self_attn.o_proj\n",
    "    layer_fp = block_fp.self_attn.o_proj\n",
    "    \n",
    "    H = prepare_hessian(activations)\n",
    "    Hq = prepare_hessian_q(activations, activations_q)\n",
    "    C = prepare_C(H, Hq)\n",
    "\n",
    "    configure_single_layer(layer_q, layer_fp, bits)\n",
    "    if with_opt:\n",
    "        optimize_quant_params(layer_q, layer_fp, bits, H, C)\n",
    "\n",
    "    # Collect self_attn outs\n",
    "    with torch.no_grad():\n",
    "        for act_id in range(len(activations)):\n",
    "            act = activations[act_id].cuda()\n",
    "            res_act = residual_activations[act_id].cuda()\n",
    "            activations[act_id] = (block_fp.self_attn.o_proj(act) + res_act).cpu()\n",
    "\n",
    "            act_q = activations_q[act_id].cuda()\n",
    "            res_act_q = residual_activations_q[act_id].cuda()\n",
    "            activations_q[act_id] = (block_q.self_attn.o_proj(act_q) + res_act_q).cpu()\n",
    "\n",
    "\n",
    "    ##### MLP #####\n",
    "\n",
    "    # Copy activations for the residual stream\n",
    "    residual_activations = [x.clone() for x in activations]\n",
    "    residual_activations_q = [x.clone() for x in activations_q]\n",
    "\n",
    "    # Collect activations after post_attention_layernorm\n",
    "    with torch.no_grad():\n",
    "        for act_id in range(len(activations)):\n",
    "            act = block_fp.post_attention_layernorm(activations[act_id].cuda())\n",
    "            activations[act_id] = act.cpu()\n",
    "\n",
    "            act_q = block_q.post_attention_layernorm(activations_q[act_id].cuda())\n",
    "            activations_q[act_id] = act_q.cpu()\n",
    "\n",
    "    # Initialize gate_proj and up_proj\n",
    "    H = prepare_hessian(activations)\n",
    "    Hq = prepare_hessian_q(activations, activations_q)\n",
    "    C = prepare_C(H, Hq)\n",
    "\n",
    "    block_q_mlp = block_q.mlp\n",
    "    block_fp_mlp = block_fp.mlp\n",
    "    for layer_name in [\"gate_proj\", \"up_proj\"]:\n",
    "        layer_q = getattr(block_q_mlp, layer_name)\n",
    "        layer_fp = getattr(block_fp_mlp, layer_name)\n",
    "        configure_single_layer(layer_q, layer_fp, bits)\n",
    "        if with_opt:\n",
    "            optimize_quant_params(layer_q, layer_fp, bits, H, C)\n",
    "\n",
    "    # Collect internal mlp activations\n",
    "    with torch.no_grad():\n",
    "        for act_id in range(len(activations)):\n",
    "            act = activations[act_id].cuda()\n",
    "            act = block_fp_mlp.act_fn(block_fp.mlp.gate_proj(act)) * block_fp.mlp.up_proj(act)\n",
    "            activations[act_id] = act.cpu()\n",
    "\n",
    "            act_q = activations_q[act_id].cuda()\n",
    "            act_q = block_q_mlp.act_fn(block_q.mlp.gate_proj(act_q)) * block_q.mlp.up_proj(act_q)\n",
    "            activations_q[act_id] = act_q.cpu()\n",
    "\n",
    "    # Initialize down_proj\n",
    "    layer_q = block_q.mlp.down_proj\n",
    "    layer_fp = block_fp.mlp.down_proj\n",
    "    \n",
    "    H = prepare_hessian(activations)\n",
    "    Hq = prepare_hessian_q(activations, activations_q)\n",
    "    C = prepare_C(H, Hq)\n",
    "\n",
    "    configure_single_layer(layer_q, layer_fp, bits)\n",
    "    if with_opt:\n",
    "        optimize_quant_params(layer_q, layer_fp, bits, H, C)\n",
    "\n",
    "    # Collect mlp outs\n",
    "    with torch.no_grad():\n",
    "        for act_id in range(len(activations)):\n",
    "            act = activations[act_id].cuda()\n",
    "            res_act = residual_activations[act_id].cuda()\n",
    "            activations[act_id] = (block_fp.mlp.down_proj(act) + res_act).cpu()\n",
    "\n",
    "            act_q = activations_q[act_id].cuda()\n",
    "            res_act_q = residual_activations_q[act_id].cuda()\n",
    "            activations_q[act_id] = (block_q.mlp.down_proj(act) + res_act_q).cpu()\n",
    "\n",
    "\n",
    "def init_quant_model_hessian(model_q, model_fp, bits, dataloader):\n",
    "    embed_tokens = model_fp.get_decoder().embed_tokens.cuda()\n",
    "    embed_tokens_device = embed_tokens.weight.device\n",
    "\n",
    "    _batch = next(iter(dataloader))\n",
    "    _inputs_embeds = embed_tokens(_batch.to(embed_tokens_device))\n",
    "\n",
    "    cache_position = torch.arange(_inputs_embeds.shape[1], device=_inputs_embeds.device)\n",
    "    position_ids = cache_position.unsqueeze(0)\n",
    "    causal_mask = create_causal_mask(\n",
    "        config=model_fp.config,\n",
    "        input_embeds=_inputs_embeds,\n",
    "        attention_mask=None,\n",
    "        cache_position=cache_position,\n",
    "        past_key_values=None,\n",
    "        position_ids=position_ids,\n",
    "    )\n",
    "\n",
    "    position_embeddings = model_fp.get_decoder().rotary_emb(_inputs_embeds, position_ids)\n",
    "\n",
    "    # Prepare activations\n",
    "    activations = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            activations.append(embed_tokens(batch.to(embed_tokens_device)).cpu())\n",
    "    activations_q = [a.clone() for a in activations]\n",
    "\n",
    "    for decoder_layer_id in tqdm(range(len(model_q.get_decoder().layers))):\n",
    "        # if decoder_layer_id > 2:\n",
    "        #     break\n",
    "\n",
    "        block_q = model_q.get_decoder().layers[decoder_layer_id].cuda()\n",
    "        block_fp = model_fp.get_decoder().layers[decoder_layer_id].cuda()\n",
    "\n",
    "        # init_quant_block_hessian(\n",
    "        #     block_q,\n",
    "        #     block_fp,\n",
    "        #     bits,\n",
    "        #     activations,\n",
    "        #     causal_mask,\n",
    "        #     position_embeddings,\n",
    "        #     with_opt=True\n",
    "        # )\n",
    "\n",
    "        init_quant_block_hessian_2(\n",
    "            block_q,\n",
    "            block_fp,\n",
    "            bits,\n",
    "            activations,\n",
    "            activations_q,\n",
    "            causal_mask,\n",
    "            position_embeddings,\n",
    "            with_opt=True\n",
    "        )\n",
    "\n",
    "        block_q = block_q.cpu()\n",
    "        block_fp = block_fp.cpu()\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"dataset_name\" : \"wiki\",\n",
    "    \"split\": \"train[:10000]\",\n",
    "    # \"dataset_name\" : \"slim_pajama\",\n",
    "    # \"split\": \"train[:5000]\",\n",
    "    \"seq_length\": 4096,\n",
    "    \"n_seq\" : 64, #128,\n",
    "    \"batch_size\": 8,\n",
    "    \"random_seed\": 'no_rand'\n",
    "}\n",
    "dataloader = qlib.QATDataset(\n",
    "    config=config,\n",
    "    tokenizer=tokenizer\n",
    ").get_dataloader()\n",
    "print(len(dataloader))\n",
    "init_quant_model_hessian(qmodel, model, BITS, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa9b6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/166 [00:00<?, ?it/s]W1228 18:25:51.958000 170414 site-packages/torch/_dynamo/convert_frame.py:1358] [0/8] torch._dynamo hit config.recompile_limit (8)\n",
      "W1228 18:25:51.958000 170414 site-packages/torch/_dynamo/convert_frame.py:1358] [0/8]    function: 'reconstruct_weight' (/tmp/ipykernel_170414/2410632724.py:24)\n",
      "W1228 18:25:51.958000 170414 site-packages/torch/_dynamo/convert_frame.py:1358] [0/8]    last reason: 0/7: tensor 'self._buffers['compressed_weight']' size mismatch at index 0. expected 11008, actual 4096\n",
      "W1228 18:25:51.958000 170414 site-packages/torch/_dynamo/convert_frame.py:1358] [0/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "W1228 18:25:51.958000 170414 site-packages/torch/_dynamo/convert_frame.py:1358] [0/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html\n",
      "  4%|▍         | 7/166 [00:04<01:37,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112621.53783384823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 13/166 [00:07<01:32,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113769.37736770592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 19/166 [00:10<01:28,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109991.26188648363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 25/166 [00:13<01:25,  1.65it/s]"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"dataset_name\" : \"wiki\",\n",
    "    \"split\": \"test\",\n",
    "    \"seq_length\": 2048,\n",
    "    \"batch_size\": 1,\n",
    "    \"random_seed\": 'no_rand'\n",
    "}\n",
    "\n",
    "dataloader = qlib.QATDataset(\n",
    "    config=config,\n",
    "    tokenizer=tokenizer\n",
    ").get_dataloader()\n",
    "\n",
    "qmodel = qmodel.cuda()\n",
    "# qmodel = model.cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    with torch.amp.autocast('cuda', dtype=torch.float16):\n",
    "        res = qlib.evaluate(qmodel, dataloader, print_times=25)\n",
    "        print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392e0e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "### w3gs128\n",
    "\n",
    "# base:            8.76\n",
    "# H (wiki):        6.17 \n",
    "# H (slim-pajama): 6.31 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400911e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### w2gs64\n",
    "\n",
    "# base:            9583\n",
    "# H (wiki):        27.7\n",
    "# H (slim-pajama): 36.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579a63a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
