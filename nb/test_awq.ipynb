{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import awq_inference_engine\n",
    "\n",
    "\n",
    "device     = torch.device(\"cuda\")\n",
    "dtype      = torch.float16\n",
    "BS, SL, IN_CH, OUT_CH = 4, 4096, 4096, 11008\n",
    "bit_width  = 4\n",
    "group_size = 128\n",
    "\n",
    "qmax = 2**(bit_width-1) - 1\n",
    "qmin = -2**(bit_width-1)\n",
    "\n",
    "W_fp16 = torch.randn(OUT_CH, IN_CH, dtype=dtype, device=device)\n",
    "X_fp16 = torch.randn(BS, SL, IN_CH, dtype=dtype, device=device)\n",
    "\n",
    "# # --- 2) MANUAL INT4 QUANTIZATION OF W_fp16 ------------------------------\n",
    "# W_int = W_fp16.reshape(-1, group_size)\n",
    "# w_scale = torch.maximum(\n",
    "#     W_int.amin(dim=0) / qmin, \n",
    "#     W_int.amax(dim=0) / qmax, \n",
    "# )\n",
    "# W_int /= w_scale\n",
    "# W_int = torch.clamp(W_int, qmin, qmax).round_()\n",
    "# W_int = W_int.reshape_as(W_fp16)\n",
    "# W_int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_intweight(unpacked_qweight, interleave, kstride):\n",
    "    # unpacked_qweight: [N, K]\n",
    "    N = unpacked_qweight.shape[0]\n",
    "    K = unpacked_qweight.shape[1]\n",
    "\n",
    "    Packed_Kernel = unpacked_qweight.cpu().numpy().reshape(N, K // 32, 32)\n",
    "    # np.arange(32).reshape(4, 4, 2).transpose(1, 0, 2) => [0, 1, 8, 9, 16, 17, 24, 25, ...]\n",
    "    Packed_Kernel = Packed_Kernel.reshape(N, K // 32, 4, 4, 2).transpose(0, 1, 3, 2, 4)\n",
    "    Packed_Kernel = Packed_Kernel.reshape(N, K // 32, 32)\n",
    "\n",
    "    # reorder each 8 weights for fast dequantization\n",
    "    # [0, 1, 2, 3, 4, 5, 6, 7] => [0, 2, 4, 6, 1, 3, 5, 7]\n",
    "    Packed_Kernel = Packed_Kernel.reshape(N, K // 32, 4, 8)\n",
    "    Packed_Kernel = Packed_Kernel.reshape(N, K // 32, 4, 4, 2).transpose(0, 1, 2, 4, 3)\n",
    "    Packed_Kernel = Packed_Kernel.reshape(N, K)\n",
    "\n",
    "    # interleaving every four rows\n",
    "    Packed_Kernel = Packed_Kernel.reshape(\n",
    "        N // interleave, interleave, K // kstride, kstride\n",
    "    )\n",
    "    # N // 4, K // 64, 4, 64\n",
    "    Packed_Kernel = Packed_Kernel.transpose(0, 2, 1, 3)\n",
    "    Packed_Kernel = Packed_Kernel.reshape(\n",
    "        N // interleave, K // kstride, kstride, interleave\n",
    "    )\n",
    "    # Packing -> (N // 4, K // 64, 64)\n",
    "    Packed_Kernel = (\n",
    "        Packed_Kernel[..., 0]\n",
    "        | (Packed_Kernel[..., 1] << 4)\n",
    "        | (Packed_Kernel[..., 2] << 8)\n",
    "        | (Packed_Kernel[..., 3] << 12)\n",
    "    )\n",
    "    # reshape to (N // 4, K), FP16 format\n",
    "    Packed_Kernel = Packed_Kernel.reshape(N // interleave, K)\n",
    "    qweight = (\n",
    "        torch.tensor(Packed_Kernel.astype(\"int16\"))\n",
    "        .to(unpacked_qweight.device)\n",
    "        .contiguous()\n",
    "    )\n",
    "    return qweight\n",
    "\n",
    "\n",
    "def pseudo_quantize_tensor(\n",
    "    w, n_bit=8, zero_point=True, q_group_size=-1, inplace=False, get_scale_zp=False\n",
    "):\n",
    "    org_w_shape = w.shape\n",
    "    if q_group_size > 0:\n",
    "        assert org_w_shape[-1] % q_group_size == 0\n",
    "        w = w.reshape(-1, q_group_size)\n",
    "    assert w.dim() == 2\n",
    "    if zero_point:\n",
    "        max_val = w.amax(dim=1, keepdim=True)\n",
    "        min_val = w.amin(dim=1, keepdim=True)\n",
    "        max_int = 2**n_bit - 1\n",
    "        min_int = 0\n",
    "        scales = (max_val - min_val).clamp(min=1e-5) / max_int\n",
    "        zeros = (-torch.round(min_val / scales)).clamp_(min_int, max_int)\n",
    "    else:  # we actually never used this\n",
    "        assert min_val is None\n",
    "        max_val = w.abs().amax(dim=1, keepdim=True)\n",
    "        max_val = max_val.clamp(min=1e-5)\n",
    "        max_int = 2 ** (n_bit - 1) - 1\n",
    "        min_int = -(2 ** (n_bit - 1))\n",
    "        scales = max_val / max_int\n",
    "        zeros = 0\n",
    "\n",
    "    assert torch.isnan(scales).sum() == 0\n",
    "    assert torch.isnan(w).sum() == 0\n",
    "\n",
    "    if inplace:\n",
    "        (\n",
    "            (w.div_(scales).round_().add_(zeros)).clamp_(min_int, max_int).sub_(zeros)\n",
    "        ).mul_(scales)\n",
    "    else:\n",
    "        w = (\n",
    "            torch.clamp(torch.round(w / scales) + zeros, min_int, max_int) - zeros\n",
    "        ) * scales\n",
    "    assert torch.isnan(w).sum() == 0\n",
    "\n",
    "    w = w.reshape(org_w_shape)\n",
    "\n",
    "    if get_scale_zp:\n",
    "        return w, scales.view(w.shape[0], -1), zeros.view(w.shape[0], -1)\n",
    "    else:\n",
    "        return w\n",
    "    \n",
    "\n",
    "def make_divisible(c, divisor):\n",
    "    return (c + divisor - 1) // divisor\n",
    "\n",
    "def calculate_zeros_width(in_features, group_size=128, pack_num=8):\n",
    "    if group_size >= 128:\n",
    "        size_multiplier = 1\n",
    "    elif group_size == 64:\n",
    "        size_multiplier = 2\n",
    "    elif group_size == 32:\n",
    "        size_multiplier = 4\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    base_width = make_divisible(in_features // group_size, pack_num)\n",
    "    base_width = make_divisible(base_width, size_multiplier) * size_multiplier\n",
    "    return base_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wq_fp16, scales, zeros = pseudo_quantize_tensor(\n",
    "\tW_fp16, n_bit=bit_width, zero_point=True, get_scale_zp=True, q_group_size=group_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_zeros = zeros * scales\n",
    "dtype = scales.dtype\n",
    "\n",
    "pack_num = 32 // bit_width\n",
    "\n",
    "qscales = torch.zeros(\n",
    "            (\n",
    "                scales.shape[0],\n",
    "                calculate_zeros_width(IN_CH, group_size) * pack_num,\n",
    "            ),\n",
    "            dtype=dtype,\n",
    "            device=scales.device,\n",
    "        )\n",
    "qscales[:, : scales.shape[1]] = scales\n",
    "prepared_scales = qscales.transpose(1, 0).contiguous()\n",
    "\n",
    "intweight = []\n",
    "for idx in range(IN_CH):\n",
    "\tintweight.append(\n",
    "\t\ttorch.round(\n",
    "\t\t\t(Wq_fp16[:, idx] + scale_zeros[:, idx // group_size])\n",
    "\t\t\t/ qscales[:, idx // group_size]\n",
    "\t\t).to(torch.int)[:, None]\n",
    "\t)\n",
    "intweight = torch.cat(intweight, dim=1)\n",
    "\n",
    "qweight = pack_intweight(\n",
    "            intweight.contiguous(), interleave=4, kstride=64\n",
    "        )\n",
    "\n",
    "zeros = zeros.to(dtype=torch.int32)\n",
    "scaled_zeros = torch.zeros_like(qscales)\n",
    "# scaled_zeros[:, :scales.shape[1]] = -(qscales[:, :scales.shape[1]] * (zeros.to(torch.float32) - 8.0)).to(torch.float16)\n",
    "scaled_zeros[:, : scales.shape[1]] = -(\n",
    "    qscales[:, : scales.shape[1]] * (zeros.to(torch.float32))\n",
    ").to(dtype)\n",
    "scaled_zeros = scaled_zeros.transpose(1, 0).contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54.8 ms ± 19.4 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100 -r 7\n",
    "out = awq_inference_engine.gemm_forward_cuda_new(\n",
    "                X_fp16, qweight, prepared_scales, scaled_zeros\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 3612.18 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "39.5 ms ± 24.3 ms per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 100 -r 7\n",
    "out = torch.nn.functional.linear(input=X_fp16, weight=W_fp16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
