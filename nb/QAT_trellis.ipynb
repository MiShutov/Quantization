{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "import nip\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sys\n",
    "sys.path.append(\"/home/msst/repo/Quantization\")\n",
    "import qlib\n",
    "\n",
    "\n",
    "from bitsandbytes.optim.adamw import AdamW as AdamW8bit\n",
    "from torch.optim.adamw import AdamW\n",
    "from transformers.optimization import Adafactor\n",
    "\n",
    "\n",
    "DEVICE = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOL, Im custom!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/msst/repo/Quantization/logs/checkpoints_Llama2-7b-hf/trellis/T256_L16_V2_K2_lbits10_LowBitSym_qtip were not used when initializing QuantizedLlamaForCausalLM: {'model.layers.4.self_attn.v_proj.weight_quantizer.state_candidates', 'model.layers.11.mlp.gate_proj.weight_quantizer.state_candidates', 'model.layers.12.self_attn.q_proj.weight_quantizer.state_candidates', 'model.layers.23.self_attn.v_proj.weight_quantizer.sumdelta', 'model.layers.10.mlp.up_proj.weight_quantizer.state_candidates', 'model.layers.26.self_attn.v_proj.weight_quantizer.sumdelta', 'model.layers.16.self_attn.q_proj.weight_quantizer.sumdelta', 'model.layers.4.self_attn.o_proj.weight_quantizer.sumdelta', 'model.layers.9.self_attn.v_proj.weight_quantizer.state_candidates', 'model.layers.6.mlp.up_proj.weight_quantizer.sumdelta', 'model.layers.11.self_attn.o_proj.weight_quantizer.state_candidates', 'model.layers.16.self_attn.o_proj.weight_quantizer.sumdelta', 'model.layers.6.mlp.down_proj.weight_quantizer.state_candidates', 'model.layers.24.self_attn.k_proj.weight_quantizer.sumdelta', 'model.layers.13.self_attn.v_proj.weight_quantizer.sumdelta', 'model.layers.29.self_attn.v_proj.weight_quantizer.sumdelta', 'model.layers.9.self_attn.q_proj.weight_quantizer.sumdelta', 'model.layers.26.self_attn.q_proj.weight_quantizer.state_candidates', 'model.layers.20.mlp.gate_proj.weight_quantizer.state_candidates', 'model.layers.5.self_attn.q_proj.weight_quantizer.state_candidates', 'model.layers.20.self_attn.k_proj.weight_quantizer.sumdelta', 'model.layers.16.mlp.up_proj.weight_quantizer.state_candidates', 'model.layers.7.self_attn.o_proj.weight_quantizer.state_candidates', 'model.layers.18.self_attn.k_proj.weight_quantizer.sumdelta', 'model.layers.19.self_attn.v_proj.weight_quantizer.sumdelta', 'model.layers.24.self_attn.q_proj.weight_quantizer.state_candidates', 'model.layers.22.self_attn.o_proj.weight_quantizer.state_candidates', 'model.layers.23.mlp.down_proj.weight_quantizer.sumdelta', 'model.layers.31.self_attn.k_proj.weight_quantizer.state_candidates', 'model.layers.11.self_attn.o_proj.weight_quantizer.sumdelta', 'model.layers.17.self_attn.v_proj.weight_quantizer.state_candidates', 'model.layers.2.self_attn.v_proj.weight_quantizer.state_candidates', 'model.layers.0.mlp.down_proj.weight_quantizer.state_candidates', 'model.layers.31.self_attn.v_proj.weight_quantizer.state_candidates', 'model.layers.29.mlp.gate_proj.weight_quantizer.state_candidates', 'model.layers.18.self_attn.k_proj.weight_quantizer.state_candidates', 'model.layers.8.self_attn.v_proj.weight_quantizer.sumdelta', 'model.layers.31.mlp.gate_proj.weight_quantizer.sumdelta', 'model.layers.9.self_attn.q_proj.weight_quantizer.state_candidates', 'model.layers.22.mlp.gate_proj.weight_quantizer.sumdelta', 'model.layers.21.mlp.down_proj.weight_quantizer.sumdelta', 'model.layers.17.self_attn.o_proj.weight_quantizer.sumdelta', 'model.layers.2.mlp.down_proj.weight_quantizer.sumdelta', 'model.layers.11.self_attn.q_proj.weight_quantizer.sumdelta', 'model.layers.19.mlp.up_proj.weight_quantizer.sumdelta', 'model.layers.4.mlp.down_proj.weight_quantizer.sumdelta', 'model.layers.16.self_attn.q_proj.weight_quantizer.state_candidates', 'model.layers.6.mlp.gate_proj.weight_quantizer.state_candidates', 'model.layers.24.mlp.down_proj.weight_quantizer.sumdelta', 'model.layers.24.mlp.gate_proj.weight_quantizer.sumdelta', 'model.layers.7.self_attn.v_proj.weight_quantizer.state_candidates', 'model.layers.30.self_attn.o_proj.weight_quantizer.state_candidates', 'model.layers.11.self_attn.v_proj.weight_quantizer.state_candidates', 'model.layers.13.mlp.up_proj.weight_quantizer.state_candidates', 'model.layers.8.self_attn.o_proj.weight_quantizer.state_candidates', 'model.layers.12.self_attn.k_proj.weight_quantizer.sumdelta', 'model.layers.15.mlp.gate_proj.weight_quantizer.sumdelta', 'model.layers.16.self_attn.o_proj.weight_quantizer.state_candidates', 'model.layers.17.mlp.gate_proj.weight_quantizer.state_candidates', 'model.layers.0.mlp.gate_proj.weight_quantizer.sumdelta', 'model.layers.20.self_attn.q_proj.weight_quantizer.state_candidates', 'model.layers.16.self_attn.v_proj.weight_quantizer.sumdelta', 'model.layers.23.self_attn.q_proj.weight_quantizer.state_candidates', 'model.layers.21.self_attn.o_proj.weight_quantizer.sumdelta', 'model.layers.29.self_attn.o_proj.weight_quantizer.sumdelta', 'model.layers.15.self_attn.o_proj.weight_quantizer.sumdelta', 'model.layers.7.self_attn.q_proj.weight_quantizer.state_candidates', 'model.layers.12.self_attn.k_proj.weight_quantizer.state_candidates', 'model.layers.19.self_attn.k_proj.weight_quantizer.state_candidates', 'model.layers.26.self_attn.k_proj.weight_quantizer.sumdelta', 'model.layers.30.mlp.down_proj.weight_quantizer.sumdelta', 'model.layers.12.self_attn.o_proj.weight_quantizer.sumdelta', 'model.layers.15.self_attn.v_proj.weight_quantizer.state_candidates', 'model.layers.9.mlp.gate_proj.weight_quantizer.sumdelta', 'model.layers.17.mlp.gate_proj.weight_quantizer.sumdelta', 'model.layers.28.self_attn.k_proj.weight_quantizer.state_candidates', 'model.layers.22.self_attn.q_proj.weight_quantizer.sumdelta', 'model.layers.20.self_attn.v_proj.weight_quantizer.state_candidates', 'model.layers.20.self_attn.o_proj.weight_quantizer.state_candidates', 'model.layers.8.mlp.gate_proj.weight_quantizer.sumdelta', 'model.layers.23.mlp.up_proj.weight_quantizer.state_candidates', 'model.layers.23.self_attn.k_proj.weight_quantizer.sumdelta', 'model.layers.22.self_attn.v_proj.weight_quantizer.state_candidates', 'model.layers.3.mlp.up_proj.weight_quantizer.state_candidates', 'model.layers.1.self_attn.v_proj.weight_quantizer.state_candidates', 'model.layers.18.mlp.gate_proj.weight_quantizer.state_candidates', 'model.layers.22.self_attn.q_proj.weight_quantizer.state_candidates', 'model.layers.4.self_attn.v_proj.weight_quantizer.sumdelta', 'model.layers.27.self_attn.k_proj.weight_quantizer.state_candidates', 'model.layers.25.self_attn.o_proj.weight_quantizer.state_candidates', 'model.layers.2.mlp.up_proj.weight_quantizer.state_candidates', 'model.layers.13.self_attn.k_proj.weight_quantizer.state_candidates', 'model.layers.18.mlp.up_proj.weight_quantizer.sumdelta', 'model.layers.31.self_attn.q_proj.weight_quantizer.sumdelta', 'model.layers.22.self_attn.o_proj.weight_quantizer.sumdelta', 'model.layers.16.self_attn.k_proj.weight_quantizer.state_candidates', 'model.layers.17.mlp.down_proj.weight_quantizer.sumdelta', 'model.layers.0.mlp.gate_proj.weight_quantizer.state_candidates', 'model.layers.10.self_attn.v_proj.weight_quantizer.sumdelta', 'model.layers.1.self_attn.v_proj.weight_quantizer.sumdelta', 'model.layers.15.self_attn.q_proj.weight_quantizer.state_candidates', 'model.layers.26.mlp.up_proj.weight_quantizer.sumdelta', 'model.layers.16.mlp.down_proj.weight_quantizer.sumdelta', 'model.layers.19.self_attn.o_proj.weight_quantizer.state_candidates', 'model.layers.13.mlp.gate_proj.weight_quantizer.sumdelta', 'model.layers.21.self_attn.k_proj.weight_quantizer.state_candidates', 'model.layers.31.mlp.up_proj.weight_quantizer.state_candidates', 'model.layers.1.self_attn.o_proj.weight_quantizer.state_candidates', 'model.layers.5.self_attn.q_proj.weight_quantizer.sumdelta', 'model.layers.26.mlp.gate_proj.weight_quantizer.state_candidates', 'model.layers.21.self_attn.q_proj.weight_quantizer.sumdelta', 'model.layers.25.self_attn.v_proj.weight_quantizer.state_candidates', 'model.layers.10.self_attn.q_proj.weight_quantizer.state_candidates', 'model.layers.9.mlp.down_proj.weight_quantizer.state_candidates', 'model.layers.4.mlp.down_proj.weight_quantizer.state_candidates', 'model.layers.10.self_attn.k_proj.weight_quantizer.sumdelta', 'model.layers.8.self_attn.k_proj.weight_quantizer.state_candidates', 'model.layers.28.mlp.gate_proj.weight_quantizer.state_candidates', 'model.layers.1.self_attn.q_proj.weight_quantizer.sumdelta', 'model.layers.11.self_attn.q_proj.weight_quantizer.state_candidates', 'model.layers.28.self_attn.q_proj.weight_quantizer.state_candidates', 'model.layers.9.self_attn.o_proj.weight_quantizer.state_candidates', 'model.layers.26.mlp.up_proj.weight_quantizer.state_candidates', 'model.layers.3.mlp.gate_proj.weight_quantizer.state_candidates', 'model.layers.1.mlp.down_proj.weight_quantizer.sumdelta', 'model.layers.16.self_attn.k_proj.weight_quantizer.sumdelta', 'model.layers.27.self_attn.o_proj.weight_quantizer.sumdelta', 'model.layers.25.self_attn.q_proj.weight_quantizer.state_candidates', 'model.layers.18.self_attn.o_proj.weight_quantizer.sumdelta', 'model.layers.0.self_attn.q_proj.weight_quantizer.sumdelta', 'model.layers.27.self_attn.q_proj.weight_quantizer.sumdelta', 'model.layers.26.mlp.gate_proj.weight_quantizer.sumdelta', 'model.layers.4.mlp.up_proj.weight_quantizer.sumdelta', 'model.layers.3.mlp.down_proj.weight_quantizer.sumdelta', 'model.layers.20.self_attn.q_proj.weight_quantizer.sumdelta', 'model.layers.11.mlp.down_proj.weight_quantizer.sumdelta', 'model.layers.11.mlp.up_proj.weight_quantizer.state_candidates', 'model.layers.14.self_attn.k_proj.weight_quantizer.sumdelta', 'model.layers.3.self_attn.v_proj.weight_quantizer.state_candidates', 'model.layers.29.mlp.up_proj.weight_quantizer.state_candidates', 'model.layers.7.self_attn.o_proj.weight_quantizer.sumdelta', 'model.layers.16.mlp.up_proj.weight_quantizer.sumdelta', 'model.layers.24.self_attn.v_proj.weight_quantizer.state_candidates', 'model.layers.4.mlp.gate_proj.weight_quantizer.sumdelta', 'model.layers.2.self_attn.o_proj.weight_quantizer.sumdelta', 'model.layers.23.self_attn.v_proj.weight_quantizer.state_candidates', 'model.layers.9.mlp.gate_proj.weight_quantizer.state_candidates', 'model.layers.0.mlp.up_proj.weight_quantizer.sumdelta', 'model.layers.0.self_attn.q_proj.weight_quantizer.state_candidates', 'model.layers.29.mlp.up_proj.weight_quantizer.sumdelta', 'model.layers.12.self_attn.v_proj.weight_quantizer.state_candidates', 'model.layers.11.self_attn.k_proj.weight_quantizer.sumdelta', 'model.layers.8.self_attn.q_proj.weight_quantizer.state_candidates', 'model.layers.17.self_attn.q_proj.weight_quantizer.state_candidates', 'model.layers.15.self_attn.q_proj.weight_quantizer.sumdelta', 'model.layers.23.mlp.gate_proj.weight_quantizer.state_candidates', 'model.layers.17.self_attn.k_proj.weight_quantizer.sumdelta', 'model.layers.21.self_attn.v_proj.weight_quantizer.state_candidates', 'model.layers.19.mlp.gate_proj.weight_quantizer.state_candidates', 'model.layers.24.mlp.up_proj.weight_quantizer.sumdelta', 'model.layers.6.self_attn.o_proj.weight_quantizer.state_candidates', 'model.layers.20.mlp.down_proj.weight_quantizer.sumdelta', 'model.layers.9.mlp.up_proj.weight_quantizer.state_candidates', 'model.layers.25.mlp.down_proj.weight_quantizer.state_candidates', 'model.layers.10.self_attn.k_proj.weight_quantizer.state_candidates', 'model.layers.2.mlp.gate_proj.weight_quantizer.state_candidates', 'model.layers.9.self_attn.v_proj.weight_quantizer.sumdelta', 'model.layers.8.mlp.down_proj.weight_quantizer.state_candidates', 'model.layers.30.self_attn.v_proj.weight_quantizer.sumdelta', 'model.layers.14.mlp.down_proj.weight_quantizer.sumdelta', 'model.layers.26.self_attn.o_proj.weight_quantizer.state_candidates', 'model.layers.2.self_attn.k_proj.weight_quantizer.state_candidates', 'model.layers.27.mlp.up_proj.weight_quantizer.sumdelta', 'model.layers.10.self_attn.q_proj.weight_quantizer.sumdelta', 'model.layers.1.self_attn.k_proj.weight_quantizer.state_candidates', 'model.layers.7.self_attn.q_proj.weight_quantizer.sumdelta', 'model.layers.14.mlp.gate_proj.weight_quantizer.sumdelta', 'model.layers.8.self_attn.k_proj.weight_quantizer.sumdelta', 'model.layers.10.self_attn.o_proj.weight_quantizer.state_candidates', 'model.layers.13.self_attn.q_proj.weight_quantizer.state_candidates', 'model.layers.0.self_attn.o_proj.weight_quantizer.sumdelta', 'model.layers.18.self_attn.v_proj.weight_quantizer.state_candidates', 'model.layers.28.mlp.down_proj.weight_quantizer.state_candidates', 'model.layers.3.mlp.down_proj.weight_quantizer.state_candidates', 'model.layers.22.mlp.down_proj.weight_quantizer.sumdelta', 'model.layers.4.mlp.gate_proj.weight_quantizer.state_candidates', 'model.layers.23.self_attn.o_proj.weight_quantizer.state_candidates', 'model.layers.2.mlp.down_proj.weight_quantizer.state_candidates', 'model.layers.21.self_attn.q_proj.weight_quantizer.state_candidates', 'model.layers.5.self_attn.o_proj.weight_quantizer.state_candidates', 'model.layers.18.self_attn.o_proj.weight_quantizer.state_candidates', 'model.layers.8.self_attn.v_proj.weight_quantizer.state_candidates', 'model.layers.0.self_attn.v_proj.weight_quantizer.state_candidates', 'model.layers.10.mlp.up_proj.weight_quantizer.sumdelta', 'model.layers.18.self_attn.q_proj.weight_quantizer.sumdelta', 'model.layers.24.self_attn.o_proj.weight_quantizer.state_candidates', 'model.layers.3.self_attn.v_proj.weight_quantizer.sumdelta', 'model.layers.8.mlp.up_proj.weight_quantizer.sumdelta', 'model.layers.7.self_attn.k_proj.weight_quantizer.sumdelta', 'model.layers.27.mlp.gate_proj.weight_quantizer.sumdelta', 'model.layers.12.mlp.gate_proj.weight_quantizer.sumdelta', 'model.layers.23.self_attn.o_proj.weight_quantizer.sumdelta', 'model.layers.17.mlp.up_proj.weight_quantizer.state_candidates', 'model.layers.20.self_attn.o_proj.weight_quantizer.sumdelta', 'model.layers.19.self_attn.q_proj.weight_quantizer.state_candidates', 'model.layers.28.self_attn.k_proj.weight_quantizer.sumdelta', 'model.layers.3.self_attn.k_proj.weight_quantizer.state_candidates', 'model.layers.27.mlp.down_proj.weight_quantizer.state_candidates', 'model.layers.2.mlp.up_proj.weight_quantizer.sumdelta', 'model.layers.14.mlp.up_proj.weight_quantizer.sumdelta', 'model.layers.4.self_attn.q_proj.weight_quantizer.sumdelta', 'model.layers.0.self_attn.v_proj.weight_quantizer.sumdelta', 'model.layers.15.mlp.down_proj.weight_quantizer.state_candidates', 'model.layers.21.mlp.gate_proj.weight_quantizer.sumdelta', 'model.layers.21.self_attn.k_proj.weight_quantizer.sumdelta', 'model.layers.31.self_attn.q_proj.weight_quantizer.state_candidates', 'model.layers.22.mlp.down_proj.weight_quantizer.state_candidates', 'model.layers.1.mlp.down_proj.weight_quantizer.state_candidates', 'model.layers.26.self_attn.v_proj.weight_quantizer.state_candidates', 'model.layers.21.mlp.up_proj.weight_quantizer.state_candidates', 'model.layers.4.self_attn.k_proj.weight_quantizer.sumdelta', 'model.layers.13.mlp.down_proj.weight_quantizer.state_candidates', 'model.layers.6.self_attn.q_proj.weight_quantizer.sumdelta', 'model.layers.12.self_attn.q_proj.weight_quantizer.sumdelta', 'model.layers.0.self_attn.k_proj.weight_quantizer.sumdelta', 'model.layers.12.self_attn.o_proj.weight_quantizer.state_candidates', 'model.layers.11.mlp.down_proj.weight_quantizer.state_candidates', 'model.layers.30.self_attn.k_proj.weight_quantizer.sumdelta', 'model.layers.0.mlp.up_proj.weight_quantizer.state_candidates', 'model.layers.4.mlp.up_proj.weight_quantizer.state_candidates', 'model.layers.18.self_attn.v_proj.weight_quantizer.sumdelta', 'model.layers.26.mlp.down_proj.weight_quantizer.sumdelta', 'model.layers.6.mlp.gate_proj.weight_quantizer.sumdelta', 'model.layers.14.mlp.up_proj.weight_quantizer.state_candidates', 'model.layers.18.mlp.down_proj.weight_quantizer.sumdelta', 'model.layers.23.self_attn.k_proj.weight_quantizer.state_candidates', 'model.layers.25.self_attn.o_proj.weight_quantizer.sumdelta', 'model.layers.31.mlp.down_proj.weight_quantizer.state_candidates', 'model.layers.9.self_attn.k_proj.weight_quantizer.sumdelta', 'model.layers.5.self_attn.k_proj.weight_quantizer.sumdelta', 'model.layers.6.self_attn.k_proj.weight_quantizer.state_candidates', 'model.layers.15.self_attn.v_proj.weight_quantizer.sumdelta', 'model.layers.28.self_attn.v_proj.weight_quantizer.state_candidates', 'model.layers.26.self_attn.o_proj.weight_quantizer.sumdelta', 'model.layers.9.mlp.down_proj.weight_quantizer.sumdelta', 'model.layers.8.self_attn.o_proj.weight_quantizer.sumdelta', 'model.layers.4.self_attn.o_proj.weight_quantizer.state_candidates', 'model.layers.29.mlp.gate_proj.weight_quantizer.sumdelta', 'model.layers.1.self_attn.k_proj.weight_quantizer.sumdelta', 'model.layers.26.mlp.down_proj.weight_quantizer.state_candidates', 'model.layers.5.mlp.down_proj.weight_quantizer.sumdelta', 'model.layers.28.mlp.up_proj.weight_quantizer.state_candidates', 'model.layers.7.mlp.up_proj.weight_quantizer.sumdelta', 'model.layers.7.mlp.down_proj.weight_quantizer.sumdelta', 'model.layers.8.mlp.up_proj.weight_quantizer.state_candidates', 'model.layers.25.mlp.gate_proj.weight_quantizer.state_candidates', 'model.layers.25.self_attn.k_proj.weight_quantizer.state_candidates', 'model.layers.27.mlp.up_proj.weight_quantizer.state_candidates', 'model.layers.23.mlp.up_proj.weight_quantizer.sumdelta', 'model.layers.21.mlp.up_proj.weight_quantizer.sumdelta', 'model.layers.14.self_attn.v_proj.weight_quantizer.sumdelta', 'model.layers.6.self_attn.v_proj.weight_quantizer.state_candidates', 'model.layers.23.mlp.gate_proj.weight_quantizer.sumdelta', 'model.layers.3.mlp.gate_proj.weight_quantizer.sumdelta', 'model.layers.13.mlp.gate_proj.weight_quantizer.state_candidates', 'model.layers.19.self_attn.o_proj.weight_quantizer.sumdelta', 'model.layers.27.mlp.gate_proj.weight_quantizer.state_candidates', 'model.layers.2.self_attn.q_proj.weight_quantizer.state_candidates', 'model.layers.12.mlp.gate_proj.weight_quantizer.state_candidates', 'model.layers.19.mlp.gate_proj.weight_quantizer.sumdelta', 'model.layers.13.mlp.down_proj.weight_quantizer.sumdelta', 'model.layers.30.self_attn.k_proj.weight_quantizer.state_candidates', 'model.layers.29.self_attn.k_proj.weight_quantizer.sumdelta', 'model.layers.7.mlp.up_proj.weight_quantizer.state_candidates', 'model.layers.25.self_attn.q_proj.weight_quantizer.sumdelta', 'model.layers.10.mlp.gate_proj.weight_quantizer.sumdelta', 'model.layers.20.self_attn.k_proj.weight_quantizer.state_candidates', 'model.layers.24.mlp.down_proj.weight_quantizer.state_candidates', 'model.layers.20.self_attn.v_proj.weight_quantizer.sumdelta', 'model.layers.27.self_attn.k_proj.weight_quantizer.sumdelta', 'model.layers.12.mlp.up_proj.weight_quantizer.sumdelta', 'model.layers.30.mlp.up_proj.weight_quantizer.state_candidates', 'model.layers.16.mlp.gate_proj.weight_quantizer.sumdelta', 'model.layers.6.self_attn.v_proj.weight_quantizer.sumdelta', 'model.layers.6.self_attn.o_proj.weight_quantizer.sumdelta', 'model.layers.7.mlp.down_proj.weight_quantizer.state_candidates', 'model.layers.6.self_attn.q_proj.weight_quantizer.state_candidates', 'model.layers.3.self_attn.o_proj.weight_quantizer.sumdelta', 'model.layers.24.mlp.up_proj.weight_quantizer.state_candidates', 'model.layers.29.self_attn.o_proj.weight_quantizer.state_candidates', 'model.layers.30.mlp.gate_proj.weight_quantizer.state_candidates', 'model.layers.20.mlp.up_proj.weight_quantizer.state_candidates', 'model.layers.15.self_attn.k_proj.weight_quantizer.state_candidates', 'model.layers.22.self_attn.k_proj.weight_quantizer.state_candidates', 'model.layers.13.self_attn.o_proj.weight_quantizer.sumdelta', 'model.layers.28.self_attn.v_proj.weight_quantizer.sumdelta', 'model.layers.17.self_attn.k_proj.weight_quantizer.state_candidates', 'model.layers.1.mlp.up_proj.weight_quantizer.state_candidates', 'model.layers.7.mlp.gate_proj.weight_quantizer.state_candidates', 'model.layers.21.mlp.down_proj.weight_quantizer.state_candidates', 'model.layers.23.self_attn.q_proj.weight_quantizer.sumdelta', 'model.layers.0.self_attn.o_proj.weight_quantizer.state_candidates', 'model.layers.30.mlp.down_proj.weight_quantizer.state_candidates', 'model.layers.3.self_attn.q_proj.weight_quantizer.sumdelta', 'model.layers.10.self_attn.v_proj.weight_quantizer.state_candidates', 'model.layers.21.self_attn.o_proj.weight_quantizer.state_candidates', 'model.layers.27.self_attn.q_proj.weight_quantizer.state_candidates', 'model.layers.30.mlp.gate_proj.weight_quantizer.sumdelta', 'model.layers.24.self_attn.v_proj.weight_quantizer.sumdelta', 'model.layers.15.self_attn.o_proj.weight_quantizer.state_candidates', 'model.layers.7.self_attn.v_proj.weight_quantizer.sumdelta', 'model.layers.31.mlp.up_proj.weight_quantizer.sumdelta', 'model.layers.10.mlp.down_proj.weight_quantizer.state_candidates', 'model.layers.25.mlp.up_proj.weight_quantizer.sumdelta', 'model.layers.14.self_attn.q_proj.weight_quantizer.sumdelta', 'model.layers.28.mlp.up_proj.weight_quantizer.sumdelta', 'model.layers.26.self_attn.q_proj.weight_quantizer.sumdelta', 'model.layers.5.mlp.gate_proj.weight_quantizer.sumdelta', 'model.layers.28.self_attn.q_proj.weight_quantizer.sumdelta', 'model.layers.12.mlp.down_proj.weight_quantizer.state_candidates', 'model.layers.1.self_attn.o_proj.weight_quantizer.sumdelta', 'model.layers.0.mlp.down_proj.weight_quantizer.sumdelta', 'model.layers.25.mlp.up_proj.weight_quantizer.state_candidates', 'model.layers.24.self_attn.o_proj.weight_quantizer.sumdelta', 'model.layers.28.mlp.down_proj.weight_quantizer.sumdelta', 'model.layers.29.self_attn.k_proj.weight_quantizer.state_candidates', 'model.layers.8.mlp.gate_proj.weight_quantizer.state_candidates', 'model.layers.7.self_attn.k_proj.weight_quantizer.state_candidates', 'model.layers.4.self_attn.q_proj.weight_quantizer.state_candidates', 'model.layers.21.self_attn.v_proj.weight_quantizer.sumdelta', 'model.layers.7.mlp.gate_proj.weight_quantizer.sumdelta', 'model.layers.17.self_attn.v_proj.weight_quantizer.sumdelta', 'model.layers.31.self_attn.v_proj.weight_quantizer.sumdelta', 'model.layers.4.self_attn.k_proj.weight_quantizer.state_candidates', 'model.layers.5.self_attn.o_proj.weight_quantizer.sumdelta', 'model.layers.27.self_attn.o_proj.weight_quantizer.state_candidates', 'model.layers.16.mlp.down_proj.weight_quantizer.state_candidates', 'model.layers.31.self_attn.o_proj.weight_quantizer.sumdelta', 'model.layers.9.self_attn.k_proj.weight_quantizer.state_candidates', 'model.layers.30.self_attn.o_proj.weight_quantizer.sumdelta', 'model.layers.13.self_attn.q_proj.weight_quantizer.sumdelta', 'model.layers.5.self_attn.v_proj.weight_quantizer.sumdelta', 'model.layers.31.mlp.gate_proj.weight_quantizer.state_candidates', 'model.layers.2.self_attn.q_proj.weight_quantizer.sumdelta', 'model.layers.19.self_attn.k_proj.weight_quantizer.sumdelta', 'model.layers.1.self_attn.q_proj.weight_quantizer.state_candidates', 'model.layers.27.self_attn.v_proj.weight_quantizer.sumdelta', 'model.layers.3.self_attn.o_proj.weight_quantizer.state_candidates', 'model.layers.14.self_attn.k_proj.weight_quantizer.state_candidates', 'model.layers.22.self_attn.k_proj.weight_quantizer.sumdelta', 'model.layers.29.mlp.down_proj.weight_quantizer.sumdelta', 'model.layers.27.mlp.down_proj.weight_quantizer.sumdelta', 'model.layers.6.mlp.down_proj.weight_quantizer.sumdelta', 'model.layers.2.self_attn.o_proj.weight_quantizer.state_candidates', 'model.layers.13.self_attn.o_proj.weight_quantizer.state_candidates', 'model.layers.24.mlp.gate_proj.weight_quantizer.state_candidates', 'model.layers.31.mlp.down_proj.weight_quantizer.sumdelta', 'model.layers.24.self_attn.q_proj.weight_quantizer.sumdelta', 'model.layers.5.mlp.down_proj.weight_quantizer.state_candidates', 'model.layers.3.self_attn.q_proj.weight_quantizer.state_candidates', 'model.layers.5.self_attn.v_proj.weight_quantizer.state_candidates', 'model.layers.15.mlp.gate_proj.weight_quantizer.state_candidates', 'model.layers.27.self_attn.v_proj.weight_quantizer.state_candidates', 'model.layers.12.self_attn.v_proj.weight_quantizer.sumdelta', 'model.layers.10.mlp.gate_proj.weight_quantizer.state_candidates', 'model.layers.13.self_attn.k_proj.weight_quantizer.sumdelta', 'model.layers.14.self_attn.q_proj.weight_quantizer.state_candidates', 'model.layers.30.self_attn.q_proj.weight_quantizer.sumdelta', 'model.layers.6.mlp.up_proj.weight_quantizer.state_candidates', 'model.layers.29.self_attn.q_proj.weight_quantizer.sumdelta', 'model.layers.2.self_attn.v_proj.weight_quantizer.sumdelta', 'model.layers.16.self_attn.v_proj.weight_quantizer.state_candidates', 'model.layers.18.mlp.gate_proj.weight_quantizer.sumdelta', 'model.layers.8.self_attn.q_proj.weight_quantizer.sumdelta', 'model.layers.19.self_attn.q_proj.weight_quantizer.sumdelta', 'model.layers.29.self_attn.v_proj.weight_quantizer.state_candidates', 'model.layers.14.self_attn.v_proj.weight_quantizer.state_candidates', 'model.layers.17.self_attn.o_proj.weight_quantizer.state_candidates', 'model.layers.30.self_attn.q_proj.weight_quantizer.state_candidates', 'model.layers.15.mlp.up_proj.weight_quantizer.sumdelta', 'model.layers.22.mlp.up_proj.weight_quantizer.state_candidates', 'model.layers.1.mlp.gate_proj.weight_quantizer.sumdelta', 'model.layers.25.mlp.gate_proj.weight_quantizer.sumdelta', 'model.layers.22.mlp.gate_proj.weight_quantizer.state_candidates', 'model.layers.9.self_attn.o_proj.weight_quantizer.sumdelta', 'model.layers.28.mlp.gate_proj.weight_quantizer.sumdelta', 'model.layers.28.self_attn.o_proj.weight_quantizer.sumdelta', 'model.layers.14.self_attn.o_proj.weight_quantizer.sumdelta', 'model.layers.1.mlp.up_proj.weight_quantizer.sumdelta', 'model.layers.10.mlp.down_proj.weight_quantizer.sumdelta', 'model.layers.8.mlp.down_proj.weight_quantizer.sumdelta', 'model.layers.5.self_attn.k_proj.weight_quantizer.state_candidates', 'model.layers.11.mlp.gate_proj.weight_quantizer.sumdelta', 'model.layers.3.self_attn.k_proj.weight_quantizer.sumdelta', 'model.layers.12.mlp.up_proj.weight_quantizer.state_candidates', 'model.layers.13.self_attn.v_proj.weight_quantizer.state_candidates', 'model.layers.23.mlp.down_proj.weight_quantizer.state_candidates', 'model.layers.15.mlp.down_proj.weight_quantizer.sumdelta', 'model.layers.15.mlp.up_proj.weight_quantizer.state_candidates', 'model.layers.15.self_attn.k_proj.weight_quantizer.sumdelta', 'model.layers.24.self_attn.k_proj.weight_quantizer.state_candidates', 'model.layers.17.mlp.down_proj.weight_quantizer.state_candidates', 'model.layers.19.mlp.up_proj.weight_quantizer.state_candidates', 'model.layers.26.self_attn.k_proj.weight_quantizer.state_candidates', 'model.layers.18.mlp.down_proj.weight_quantizer.state_candidates', 'model.layers.14.mlp.gate_proj.weight_quantizer.state_candidates', 'model.layers.6.self_attn.k_proj.weight_quantizer.sumdelta', 'model.layers.17.self_attn.q_proj.weight_quantizer.sumdelta', 'model.layers.18.mlp.up_proj.weight_quantizer.state_candidates', 'model.layers.5.mlp.up_proj.weight_quantizer.state_candidates', 'model.layers.19.self_attn.v_proj.weight_quantizer.state_candidates', 'model.layers.20.mlp.up_proj.weight_quantizer.sumdelta', 'model.layers.9.mlp.up_proj.weight_quantizer.sumdelta', 'model.layers.5.mlp.gate_proj.weight_quantizer.state_candidates', 'model.layers.13.mlp.up_proj.weight_quantizer.sumdelta', 'model.layers.19.mlp.down_proj.weight_quantizer.sumdelta', 'model.layers.11.self_attn.k_proj.weight_quantizer.state_candidates', 'model.layers.11.self_attn.v_proj.weight_quantizer.sumdelta', 'model.layers.11.mlp.up_proj.weight_quantizer.sumdelta', 'model.layers.17.mlp.up_proj.weight_quantizer.sumdelta', 'model.layers.19.mlp.down_proj.weight_quantizer.state_candidates', 'model.layers.29.self_attn.q_proj.weight_quantizer.state_candidates', 'model.layers.14.mlp.down_proj.weight_quantizer.state_candidates', 'model.layers.16.mlp.gate_proj.weight_quantizer.state_candidates', 'model.layers.31.self_attn.k_proj.weight_quantizer.sumdelta', 'model.layers.20.mlp.down_proj.weight_quantizer.state_candidates', 'model.layers.30.mlp.up_proj.weight_quantizer.sumdelta', 'model.layers.3.mlp.up_proj.weight_quantizer.sumdelta', 'model.layers.0.self_attn.k_proj.weight_quantizer.state_candidates', 'model.layers.21.mlp.gate_proj.weight_quantizer.state_candidates', 'model.layers.10.self_attn.o_proj.weight_quantizer.sumdelta', 'model.layers.20.mlp.gate_proj.weight_quantizer.sumdelta', 'model.layers.31.self_attn.o_proj.weight_quantizer.state_candidates', 'model.layers.29.mlp.down_proj.weight_quantizer.state_candidates', 'model.layers.28.self_attn.o_proj.weight_quantizer.state_candidates', 'model.layers.2.self_attn.k_proj.weight_quantizer.sumdelta', 'model.layers.25.self_attn.v_proj.weight_quantizer.sumdelta', 'model.layers.25.self_attn.k_proj.weight_quantizer.sumdelta', 'model.layers.5.mlp.up_proj.weight_quantizer.sumdelta', 'model.layers.2.mlp.gate_proj.weight_quantizer.sumdelta', 'model.layers.25.mlp.down_proj.weight_quantizer.sumdelta', 'model.layers.22.mlp.up_proj.weight_quantizer.sumdelta', 'model.layers.30.self_attn.v_proj.weight_quantizer.state_candidates', 'model.layers.12.mlp.down_proj.weight_quantizer.sumdelta', 'model.layers.22.self_attn.v_proj.weight_quantizer.sumdelta', 'model.layers.18.self_attn.q_proj.weight_quantizer.state_candidates', 'model.layers.14.self_attn.o_proj.weight_quantizer.state_candidates', 'model.layers.1.mlp.gate_proj.weight_quantizer.state_candidates'}\n",
      "- This IS expected if you are initializing QuantizedLlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing QuantizedLlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "qmodel = qlib.QuantizedLlamaForCausalLM.from_pretrained(\n",
    "\t'/home/msst/repo/Quantization/logs/checkpoints_Llama2-7b-hf/trellis/T256_L16_V2_K2_lbits10_LowBitSym_qtip',\n",
    "    torch_dtype=torch.float16,\n",
    ").to(DEVICE)\n",
    "\n",
    "model_name = 'Llama2-7b-hf'\n",
    "tokenizer = qlib.load_tokenizer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnowledgeDistillationDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        #self.lm_head = lm_head.cpu() #.to(DEVICE)\n",
    "        self.len = len(self.data['decoder_output'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        assert index < self.len\n",
    "        input_ids = self.data['input'][index]\n",
    "        decoder_output = self.data['decoder_output'][index]\n",
    "        return {\n",
    "            'input_ids' : input_ids[0],\n",
    "            'decoder_output' : decoder_output[0]\n",
    "        }\n",
    "\n",
    "kd_data_path = '/mnt/ssd_storage/ml/weights/vc_data/Llama2-7b-hf/kd_data'\n",
    "#dataset_name = 'kd_data_redpajama_decoder_output_small'\n",
    "dataset_name = 'kd_data_redpajama_decoder_output'\n",
    "kd_data = torch.load(\n",
    "    f'{kd_data_path}/{dataset_name}.pth', \n",
    "    weights_only=True\n",
    ")\n",
    "\n",
    "kd_dataset = KnowledgeDistillationDataset(kd_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for param_name, param in qmodel.named_parameters():\n",
    "#     print(param_name)\n",
    "\n",
    "trainable_params = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    qmodel.half()\n",
    "    for param_name, param in qmodel.named_parameters():\n",
    "        if ('SU' in param_name) or ('SV' in param_name) or ('norm' in param_name):\n",
    "            trainable_params.append(param)\n",
    "            param.requires_grad = True\n",
    "        \n",
    "\n",
    "optimizer_cls = Adafactor\n",
    "optimizer_kwargs = {\n",
    "    'optimizer_dict': [\n",
    "        {\n",
    "            'params': trainable_params,\n",
    "            'lr': 1e-4, \n",
    "            'weight_decay': 0.0\n",
    "        },\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.checkpoint import checkpoint\n",
    "qmodel.cuda()\n",
    "qmodel.train()\n",
    "qmodel._set_gradient_checkpointing(enable=True, gradient_checkpointing_func=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f288b0273a98407d8acb440db310e1b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainalble params: 2764800 Fraction of fp model: 0.04103041660765378%\n",
      "ce_loss: tensor(1.4146, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/msst/miniconda3/envs/qenv/lib/python3.10/site-packages/torch/autograd/graph.py:825: UserWarning: quip_lib::hadamard: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce_loss: tensor(3.8428, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "n_steps = 500\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    max_steps=n_steps,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    \n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1,\n",
    "    output_dir = './output_dir',\n",
    "    save_strategy=\"no\",\n",
    "    \n",
    "    # label_names=[],\n",
    "    # per_device_eval_batch_size=1,\n",
    "    # eval_strategy='steps',\n",
    "    # eval_steps=10,\n",
    "    # eval_on_start=True,\n",
    "\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "fp_model = qlib.load_model('Llama2-7b-hf', torch_dtype=torch.float16)\n",
    "lm_head = fp_model.lm_head.to(DEVICE)\n",
    "\n",
    "\n",
    "n_trainable_params = sum(p.numel() for p in trainable_params)\n",
    "fp_model_params = sum(p.numel() for p in fp_model.parameters())\n",
    "fraq_of_fp_model_params = 100 * n_trainable_params / fp_model_params\n",
    "print(\n",
    "\t'Trainalble params:', n_trainable_params,\n",
    "\tf'Fraction of fp model: {fraq_of_fp_model_params}%',\n",
    ") \n",
    "\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        outputs = model(inputs['input_ids'], labels=inputs['input_ids'])\n",
    "        ce_loss = outputs.loss\n",
    "        print(\"ce_loss:\", ce_loss)\n",
    "\n",
    "        qmodel_logits = outputs.logits\n",
    "        fpmodel_logits = lm_head(inputs['decoder_output'])\n",
    "        \n",
    "        n_tokens = torch.prod(torch.tensor(qmodel_logits.shape[:-1]))\n",
    "        \n",
    "        T = 1 #2\n",
    "        kd_loss = torch.nn.functional.kl_div(\n",
    "                torch.log_softmax(qmodel_logits / T, dim=-1),\n",
    "                torch.softmax(fpmodel_logits / T, dim=-1),\n",
    "                reduction='batchmean',\n",
    "            ) * (T**2) / n_tokens\n",
    "        \n",
    "        # print(\"kd_loss:\", kd_loss.item(), 'ce_loss:', ce_loss.item())\n",
    "        # sim = torch.argmax(qmodel_logits, dim=-1) == torch.argmax(fpmodel_logits, dim=-1)\n",
    "        # print('right_labels ratio:', sim.sum().item() / n_tokens)\n",
    "\n",
    "        #total_loss = ce_loss + 0.1 * kd_loss\n",
    "        total_loss = kd_loss\n",
    "\n",
    "        return (total_loss, outputs) if return_outputs else total_loss\n",
    "\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=qmodel,\n",
    "    args=training_args,\n",
    "    #train_dataset=train_dataset,\n",
    "    train_dataset=kd_dataset,\n",
    "    #eval_dataset=eval_dataset,\n",
    "    optimizer_cls_and_kwargs=(optimizer_cls, optimizer_kwargs)\n",
    ")\n",
    "\n",
    "trainer.can_return_loss = True\n",
    "with torch.amp.autocast('cuda', dtype=torch.float16):\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#qmodel.save_pretrained('/home/msst/repo/Quantization/logs/checkpoints_Llama2-7b-hf/trellis/T256_L16_V2_K2_lbits10_LowBitSym_qtip' + f'_QAT_KDLoss_{n_steps}steps')\n",
    "qmodel.save_pretrained('/home/msst/repo/Quantization/logs/checkpoints_Llama2-7b-hf/trellis/T256_L16_V2_K2_lbits10_LowBitSym_qtip' + f'_QAT_KDLoss_425steps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
