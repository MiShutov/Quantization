{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "import os\n",
    "import nip\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import sys\n",
    "sys.path.append(\"/home/msst/repo/Quantization\")\n",
    "import qlib\n",
    "\n",
    "from torch.optim.adamw import AdamW\n",
    "from transformers.optimization import Adafactor\n",
    "from bitsandbytes.optim.adamw import AdamW as AdamW8bit\n",
    "\n",
    "DEVICE = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOL, Im custom!\n"
     ]
    }
   ],
   "source": [
    "path_to_checkpoints = \"/home/msst/repo/Quantization/logs/checkpoints_Llama2-7b-hf/trellis/\"\n",
    "chpnt_name = 'gs128_lukashevich_ptq_act8bit_calib'\n",
    "\n",
    "qmodel = qlib.QuantizedLlamaForCausalLM.from_pretrained(\n",
    "\tos.path.join(path_to_checkpoints, chpnt_name),\n",
    "    torch_dtype=torch.float16,\n",
    ").to(DEVICE)\n",
    "qmodel.cuda()\n",
    "qmodel.train()\n",
    "qmodel._set_gradient_checkpointing(enable=True, gradient_checkpointing_func=checkpoint)\n",
    "\n",
    "\n",
    "model_name = 'Llama2-7b-hf'\n",
    "tokenizer = qlib.load_tokenizer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainalble params: 1.139e+06 Fraction of fp model params: 0.017%\n",
      "Trainalble params: 1.313e+08 Fraction of fp model params: 1.949%\n",
      "Trainalble params: 5.059e+07 Fraction of fp model params: 0.751%\n",
      "Trainalble params: 1.139e+06 Fraction of fp model params: 0.017%\n"
     ]
    }
   ],
   "source": [
    "optimizer_cls = AdamW\n",
    "#optimizer_cls = Adafactor\n",
    "\n",
    "trainable_params_g1 = []\n",
    "trainable_params_g2 = []\n",
    "trainable_params_g3 = []\n",
    "trainable_params_g4 = []\n",
    "# lr_g1 = 5e-4\n",
    "# lr_g2 = 5e-5\n",
    "# lr_g3 = 5e-4\n",
    "# lr_g4 = 1e-5\n",
    "\n",
    "lr_g1 = 1e-4\n",
    "lr_g2 = 1e-5\n",
    "lr_g3 = 1e-4\n",
    "lr_g4 = 1e-5\n",
    "\n",
    "with torch.no_grad():\n",
    "    for param_name, param in qmodel.named_parameters():\n",
    "        if ('SU' in param_name) or ('SV' in param_name):\n",
    "            param.data = param.data.to(torch.float32)\n",
    "            trainable_params_g1.append(param)\n",
    "            param.requires_grad = True\n",
    "        if ('lm_head' in param_name) or ('norm' in param_name):\n",
    "        #if ('norm' in param_name):\n",
    "            param.data = param.data.to(torch.float32)\n",
    "            trainable_params_g2.append(param)\n",
    "            param.requires_grad = True\n",
    "        if ('weight_scales' in param_name):\n",
    "            param.data = param.data.to(torch.float32)\n",
    "            trainable_params_g3.append(param)\n",
    "            param.requires_grad = True\n",
    "        if ('act_scale' in param_name):\n",
    "            param.data = param.data.to(torch.float32)\n",
    "            trainable_params_g4.append(param)\n",
    "            param.requires_grad = True\n",
    "\n",
    "optimizer_kwargs = {\n",
    "    'optimizer_dict': [\n",
    "        # {\n",
    "        #     'params': trainable_params_g1,\n",
    "        #     'lr': lr_g1, \n",
    "        #     'weight_decay': 0.0\n",
    "        # },\n",
    "        # {\n",
    "        #     'params': trainable_params_g2,\n",
    "        #     'lr': lr_g2, \n",
    "        #     'weight_decay': 0.0\n",
    "        # },\n",
    "        # {\n",
    "        #     'params': trainable_params_g3,\n",
    "        #     'lr': lr_g3, \n",
    "        #     'weight_decay': 0.0\n",
    "        # },\n",
    "        {\n",
    "            'params': trainable_params_g4,\n",
    "            'lr': lr_g4, \n",
    "            'weight_decay': 0.0\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "\n",
    "N_FP_MODEL_PARAMS = 6738415616 # Llama2-7B\n",
    "\n",
    "def print_number_of_params(group):\n",
    "    n_trainable_params = sum(p.numel() for p in group)\n",
    "    fraq_of_fp_model_params = 100 * n_trainable_params / N_FP_MODEL_PARAMS\n",
    "    print(\n",
    "        f'Trainalble params: {n_trainable_params:.3e}',\n",
    "        f'Fraction of fp model params: {fraq_of_fp_model_params:.3f}%',\n",
    "    )\n",
    "\n",
    "for g in (trainable_params_g1, \n",
    "          trainable_params_g2, \n",
    "          trainable_params_g3, \n",
    "          trainable_params_g4):\n",
    "    print_number_of_params(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d967043215954553b4926a6a61ebcf50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scales: torch.float32\n",
      "SU: torch.float32\n",
      "lm_head: torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/msst/miniconda3/envs/qenv/lib/python3.10/site-packages/torch/autograd/graph.py:825: UserWarning: quip_lib::hadamard: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 7/10 06:57 < 04:10, 0.01 it/s, Epoch 0.11/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>7.989200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>11.075700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>10.106000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>9.898900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>11.325200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 91\u001b[0m\n\u001b[1;32m     89\u001b[0m trainer\u001b[38;5;241m.\u001b[39mcan_return_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16):\n\u001b[0;32m---> 91\u001b[0m    \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/qenv/lib/python3.10/site-packages/transformers/trainer.py:2173\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2171\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2172\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2174\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2178\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/qenv/lib/python3.10/site-packages/transformers/trainer.py:2533\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2526\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2527\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2528\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2529\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2530\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2531\u001b[0m )\n\u001b[1;32m   2532\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2533\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2535\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2536\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2537\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2538\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2539\u001b[0m ):\n\u001b[1;32m   2540\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2541\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/envs/qenv/lib/python3.10/site-packages/transformers/trainer.py:3714\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   3711\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_accepts_loss_kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3712\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n\u001b[0;32m-> 3714\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3716\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/miniconda3/envs/qenv/lib/python3.10/site-packages/accelerate/accelerator.py:2329\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2328\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2329\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/qenv/lib/python3.10/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/qenv/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/qenv/lib/python3.10/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_steps = 10 #100\n",
    "grad_acc = 5 \n",
    "loss_type = 'CE'\n",
    "\n",
    "\n",
    "if loss_type=='KD' or loss_type=='KD+CE':\n",
    "    kd_data_path = '/mnt/ssd_storage/ml/weights/vc_data/Llama2-7b-hf/kd_data'\n",
    "    #dataset_name = 'kd_data_redpajama_decoder_output_small'\n",
    "    dataset_name = 'kd_data_redpajama_decoder_output'\n",
    "    kd_data = torch.load(\n",
    "        f'{kd_data_path}/{dataset_name}.pth',\n",
    "        weights_only=True\n",
    "    )\n",
    "    train_dataset = qlib.KnowledgeDistillationDataset(kd_data)\n",
    "else:\n",
    "    train_dataset = qlib.QATDataset(\n",
    "        #config=nip.load('/home/msst/repo/Quantization/configs/data/redpajama_train_seqlen4096_large.yaml'),\n",
    "        config=nip.load('/home/msst/repo/Quantization/configs/data/redpajama_train_seqlen4096.yaml'),\n",
    "        tokenizer=qlib.load_tokenizer('Llama2-7b-hf'),\n",
    "        return_dict=True\n",
    "    )\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    max_steps=n_steps,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=grad_acc,\n",
    "    gradient_checkpointing=True,\n",
    "    \n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1,\n",
    "    output_dir = './output_dir',\n",
    "    save_strategy=\"no\",\n",
    "    \n",
    "    # label_names=[],\n",
    "    # per_device_eval_batch_size=1,\n",
    "    # eval_strategy='steps',\n",
    "    # eval_steps=10,\n",
    "    # eval_on_start=True,\n",
    "\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "if loss_type=='KD' or loss_type=='KD+CE':\n",
    "    fp_model = qlib.load_model('Llama2-7b-hf', torch_dtype=torch.float16)\n",
    "    lm_head = fp_model.lm_head.to(torch.float32).to(DEVICE)\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        outputs = model(inputs['input_ids'], labels=inputs['input_ids'])\n",
    "        if loss_type=='CE' or loss_type=='KD+CE':\n",
    "            ce_loss = outputs.loss\n",
    "        \n",
    "        if loss_type=='KD' or loss_type=='KD+CE':\n",
    "            qmodel_logits = outputs.logits\n",
    "            fpmodel_logits = lm_head(inputs['decoder_output'].to(torch.float32))\n",
    "            \n",
    "            n_tokens = torch.prod(torch.tensor(qmodel_logits.shape[:-1]))\n",
    "            \n",
    "            T = 1 #2\n",
    "            kd_loss = torch.nn.functional.kl_div(\n",
    "                    torch.log_softmax(qmodel_logits / T, dim=-1),\n",
    "                    torch.softmax(fpmodel_logits / T, dim=-1),\n",
    "                    reduction='batchmean',\n",
    "                ) * (T**2) / n_tokens\n",
    "        \n",
    "        if loss_type=='KD':\n",
    "            total_loss = kd_loss\n",
    "        elif loss_type=='CE':\n",
    "            total_loss = ce_loss\n",
    "        elif loss_type=='KD+CE':\n",
    "            print(\"kd_loss:\", kd_loss.item(), 'ce_loss:', ce_loss.item())\n",
    "            total_loss = ce_loss + 10 * kd_loss\n",
    "\n",
    "        return (total_loss, outputs) if return_outputs else total_loss\n",
    "\n",
    "print('scales:', qmodel.get_decoder().layers[31].mlp.up_proj.weight_scales.dtype)\n",
    "print('SU:', qmodel.get_decoder().layers[31].mlp.up_proj.SU.dtype)\n",
    "print('lm_head:', qmodel.lm_head.weight.dtype)\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=qmodel,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    #eval_dataset=eval_dataset,\n",
    "    optimizer_cls_and_kwargs=(optimizer_cls, optimizer_kwargs)\n",
    ")\n",
    "\n",
    "trainer.can_return_loss = True\n",
    "with torch.amp.autocast('cuda', dtype=torch.float16):\n",
    "   trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_save = os.path.join(\n",
    "\tpath_to_checkpoints, \n",
    "\tf'{chpnt_name}_qat_{loss_type}_{n_steps}steps_{grad_acc}ga_{optimizer_cls.__name__}'\n",
    ")\n",
    "qmodel.half().save_pretrained(path_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
