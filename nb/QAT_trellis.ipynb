{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "import os\n",
    "import nip\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import sys\n",
    "sys.path.append(\"/home/msst/repo/Quantization\")\n",
    "import qlib\n",
    "\n",
    "from torch.optim.adamw import AdamW\n",
    "from transformers.optimization import Adafactor\n",
    "from bitsandbytes.optim.adamw import AdamW as AdamW8bit\n",
    "\n",
    "DEVICE = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOL, Im custom!\n"
     ]
    }
   ],
   "source": [
    "path_to_checkpoints = \"/home/msst/repo/Quantization/logs/checkpoints_Llama2-7b-hf/trellis/\"\n",
    "chpnt_name = 'T256_L16_V2_K2_cbs10_LowBitSym_qtip_ptq_bs5'\n",
    "\n",
    "qmodel = qlib.QuantizedLlamaForCausalLM.from_pretrained(\n",
    "\tos.path.join(path_to_checkpoints, chpnt_name),\n",
    "    torch_dtype=torch.float16,\n",
    ").to(DEVICE)\n",
    "qmodel.cuda()\n",
    "qmodel.train()\n",
    "qmodel._set_gradient_checkpointing(enable=True, gradient_checkpointing_func=checkpoint)\n",
    "\n",
    "\n",
    "model_name = 'Llama2-7b-hf'\n",
    "tokenizer = qlib.load_tokenizer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainalble params: 2.499e+06 Fraction of fp model params: 0.037%\n",
      "Trainalble params: 1.313e+08 Fraction of fp model params: 1.949%\n",
      "Trainalble params: 2.530e+07 Fraction of fp model params: 0.375%\n",
      "Trainalble params: 0.000e+00 Fraction of fp model params: 0.000%\n"
     ]
    }
   ],
   "source": [
    "optimizer_cls = AdamW\n",
    "#optimizer_cls = Adafactor\n",
    "\n",
    "trainable_params_g1 = []\n",
    "trainable_params_g2 = []\n",
    "trainable_params_g3 = []\n",
    "trainable_params_g4 = []\n",
    "lr_g1 = 5e-4\n",
    "lr_g2 = 5e-5\n",
    "lr_g3 = 5e-4\n",
    "lr_g4 = 1e-4\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for param_name, param in qmodel.named_parameters():\n",
    "        if ('SU' in param_name) or ('SV' in param_name):\n",
    "            param.data = param.data.to(torch.float32)\n",
    "            trainable_params_g1.append(param)\n",
    "            param.requires_grad = True\n",
    "        if ('lm_head' in param_name) or ('norm' in param_name):\n",
    "        #if ('norm' in param_name):\n",
    "            param.data = param.data.to(torch.float32)\n",
    "            trainable_params_g2.append(param)\n",
    "            param.requires_grad = True\n",
    "        if ('scales' in param_name):\n",
    "            param.data = param.data.to(torch.float32)\n",
    "            trainable_params_g3.append(param)\n",
    "            param.requires_grad = True\n",
    "        # if ('act_scale' in param_name):\n",
    "        #     param.data = param.data.to(torch.float32)\n",
    "        #     trainable_params_g4.append(param)\n",
    "        #     param.requires_grad = True\n",
    "\n",
    "optimizer_kwargs = {\n",
    "    'optimizer_dict': [\n",
    "        {\n",
    "            'params': trainable_params_g1,\n",
    "            'lr': lr_g1, \n",
    "            'weight_decay': 0.0\n",
    "        },\n",
    "        {\n",
    "            'params': trainable_params_g2,\n",
    "            'lr': lr_g2, \n",
    "            'weight_decay': 0.0\n",
    "        },\n",
    "        {\n",
    "            'params': trainable_params_g3,\n",
    "            'lr': lr_g3, \n",
    "            'weight_decay': 0.0\n",
    "        },\n",
    "        # {\n",
    "        #     'params': trainable_params_g4,\n",
    "        #     'lr': lr_g4, \n",
    "        #     'weight_decay': 0.0\n",
    "        # },\n",
    "    ]\n",
    "}\n",
    "\n",
    "N_FP_MODEL_PARAMS = 6738415616 # Llama2-7B\n",
    "\n",
    "def print_number_of_params(group):\n",
    "    n_trainable_params = sum(p.numel() for p in group)\n",
    "    fraq_of_fp_model_params = 100 * n_trainable_params / N_FP_MODEL_PARAMS\n",
    "    print(\n",
    "        f'Trainalble params: {n_trainable_params:.3e}',\n",
    "        f'Fraction of fp model params: {fraq_of_fp_model_params:.3f}%',\n",
    "    )\n",
    "\n",
    "for g in (trainable_params_g1, \n",
    "          trainable_params_g2, \n",
    "          trainable_params_g3, \n",
    "          trainable_params_g4):\n",
    "    print_number_of_params(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15140f40cca5492480f711fdc288ae9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scales: torch.float32\n",
      "SU: torch.float32\n",
      "lm_head: torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/msst/miniconda3/envs/qenv/lib/python3.10/site-packages/torch/autograd/graph.py:825: UserWarning: quip_lib::hadamard: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  2/125 : < :, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_steps = 125\n",
    "grad_acc = 20\n",
    "loss_type = 'CE'\n",
    "\n",
    "\n",
    "if loss_type=='KD' or loss_type=='KD+CE':\n",
    "    kd_data_path = '/mnt/ssd_storage/ml/weights/vc_data/Llama2-7b-hf/kd_data'\n",
    "    #dataset_name = 'kd_data_redpajama_decoder_output_small'\n",
    "    dataset_name = 'kd_data_redpajama_decoder_output'\n",
    "    kd_data = torch.load(\n",
    "        f'{kd_data_path}/{dataset_name}.pth',\n",
    "        weights_only=True\n",
    "    )\n",
    "    train_dataset = qlib.KnowledgeDistillationDataset(kd_data)\n",
    "else:\n",
    "    train_dataset = qlib.QATDataset(\n",
    "        config=nip.load('/home/msst/repo/Quantization/configs/data/redpajama_train_seqlen4096_large.yaml'),\n",
    "        tokenizer=qlib.load_tokenizer('Llama2-7b-hf'),\n",
    "        return_dict=True\n",
    "    )\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    max_steps=n_steps,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=grad_acc,\n",
    "    gradient_checkpointing=True,\n",
    "    \n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1,\n",
    "    output_dir = './output_dir',\n",
    "    save_strategy=\"no\",\n",
    "    \n",
    "    # label_names=[],\n",
    "    # per_device_eval_batch_size=1,\n",
    "    # eval_strategy='steps',\n",
    "    # eval_steps=10,\n",
    "    # eval_on_start=True,\n",
    "\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "if loss_type=='KD' or loss_type=='KD+CE':\n",
    "    fp_model = qlib.load_model('Llama2-7b-hf', torch_dtype=torch.float16)\n",
    "    lm_head = fp_model.lm_head.to(torch.float32).to(DEVICE)\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        outputs = model(inputs['input_ids'], labels=inputs['input_ids'])\n",
    "        if loss_type=='CE' or loss_type=='KD+CE':\n",
    "            ce_loss = outputs.loss\n",
    "        \n",
    "        if loss_type=='KD' or loss_type=='KD+CE':\n",
    "            qmodel_logits = outputs.logits\n",
    "            fpmodel_logits = lm_head(inputs['decoder_output'].to(torch.float32))\n",
    "            \n",
    "            n_tokens = torch.prod(torch.tensor(qmodel_logits.shape[:-1]))\n",
    "            \n",
    "            T = 1 #2\n",
    "            kd_loss = torch.nn.functional.kl_div(\n",
    "                    torch.log_softmax(qmodel_logits / T, dim=-1),\n",
    "                    torch.softmax(fpmodel_logits / T, dim=-1),\n",
    "                    reduction='batchmean',\n",
    "                ) * (T**2) / n_tokens\n",
    "        \n",
    "        if loss_type=='KD':\n",
    "            total_loss = kd_loss\n",
    "        elif loss_type=='CE':\n",
    "            total_loss = ce_loss\n",
    "        elif loss_type=='KD+CE':\n",
    "            print(\"kd_loss:\", kd_loss.item(), 'ce_loss:', ce_loss.item())\n",
    "            total_loss = ce_loss + 10 * kd_loss\n",
    "\n",
    "        return (total_loss, outputs) if return_outputs else total_loss\n",
    "\n",
    "print('scales:', qmodel.get_decoder().layers[31].mlp.up_proj.scales.dtype)\n",
    "print('SU:', qmodel.get_decoder().layers[31].mlp.up_proj.SU.dtype)\n",
    "print('lm_head:', qmodel.lm_head.weight.dtype)\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=qmodel,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    #eval_dataset=eval_dataset,\n",
    "    optimizer_cls_and_kwargs=(optimizer_cls, optimizer_kwargs)\n",
    ")\n",
    "\n",
    "trainer.can_return_loss = True\n",
    "with torch.amp.autocast('cuda', dtype=torch.float16):\n",
    "   trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_save = os.path.join(\n",
    "\tpath_to_checkpoints, \n",
    "\tf'{chpnt_name}_qat_{loss_type}_{n_steps}steps_{grad_acc}ga_{optimizer_cls.__name__}'\n",
    ")\n",
    "qmodel.half().save_pretrained(path_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
