{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "import os\n",
    "import nip\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "import sys\n",
    "sys.path.append(\"/home/msst/repo/Quantization\")\n",
    "import qlib\n",
    "\n",
    "from torch.optim.adamw import AdamW\n",
    "from transformers.optimization import Adafactor\n",
    "from bitsandbytes.optim.adamw import AdamW as AdamW8bit\n",
    "\n",
    "DEVICE = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOL, Im custom!\n"
     ]
    }
   ],
   "source": [
    "path_to_checkpoints = \"/home/msst/repo/Quantization/logs/checkpoints_Llama2-7b-hf/trellis/\"\n",
    "chpnt_name = 'gs128_lukashevich_ptq'\n",
    "\n",
    "qmodel = qlib.QuantizedLlamaForCausalLM.from_pretrained(\n",
    "\tos.path.join(path_to_checkpoints, chpnt_name),\n",
    "    torch_dtype=torch.float16,\n",
    ").to(DEVICE)\n",
    "qmodel.cuda()\n",
    "qmodel.train()\n",
    "qmodel._set_gradient_checkpointing(enable=True, gradient_checkpointing_func=checkpoint)\n",
    "\n",
    "\n",
    "model_name = 'Llama2-7b-hf'\n",
    "tokenizer = qlib.load_tokenizer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainalble params: 1.139e+06 Fraction of fp model params: 0.017%\n",
      "Trainalble params: 1.313e+08 Fraction of fp model params: 1.949%\n",
      "Trainalble params: 5.059e+07 Fraction of fp model params: 0.751%\n",
      "Trainalble params: 0.000e+00 Fraction of fp model params: 0.000%\n"
     ]
    }
   ],
   "source": [
    "optimizer_cls = AdamW\n",
    "#optimizer_cls = Adafactor\n",
    "\n",
    "trainable_params_g1 = []\n",
    "trainable_params_g2 = []\n",
    "trainable_params_g3 = []\n",
    "trainable_params_g4 = []\n",
    "lr_g1 = 5e-4\n",
    "lr_g2 = 5e-5\n",
    "lr_g3 = 5e-4\n",
    "lr_g4 = 1e-5\n",
    "# lr_g1 = 1e-4\n",
    "# lr_g2 = 1e-5\n",
    "# lr_g3 = 1e-4\n",
    "# lr_g4 = 1e-5\n",
    "\n",
    "with torch.no_grad():\n",
    "    for param_name, param in qmodel.named_parameters():\n",
    "        if ('SU' in param_name) or ('SV' in param_name):\n",
    "            param.data = param.data.to(torch.float32)\n",
    "            trainable_params_g1.append(param)\n",
    "            param.requires_grad = True\n",
    "        if ('lm_head' in param_name) or ('norm' in param_name):\n",
    "        #if ('norm' in param_name):\n",
    "            param.data = param.data.to(torch.float32)\n",
    "            trainable_params_g2.append(param)\n",
    "            param.requires_grad = True\n",
    "        if ('weight_scales' in param_name):\n",
    "            param.data = param.data.to(torch.float32)\n",
    "            trainable_params_g3.append(param)\n",
    "            param.requires_grad = True\n",
    "        if ('act_scale' in param_name):\n",
    "            param.data = param.data.to(torch.float32)\n",
    "            trainable_params_g4.append(param)\n",
    "            param.requires_grad = True\n",
    "\n",
    "optimizer_kwargs = {\n",
    "    'optimizer_dict': [\n",
    "        {\n",
    "            'params': trainable_params_g1,\n",
    "            'lr': lr_g1, \n",
    "            'weight_decay': 0.0\n",
    "        },\n",
    "        {\n",
    "            'params': trainable_params_g2,\n",
    "            'lr': lr_g2, \n",
    "            'weight_decay': 0.0\n",
    "        },\n",
    "        {\n",
    "            'params': trainable_params_g3,\n",
    "            'lr': lr_g3, \n",
    "            'weight_decay': 0.0\n",
    "        },\n",
    "        # {\n",
    "        #     'params': trainable_params_g4,\n",
    "        #     'lr': lr_g4, \n",
    "        #     'weight_decay': 0.0\n",
    "        # },\n",
    "    ]\n",
    "}\n",
    "\n",
    "N_FP_MODEL_PARAMS = 6738415616 # Llama2-7B\n",
    "\n",
    "def print_number_of_params(group):\n",
    "    n_trainable_params = sum(p.numel() for p in group)\n",
    "    fraq_of_fp_model_params = 100 * n_trainable_params / N_FP_MODEL_PARAMS\n",
    "    print(\n",
    "        f'Trainalble params: {n_trainable_params:.3e}',\n",
    "        f'Fraction of fp model params: {fraq_of_fp_model_params:.3f}%',\n",
    "    )\n",
    "\n",
    "for g in (trainable_params_g1, \n",
    "          trainable_params_g2, \n",
    "          trainable_params_g3, \n",
    "          trainable_params_g4):\n",
    "    print_number_of_params(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2de7342a0a2c4a77b16600a644ee0171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scales: torch.float32\n",
      "SU: torch.float32\n",
      "lm_head: torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/msst/miniconda3/envs/qenv/lib/python3.10/site-packages/torch/autograd/graph.py:825: UserWarning: quip_lib::hadamard: an autograd kernel was not registered to the Autograd key(s) but we are trying to backprop through it. This may lead to silently incorrect behavior. This behavior is deprecated and will be removed in a future version of PyTorch. If your operator is differentiable, please ensure you have registered an autograd kernel to the correct Autograd key (e.g. DispatchKey::Autograd, DispatchKey::CompositeImplicitAutograd). If your operator is not differentiable, or to squash this warning and use the previous behavior, please register torch::CppFunction::makeFallthrough() to DispatchKey::Autograd. (Triggered internally at ../torch/csrc/autograd/autograd_not_implemented_fallback.cpp:62.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 9:02:56, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>98.396900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>105.789600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>109.340700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>103.520900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>105.845200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>100.664500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>101.519000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>102.216700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>98.379800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>97.813700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>97.189100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>95.681100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>103.187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>90.507800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>92.161700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>89.907000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>95.861900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>98.801500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>95.134200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>94.455200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>96.149200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>96.488600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>93.103300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>92.273200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>95.203600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>91.413400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>93.245100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>99.100500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>95.449200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>97.542700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>93.314100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>96.261600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>94.240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>93.502100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>97.099700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>90.850400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>97.924500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>90.814600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>94.434400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>92.172800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>94.182700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>88.697000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>91.251700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>93.740700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>90.941200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>95.572300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>91.844200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>94.799800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>88.536600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>98.667400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_steps = 50 #100\n",
    "grad_acc = 50\n",
    "loss_type = 'CE'\n",
    "\n",
    "\n",
    "if loss_type=='KD' or loss_type=='KD+CE':\n",
    "    kd_data_path = '/mnt/ssd_storage/ml/weights/vc_data/Llama2-7b-hf/kd_data'\n",
    "    #dataset_name = 'kd_data_redpajama_decoder_output_small'\n",
    "    dataset_name = 'kd_data_redpajama_decoder_output'\n",
    "    kd_data = torch.load(\n",
    "        f'{kd_data_path}/{dataset_name}.pth',\n",
    "        weights_only=True\n",
    "    )\n",
    "    train_dataset = qlib.KnowledgeDistillationDataset(kd_data)\n",
    "else:\n",
    "    train_dataset = qlib.QATDataset(\n",
    "        config=nip.load('/home/msst/repo/Quantization/configs/data/redpajama_train_seqlen4096_large.yaml'),\n",
    "        #config=nip.load('/home/msst/repo/Quantization/configs/data/redpajama_train_seqlen4096.yaml'),\n",
    "        tokenizer=qlib.load_tokenizer('Llama2-7b-hf'),\n",
    "        return_dict=True\n",
    "    )\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    max_steps=n_steps,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=grad_acc,\n",
    "    gradient_checkpointing=True,\n",
    "    \n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1,\n",
    "    output_dir = './output_dir',\n",
    "    save_strategy=\"no\",\n",
    "    \n",
    "    # label_names=[],\n",
    "    # per_device_eval_batch_size=1,\n",
    "    # eval_strategy='steps',\n",
    "    # eval_steps=10,\n",
    "    # eval_on_start=True,\n",
    "\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "if loss_type=='KD' or loss_type=='KD+CE':\n",
    "    fp_model = qlib.load_model('Llama2-7b-hf', torch_dtype=torch.float16)\n",
    "    lm_head = fp_model.lm_head.to(torch.float32).to(DEVICE)\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        outputs = model(inputs['input_ids'], labels=inputs['input_ids'])\n",
    "        if loss_type=='CE' or loss_type=='KD+CE':\n",
    "            ce_loss = outputs.loss\n",
    "        \n",
    "        if loss_type=='KD' or loss_type=='KD+CE':\n",
    "            qmodel_logits = outputs.logits\n",
    "            fpmodel_logits = lm_head(inputs['decoder_output'].to(torch.float32))\n",
    "            \n",
    "            n_tokens = torch.prod(torch.tensor(qmodel_logits.shape[:-1]))\n",
    "            \n",
    "            T = 1 #2\n",
    "            kd_loss = torch.nn.functional.kl_div(\n",
    "                    torch.log_softmax(qmodel_logits / T, dim=-1),\n",
    "                    torch.softmax(fpmodel_logits / T, dim=-1),\n",
    "                    reduction='batchmean',\n",
    "                ) * (T**2) / n_tokens\n",
    "        \n",
    "        if loss_type=='KD':\n",
    "            total_loss = kd_loss\n",
    "        elif loss_type=='CE':\n",
    "            total_loss = ce_loss\n",
    "        elif loss_type=='KD+CE':\n",
    "            print(\"kd_loss:\", kd_loss.item(), 'ce_loss:', ce_loss.item())\n",
    "            total_loss = ce_loss + 10 * kd_loss\n",
    "\n",
    "        return (total_loss, outputs) if return_outputs else total_loss\n",
    "\n",
    "print('scales:', qmodel.get_decoder().layers[31].mlp.up_proj.weight_scales.dtype)\n",
    "print('SU:', qmodel.get_decoder().layers[31].mlp.up_proj.SU.dtype)\n",
    "print('lm_head:', qmodel.lm_head.weight.dtype)\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=qmodel,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    #eval_dataset=eval_dataset,\n",
    "    optimizer_cls_and_kwargs=(optimizer_cls, optimizer_kwargs)\n",
    ")\n",
    "\n",
    "trainer.can_return_loss = True\n",
    "with torch.amp.autocast('cuda', dtype=torch.float16):\n",
    "   trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_save = os.path.join(\n",
    "\tpath_to_checkpoints, \n",
    "\tf'{chpnt_name}_qat_{loss_type}_{n_steps}steps_{grad_acc}ga_{optimizer_cls.__name__}'\n",
    ")\n",
    "qmodel.half().save_pretrained(path_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
