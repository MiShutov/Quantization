{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append(\"/home/msst/repo/Quantization\")\n",
    "import qlib\n",
    "DEVICE = 'cuda:0'\n",
    "from qlib.utils.incoherence_preprocessing.incoherence_process_functions import incoherence_process, incoherence_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:05<00:00, 12.49it/s]\n",
      "100%|██████████| 64/64 [00:04<00:00, 14.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error (unpack): 0.021 ± 0.001\n",
      "error (fast)  : 0.021 ± 0.001\n"
     ]
    }
   ],
   "source": [
    "m = 2048\n",
    "n = 2048\n",
    "\n",
    "w = torch.randn(m, n, device=DEVICE)\n",
    "\n",
    "quantizer = qlib.trellis_quantizer(\n",
    "\tL=16,\n",
    "\tK=3,\n",
    "\tV=2,\n",
    "\tT=256,\n",
    "\tdecode_mode=\"LowBitSym\", \n",
    "\ttlut_bits=10\n",
    ").to(DEVICE)\n",
    "\n",
    "reco, states = quantizer.quantize(w)\n",
    "packed = quantizer.pack_trellis(states)\n",
    "\n",
    "reco_ref = quantizer.reconstruct_weight(packed, w.shape)\n",
    "err_ref  = torch.mean((reco_ref  - w)**2, dim=-1)\n",
    "print(f\"error (unpack): {err_ref.mean():.3f} ± {err_ref.std():.3f}\")\n",
    "\n",
    "\n",
    "reco_fast = quantizer.reconstruct_weight_fast(packed, w.shape)\n",
    "err_fast = torch.mean((reco_fast - w)**2, dim=-1)\n",
    "print(f\"error (fast)  : {err_fast.mean():.3f} ± {err_fast.std():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Scaling per group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c1107643036470b9f77643d7749f330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [00:19<00:00, 12.94it/s]\n",
      "100%|██████████| 256/256 [00:20<00:00, 12.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: 1.377e-05 ± 2.306e-06\n"
     ]
    }
   ],
   "source": [
    "fp_model = qlib.load_model('Llama2-7b-hf', torch_dtype=torch.float16)\n",
    "W = fp_model.get_submodule('model.layers.0.self_attn.q_proj').weight.data.to(DEVICE)\n",
    "Wr, SU, SV = incoherence_preprocess(W)\n",
    "\n",
    "Wr = Wr.reshape(-1, 256)\n",
    "scales = Wr.std()\n",
    "Wr_scaled = Wr / scales\n",
    "Wr_scaled = Wr_scaled.reshape_as(W)\n",
    "\n",
    "\n",
    "quantizer = qlib.trellis_quantizer(\n",
    "\tL=16,\n",
    "\tK=2,\n",
    "\tV=2,\n",
    "\tT=256,\n",
    "\tdecode_mode=\"LowBitSym\", \n",
    "\ttlut_bits=10\n",
    ").to(DEVICE)\n",
    "\n",
    "Wr_scaled_q, states = quantizer.quantize(Wr_scaled)\n",
    "\n",
    "Wr_scaled_q = Wr_scaled_q.reshape(-1, 256)\n",
    "Wr_q = Wr_scaled_q * scales\n",
    "Wr_q = Wr_q.reshape_as(W)\n",
    "\n",
    "W_q = incoherence_process(Wr_q, SU, SV)\n",
    "\n",
    "err  = torch.mean(((W_q - W)**2).reshape(-1, 256), dim=-1)\n",
    "print(f\"error: {err.mean():.3e} ± {err.std():.3e}\")\n",
    "\n",
    "#error: 1.512e-05 ± 2.830e-06\n",
    "\n",
    "#error: 5.038e-05 ± 5.205e-06"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Scaling per group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec45c685c66945cb951c3eca5bca707d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [00:19<00:00, 13.41it/s]\n",
      "100%|██████████| 256/256 [00:20<00:00, 12.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: 1.311e-05 ± 1.842e-06\n"
     ]
    }
   ],
   "source": [
    "fp_model = qlib.load_model('Llama2-7b-hf', torch_dtype=torch.float16)\n",
    "W = fp_model.get_submodule('model.layers.0.self_attn.q_proj').weight.data.to(DEVICE)\n",
    "Wr, SU, SV = incoherence_preprocess(W)\n",
    "\n",
    "Wr = Wr.reshape(-1, 256)\n",
    "scales = Wr.std(dim=-1, keepdim=True)\n",
    "Wr_scaled = Wr / scales\n",
    "Wr_scaled = Wr_scaled.reshape_as(W)\n",
    "\n",
    "quantizer = qlib.trellis_quantizer(\n",
    "\tL=16,\n",
    "\tK=2,\n",
    "\tV=2,\n",
    "\tT=256,\n",
    "\tdecode_mode=\"LowBitSym\", \n",
    "\ttlut_bits=10\n",
    ").to(DEVICE)\n",
    "\n",
    "Wr_scaled_q, states = quantizer.quantize(Wr_scaled)\n",
    "Wr_scaled_q = Wr_scaled_q.reshape(-1, 256)\n",
    "Wr_q = Wr_scaled_q * scales\n",
    "Wr_q = Wr_q.reshape_as(W)\n",
    "W_q = incoherence_process(Wr_q, SU, SV)\n",
    "\n",
    "err  = torch.mean(((W_q - W)**2).reshape(-1, 256), dim=-1)\n",
    "print(f\"error: {err.mean():.3e} ± {err.std():.3e}\")\n",
    "\n",
    "# error: 5.013e-05 ± 5.153e-06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# reco = quantizer.reconstruct_weight(packed, w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# reco_fast = quantizer.reconstruct_weight_fast(packed, w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
