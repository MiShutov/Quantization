{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.append(\"/home/msst/repo/Quantization\")\n",
    "import qlib\n",
    "DEVICE = 'cuda:0'\n",
    "from qlib.utils.incoherence_preprocessing.incoherence_process_functions import incoherence_process, incoherence_preprocess, incoherence_preprocess_lukashevich, incoherence_process_lukashevich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6c40381cfc84c97ae3ac124a8e2e6db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name='Llama2-7b-hf'\n",
    "DTYPE = torch.float16\n",
    "model = qlib.load_model(model_name=model_name, torch_dtype=DTYPE)\n",
    "\n",
    "module = model.get_decoder().layers[0].self_attn.q_proj.to(DEVICE)\n",
    "#module = model.get_decoder().layers[10].mlp.down_proj.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_params = qlib.TrellisQuantizerParams(\n",
    "\tT=256,\n",
    "\tL=16,\n",
    "\tK=2,\n",
    "\tV=2,\n",
    "\tdecode_mode='LowBitSym',\n",
    "\ttlut_bits=10,\n",
    ")\n",
    "\n",
    "qmodule = qlib.TrellisLinear(\n",
    "\tweight_shape=module.weight.shape, \n",
    "\tweight_scales_group_size=128,\n",
    "\tincoh_proc_mode='lukashevich',\n",
    "\tinit_device='cuda:0',\n",
    "\tinput_quantizer_params=None,\n",
    "\tweight_quantizer_params=w_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [00:19<00:00, 13.21it/s]\n",
      "100%|██████████| 256/256 [00:19<00:00, 12.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error (w-wq)^2: 0.070\n"
     ]
    }
   ],
   "source": [
    "qmodule = qmodule.wrap_module(module, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.1096,  0.1096,  0.1096,  ...,  0.1096,  0.1096,  0.1096],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qmodule.SU #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[0.0351],\n",
       "        [0.0351],\n",
       "        [0.0351],\n",
       "        ...,\n",
       "        [0.0552],\n",
       "        [0.0552],\n",
       "        [0.0552]], device='cuda:0', dtype=torch.float16, requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qmodule.weight_scales #0.0038, 0.0060"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0062, -0.0148, -0.0022,  ...,  0.0045,  0.0017, -0.0036],\n",
       "        [ 0.0142, -0.0043,  0.0028,  ..., -0.0093, -0.0114,  0.0076],\n",
       "        [-0.0146,  0.0126,  0.0005,  ...,  0.0063,  0.0188, -0.0031],\n",
       "        ...,\n",
       "        [ 0.0013,  0.0109, -0.0003,  ...,  0.0098, -0.0298,  0.0097],\n",
       "        [ 0.0256,  0.0102,  0.0032,  ..., -0.0334, -0.0156, -0.0123],\n",
       "        [-0.0134, -0.0066,  0.0018,  ...,  0.0181,  0.0166, -0.0082]],\n",
       "       device='cuda:0', dtype=torch.float16, requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0091, -0.0117, -0.0035,  ...,  0.0056,  0.0011,  0.0005],\n",
       "        [ 0.0166,  0.0061,  0.0003,  ..., -0.0093, -0.0082,  0.0106],\n",
       "        [-0.0178,  0.0137, -0.0036,  ..., -0.0067,  0.0144, -0.0065],\n",
       "        ...,\n",
       "        [ 0.0008,  0.0075,  0.0015,  ...,  0.0123, -0.0263,  0.0105],\n",
       "        [ 0.0194,  0.0103,  0.0119,  ..., -0.0352, -0.0063, -0.0071],\n",
       "        [-0.0072, -0.0068,  0.0026,  ...,  0.0115,  0.0164, -0.0096]],\n",
       "       device='cuda:0', dtype=torch.float16, grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incoherence_process_lukashevich(qmodule.weight, qmodule.SU.half())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [00:19<00:00, 13.23it/s]\n",
      "100%|██████████| 256/256 [00:19<00:00, 13.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error (unpack): 0.074 ± 0.035\n"
     ]
    }
   ],
   "source": [
    "# m = 1024\n",
    "# n = 1024\n",
    "\n",
    "# w = torch.randn(m, n, device=DEVICE)\n",
    "\n",
    "params = qlib.TrellisQuantizerParams(\n",
    "\tT=256,\n",
    "\tL=16,\n",
    "\tK=2,\n",
    "\tV=2,\n",
    "\tdecode_mode='LowBitSym',\n",
    "\ttlut_bits=10,\n",
    ")\n",
    "\n",
    "quantizer = qlib.TrellisQuantizer(\n",
    "\tparams=params\n",
    ").to(DEVICE)\n",
    "\n",
    "reco, states = quantizer.quantize(w)\n",
    "packed = quantizer.pack_trellis(states)\n",
    "\n",
    "reco_ref = quantizer.reconstruct_weight(packed, w.shape)\n",
    "err_ref  = torch.mean((reco_ref * quantizer.codebook_scale - w)**2, dim=-1)\n",
    "print(f\"error (unpack): {err_ref.mean():.3f} ± {err_ref.std():.3f}\")\n",
    "\n",
    "\n",
    "# reco_fast = quantizer.reconstruct_weight_fast(packed, w.shape)\n",
    "# err_fast = torch.mean((reco_fast * quantizer.codebook_scale - w)**2, dim=-1)\n",
    "# print(f\"error (fast)  : {err_fast.mean():.3f} ± {err_fast.std():.3f}\")\n",
    "\n",
    "# 0.128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error (fast)  : 0.074 ± 0.003\n"
     ]
    }
   ],
   "source": [
    "#Wq_scaled = incoherence_process_lukashevich(reco_ref * quantizer.codebook_scale, SU)\n",
    "Wq_scaled = incoherence_process(reco_ref * quantizer.codebook_scale, SU, SV)\n",
    "err = torch.mean((Wq_scaled - w_orig / w_orig_scales)**2, dim=-1)\n",
    "print(f\"error (fast)  : {err.mean():.3f} ± {err.std():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No active exception to reraise",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No active exception to reraise"
     ]
    }
   ],
   "source": [
    "raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without Scaling per group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c1107643036470b9f77643d7749f330",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [00:19<00:00, 12.94it/s]\n",
      "100%|██████████| 256/256 [00:20<00:00, 12.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: 1.377e-05 ± 2.306e-06\n"
     ]
    }
   ],
   "source": [
    "fp_model = qlib.load_model('Llama2-7b-hf', torch_dtype=torch.float16)\n",
    "W = fp_model.get_submodule('model.layers.0.self_attn.q_proj').weight.data.to(DEVICE)\n",
    "Wr, SU, SV = incoherence_preprocess(W)\n",
    "\n",
    "Wr = Wr.reshape(-1, 256)\n",
    "scales = Wr.std()\n",
    "Wr_scaled = Wr / scales\n",
    "Wr_scaled = Wr_scaled.reshape_as(W)\n",
    "\n",
    "\n",
    "quantizer = qlib.trellis_quantizer(\n",
    "\tL=16,\n",
    "\tK=2,\n",
    "\tV=2,\n",
    "\tT=256,\n",
    "\tdecode_mode=\"LowBitSym\", \n",
    "\ttlut_bits=10\n",
    ").to(DEVICE)\n",
    "\n",
    "Wr_scaled_q, states = quantizer.quantize(Wr_scaled)\n",
    "\n",
    "Wr_scaled_q = Wr_scaled_q.reshape(-1, 256)\n",
    "Wr_q = Wr_scaled_q * scales\n",
    "Wr_q = Wr_q.reshape_as(W)\n",
    "\n",
    "W_q = incoherence_process(Wr_q, SU, SV)\n",
    "\n",
    "err  = torch.mean(((W_q - W)**2).reshape(-1, 256), dim=-1)\n",
    "print(f\"error: {err.mean():.3e} ± {err.std():.3e}\")\n",
    "\n",
    "#error: 1.512e-05 ± 2.830e-06\n",
    "\n",
    "#error: 5.038e-05 ± 5.205e-06"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Scaling per group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec45c685c66945cb951c3eca5bca707d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256/256 [00:19<00:00, 13.41it/s]\n",
      "100%|██████████| 256/256 [00:20<00:00, 12.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: 1.311e-05 ± 1.842e-06\n"
     ]
    }
   ],
   "source": [
    "fp_model = qlib.load_model('Llama2-7b-hf', torch_dtype=torch.float16)\n",
    "W = fp_model.get_submodule('model.layers.0.self_attn.q_proj').weight.data.to(DEVICE)\n",
    "Wr, SU, SV = incoherence_preprocess(W)\n",
    "\n",
    "Wr = Wr.reshape(-1, 256)\n",
    "scales = Wr.std(dim=-1, keepdim=True)\n",
    "Wr_scaled = Wr / scales\n",
    "Wr_scaled = Wr_scaled.reshape_as(W)\n",
    "\n",
    "quantizer = qlib.trellis_quantizer(\n",
    "\tL=16,\n",
    "\tK=2,\n",
    "\tV=2,\n",
    "\tT=256,\n",
    "\tdecode_mode=\"LowBitSym\", \n",
    "\ttlut_bits=10\n",
    ").to(DEVICE)\n",
    "\n",
    "Wr_scaled_q, states = quantizer.quantize(Wr_scaled)\n",
    "Wr_scaled_q = Wr_scaled_q.reshape(-1, 256)\n",
    "Wr_q = Wr_scaled_q * scales\n",
    "Wr_q = Wr_q.reshape_as(W)\n",
    "W_q = incoherence_process(Wr_q, SU, SV)\n",
    "\n",
    "err  = torch.mean(((W_q - W)**2).reshape(-1, 256), dim=-1)\n",
    "print(f\"error: {err.mean():.3e} ± {err.std():.3e}\")\n",
    "\n",
    "# error: 5.013e-05 ± 5.153e-06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# reco = quantizer.reconstruct_weight(packed, w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# reco_fast = quantizer.reconstruct_weight_fast(packed, w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
